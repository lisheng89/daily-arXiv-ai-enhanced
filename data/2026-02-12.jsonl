{"id": "2602.10258", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.10258", "abs": "https://arxiv.org/abs/2602.10258", "authors": ["Haike Xu", "Guy Blelloch", "Laxman Dhulipala", "Lars Gottesbüren", "Rajesh Jayaram", "Jakub Łącki"], "title": "JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search", "comment": null, "summary": "Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness."}
{"id": "2602.10271", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10271", "abs": "https://arxiv.org/abs/2602.10271", "authors": ["Yongyue Zhang", "Yaxiong Wu"], "title": "MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation", "comment": "15 pages", "summary": "Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggregate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query representation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Generation (MLDocRAG) framework that leverages a Multimodal Chunk-Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine-grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing grounding and coherence in long-context multimodal question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval quality and answer accuracy, demonstrating its effectiveness for long-context multimodal understanding."}
{"id": "2602.10321", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10321", "abs": "https://arxiv.org/abs/2602.10321", "authors": ["Debayan Mukhopadhyay", "Utshab Kumar Ghosh", "Shubham Chatterjee"], "title": "Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval", "comment": null, "summary": "Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025"}
{"id": "2602.10411", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10411", "abs": "https://arxiv.org/abs/2602.10411", "authors": ["Fangye Wang", "Haowen Lin", "Yifang Yuan", "Siyuan Wang", "Xiaojiang Zhou", "Song Yang", "Pengjie Wang"], "title": "GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation", "comment": null, "summary": "Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production."}
{"id": "2602.10445", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10445", "abs": "https://arxiv.org/abs/2602.10445", "authors": ["Jie Jiang", "Xinxun Zhang", "Enming Zhang", "Yuling Xiong", "Jun Zhang", "Jingwen Wang", "Huan Yu", "Yuxiang Wang", "Hao Wang", "Xiao Yan", "Jiawei Jiang"], "title": "End-to-End Semantic ID Generation for Generative Advertisement Recommendation", "comment": null, "summary": "Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline."}
{"id": "2602.10455", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10455", "abs": "https://arxiv.org/abs/2602.10455", "authors": ["Hui Lu", "Zheng Chai", "Shipeng Bai", "Hao Zhang", "Zhifang Fan", "Kunmin Bai", "Yingwen Wu", "Bingzheng Wei", "Xiang Sun", "Ziyan Gong", "Tianyi Liu", "Hua Chen", "Deping Xie", "Zhongkai Chen", "Zhiliang Guo", "Qiwei Chen", "Yuchao Zheng"], "title": "Compute Only Once: UG-Separation for Efficient Large Recommendation Models", "comment": "Large Recommender Model, Industrial Recommenders, Scaling Law", "summary": "Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems."}
{"id": "2602.10490", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10490", "abs": "https://arxiv.org/abs/2602.10490", "authors": ["Fuchun Li", "Qian Li", "Xingyu Gao", "Bocheng Pan", "Yang Wu", "Jun Zhang", "Huan Yu", "Jie Jiang", "Jinsheng Xiao", "Hailong Shi"], "title": "ChainRec: An Agentic Recommender Learning to Route Tool Chains for Diverse and Evolving Interests", "comment": null, "summary": "Large language models (LLMs) are increasingly integrated into recommender systems, motivating recent interest in agentic and reasoning-based recommendation. However, most existing approaches still rely on fixed workflows, applying the same reasoning procedure across diverse recommendation scenarios. In practice, user contexts vary substantially-for example, in cold-start settings or during interest shifts, so an agent should adaptively decide what evidence to gather next rather than following a scripted process. To address this, we propose ChainRec, an agentic recommender that uses a planner to dynamically select reasoning tools. ChainRec builds a standardized Tool Agent Library from expert trajectories. It then trains a planner using supervised fine-tuning and preference optimization to dynamically select tools, decide their order, and determine when to stop. Experiments on AgentRecBench across Amazon, Yelp, and Goodreads show that ChainRec consistently improves Avg HR@{1,3,5} over strong baselines, with especially notable gains in cold-start and evolving-interest scenarios. Ablation studies further validate the importance of tool standardization and preference-optimized planning."}
{"id": "2602.10493", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10493", "abs": "https://arxiv.org/abs/2602.10493", "authors": ["Jingsong Su", "Xuetao Ma", "Mingming Li", "Qiannan Zhu", "Yu Guo"], "title": "Boundary-Aware Multi-Behavior Dynamic Graph Transformer for Sequential Recommendation", "comment": null, "summary": "In the landscape of contemporary recommender systems, user-item interactions are inherently dynamic and sequential, often characterized by various behaviors. Prior research has explored the modeling of user preferences through sequential interactions and the user-item interaction graph, utilizing advanced techniques such as graph neural networks and transformer-based architectures. However, these methods typically fall short in simultaneously accounting for the dynamic nature of graph topologies and the sequential pattern of interactions in user preference models. Moreover, they often fail to adequately capture the multiple user behavior boundaries during model optimization. To tackle these challenges, we introduce a boundary-aware Multi-Behavioral Dynamic Graph Transformer (MB-DGT) model that dynamically refines the graph structure to reflect the evolving patterns of user behaviors and interactions. Our model involves a transformer-based dynamic graph aggregator for user preference modeling, which assimilates the changing graph structure and the sequence of user behaviors. This integration yields a more comprehensive and dynamic representation of user preferences. For model optimization, we implement a user-specific multi-behavior loss function that delineates the interest boundaries among different behaviors, thereby enriching the personalized learning of user preferences. Comprehensive experiments across three datasets indicate that our model consistently delivers remarkable recommendation performance."}
{"id": "2602.10577", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10577", "abs": "https://arxiv.org/abs/2602.10577", "authors": ["Yiming Che", "Mansi Mane", "Keerthi Gopalakrishnan", "Parisa Kaghazgaran", "Murali Mohana Krishna Dandu", "Archana Venkatachalapathy", "Sinduja Subramaniam", "Yokila Arora", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking", "comment": null, "summary": "E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \\textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall."}
{"id": "2602.10606", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10606", "abs": "https://arxiv.org/abs/2602.10606", "authors": ["Jie Jiang", "Hongbo Tang", "Wenjie Wu", "Yangru Huang", "Zhenmao Li", "Qian Li", "Changping Wang", "Jun Zhang", "Huan Yu"], "title": "S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage", "comment": null, "summary": "Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\\% lift in GMV in online A/B tests, without requiring real-time LLM inference."}
{"id": "2602.10633", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10633", "abs": "https://arxiv.org/abs/2602.10633", "authors": ["Hongyue Zhan", "Mingming Li", "Dongqin Liu", "Hui Wang", "Yaning Zhang", "Xi Zhou", "Honglei Lv", "Jiao Dai", "Jizhong Han"], "title": "A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems", "comment": null, "summary": "With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems."}
{"id": "2602.10811", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.10811", "abs": "https://arxiv.org/abs/2602.10811", "authors": ["Mingyang Liu", "Yong Bai", "Zhangming Chan", "Sishuo Chen", "Xiang-Rong Sheng", "Han Zhu", "Jian Xu", "Xinyang Chen"], "title": "EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling", "comment": null, "summary": "Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\\% RPM (Revenue Per Mile) increase and a 1.22\\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models."}
{"id": "2602.10833", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.10833", "abs": "https://arxiv.org/abs/2602.10833", "authors": ["William Xion", "Wolfgang Nejdl"], "title": "Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval", "comment": "Accepted at ECIR 2026", "summary": "Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called \"source bias\", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers."}

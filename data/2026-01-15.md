<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 13]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas](https://arxiv.org/abs/2601.08901)
*Yuexi Shen,Minqian Liu,Dawei Zhou,Lifu Huang*

Main category: cs.IR

TL;DR: 提出Ideation Space框架，将科学知识分解为研究问题、方法和核心发现三个维度，实现细粒度文献检索和分解式新颖性评估


<details>
  <summary>Details</summary>
Motivation: 科学发现是累积过程，需要将新想法置于不断扩展的现有知识体系中。当前嵌入方法将不同概念方面混为单一表示，无法支持细粒度文献检索；LLM评估器存在奉承偏见，无法提供区分性新颖性评估

Method: 引入Ideation Space结构化表示，通过对比学习将科学知识分解为研究问题、方法和核心发现三个维度；提出分层子空间检索框架进行高效针对性文献检索，以及分解式新颖性评估算法识别想法的哪些方面是新颖的

Result: 在广泛实验中取得显著改进：Recall@30达到0.329（比基线提升16.7%），构思转换检索的Hit Rate@30达到0.643，新颖性评估与专家判断的相关性达到0.37

Conclusion: 该工作为加速和评估科学发现的未来研究提供了一个有前景的范式

Abstract: Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.

</details>


### [2] [Fine Grained Evaluation of LLMs-as-Judges](https://arxiv.org/abs/2601.08919)
*Sourav Saha,Mandar Mitra*

Main category: cs.IR

TL;DR: 研究探讨了使用大语言模型作为信息检索相关性评估者的有效性，特别是在文档级别和段落级别评估方面的表现，发现LLM作为评估者在人类监督下效果最佳。


<details>
  <summary>Details</summary>
Motivation: 当前研究关注LLM作为"法官"替代人类评估文本/图像处理系统输出的质量。本研究扩展了现有研究，特别关注LLM在信息检索标准任务中作为相关性评估者的有效性，并进一步探索LLM能否像人类评估者一样识别文档中的相关段落。

Method: 使用INEX倡议创建的基于维基百科的测试集，提示LLM不仅判断文档是否相关/不相关，还要在认为有用的文档中高亮相关段落。人类评估者在创建该集合时也收到了类似指令，这使研究者能够在文档级别和段落级别评估LLM作为评估者的质量。

Result: 研究发现LLM作为评估者在人类监督下效果最佳。研究不仅评估了文档级别的相关性判断，还量化了这些"法官"是否出于正确的原因做出正确判断。

Conclusion: LLM可以作为信息检索相关性评估者，但在段落级别的相关性识别方面需要进一步研究。最重要的是，LLM作为评估者在人类监督下表现最好，这为实际应用提供了重要指导。

Abstract: A good deal of recent research has focused on how Large Language Models
  (LLMs) may be used as `judges' in place of humans to evaluate the quality
  of the output produced by various text / image processing systems. Within
  this broader context, a number of studies have investigated the specific
  question of how effectively LLMs can be used as relevance assessors for
  the standard ad hoc task in Information Retrieval (IR). We extend these
  studies by looking at additional questions. Most importantly, we use a
  Wikipedia based test collection created by the INEX initiative, and
  prompt LLMs to not only judge whether documents are relevant /
  non-relevant, but to highlight relevant passages in documents that it
  regards as useful. The human relevance assessors involved in creating
  this collection were given analogous instructions, i.e., they were asked
  to highlight all passages within a document that respond to the
  information need expressed in a query. This enables us to evaluate the
  quality of LLMs as judges not only at the document level, but to also
  quantify how often these `judges' are right for the right reasons.
  Our findings suggest that LLMs-as-judges work best under human
  supervision.

</details>


### [3] [LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval](https://arxiv.org/abs/2601.09159)
*Zhibo Zhang,Yang Xu,Kai Ming Ting,Cam-Tu Nguyen*

Main category: cs.IR

TL;DR: IKE是一种无需训练的方法，将LLM高维嵌入转换为二进制嵌入，使用隔离核技术，显著提升检索效率同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLM嵌入通常维度很高，导致存储和检索开销大。现有方法如MRL和CSR虽然有所缓解，但仍存在检索精度下降的问题。

Method: 提出Isolation Kernel Embedding (IKE)，一种无需学习的方法，使用隔离核将LLM嵌入转换为二进制嵌入。IKE通过多样化的随机分区集合来稳健估计理想核。

Result: 在多个文本检索数据集上，IKE相比LLM嵌入实现高达16.7倍的检索加速和16倍的内存节省，同时保持相当或更好的准确性。相比CSR和其他压缩方法，IKE在检索效率和效果之间达到最佳平衡。

Conclusion: IKE提供了一种高效、轻量级的LLM嵌入压缩方案，显著降低存储和计算开销，同时保持检索性能，在效率和效果之间达到良好平衡。

Abstract: Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.

</details>


### [4] [Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models](https://arxiv.org/abs/2601.09286)
*Hanze Guo,Jianxun Lian,Xiao Zhou*

Main category: cs.IR

TL;DR: SaD提出稀疏与稠密双视图对齐框架，解决传统协同过滤在冷门物品建模中的信噪比瓶颈问题，通过双向对齐机制提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于稠密嵌入的协同过滤方法在处理冷门物品时存在信噪比瓶颈，在数据极度稀疏的情况下，参数化稠密模型的信噪比会显著下降，限制了推荐系统的性能。

Method: 提出SaD统一框架，将稠密嵌入的语义表达能力与稀疏交互模式的结构可靠性相结合。采用轻量级双向对齐机制：稠密视图通过注入语义相关性来丰富稀疏视图，而稀疏视图通过显式结构信号来正则化稠密模型。

Result: 理论证明双视图对齐能获得严格更优的全局信噪比。实验表明，在双视图对齐下，即使是简单的矩阵分解式稠密模型也能达到最先进性能。SaD在真实基准测试中持续优于强基线，在BarsMatch排行榜上排名第一。

Conclusion: SaD框架证明了从双视角（稀疏与稠密）协同利用协同过滤的持久威力，该框架即插即用，可无缝应用于广泛的现有推荐模型，为解决冷门物品推荐问题提供了有效方案。

Abstract: Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.

</details>


### [5] [On-Device Large Language Models for Sequential Recommendation](https://arxiv.org/abs/2601.09306)
*Xin Xia,Hongzhi Yin,Shane Culpepper*

Main category: cs.IR

TL;DR: OD-LLM：首个面向顺序推荐任务的LLM设备端自适应压缩框架，通过低秩结构压缩和tokenization归一化技术，在模型大小减半时保持推荐效果不变。


<details>
  <summary>Details</summary>
Motivation: 设备端推荐对于实时性、用户隐私和网络不稳定场景至关重要，但LLMs巨大的内存占用和计算开销使其难以在资源受限设备上部署。

Method: 1）低秩结构压缩算法（SVD）减少参数冗余；2）tokenization归一化技术增强低秩分解效果；3）渐进对齐算法逐层优化参数以减少性能损失。

Result: 在顺序推荐基准测试中，OD-LLM在模型大小减半的情况下，相比原始推荐模型没有效果损失，证明了其有效性和可扩展性。

Conclusion: OD-LLM为实时设备端推荐提供了一种实用的替代方案，能够替代昂贵的远程执行LLMs，实现高效准确的设备端部署。

Abstract: On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.

</details>


### [6] [LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform](https://arxiv.org/abs/2601.09366)
*Jana Isabelle Friese,Andreas Konstantin Kruff,Philipp Schaer,Norbert Fuhr,Nicola Ferro*

Main category: cs.IR

TL;DR: 提出了一个可复用的数据集和基础设施，用于研究交互式信息检索中的人类搜索行为，包含61名参与者的详细交互日志和用户特征数据。


<details>
  <summary>Details</summary>
Motivation: 为了促进交互式信息检索领域对人类搜索行为的研究，需要可复用的数据集和基础设施来支持可重复性研究和资源分享。

Method: 收集了61名参与者（122个会话）的详细交互日志，结合用户特征数据（感知速度、主题兴趣、搜索专业知识、人口统计信息），并提供完整的研究设置文档、基于网络的感知速度测试框架。

Result: 创建了一个包含丰富用户特征和交互数据的可复用数据集，提供了完整的研究基础设施，并通过示例分析展示了数据集的潜力。

Conclusion: 该工作为IIR社区提供了开放访问的资源，支持研究个体和情境因素对搜索行为的影响，以及开发和验证考虑用户差异性的用户模拟器。

Abstract: We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community.

</details>


### [7] [Dissecting Judicial Reasoning in U.S. Copyright Damage Awards](https://arxiv.org/abs/2601.09459)
*Pei-Chi Lo,Thomas Y. Lu*

Main category: cs.IR

TL;DR: 本文提出了一种基于话语分析和大型语言模型的新方法，用于量化分析版权损害赔偿判决中的司法推理模式，揭示了不同巡回法院在因素权重分配上的差异。


<details>
  <summary>Details</summary>
Motivation: 版权损害赔偿判决中的司法推理存在重大挑战。虽然联邦法院遵循1976年版权法，但不同司法管辖区对法律的解释和因素权重分配存在广泛差异。这种不一致性给诉讼当事人带来了不可预测性，并掩盖了法律决策的实证基础。

Method: 提出了一种新颖的基于话语分析的大型语言模型方法，整合了修辞结构理论（RST）和智能体工作流程。采用三阶段管道：数据集构建、话语分析和智能体特征提取，将司法意见解析为层次化话语结构，识别推理组件并提取特征标签及其对应的话语子树。

Result: 话语增强的LLM分析在分析版权损害赔偿裁决时优于传统方法，同时揭示了不同巡回法院在因素权重分配上先前未量化的变化。

Conclusion: 该方法为计算法律分析提供了方法论进步，并为司法推理提供了实践洞察。对寻求预测工具的法律从业者、研究法律原则应用的学者以及面对版权法不一致性的政策制定者具有重要意义。

Abstract: Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.

</details>


### [8] [Bridging Semantic Understanding and Popularity Bias with LLMs](https://arxiv.org/abs/2601.09478)
*Renqiang Luo,Dong Zhang,Yupeng Gao,Wen Shi,Mingliang Hou,Jiaying Liu,Zhe Wang,Shuo Yu*

Main category: cs.IR

TL;DR: FairLRM是一个通过大语言模型增强推荐系统对流行度偏差语义理解的框架，将偏差分解为物品侧和用户侧组件，显著提升公平性和推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法大多将流行度偏差的语义理解简化为多样性增强或长尾覆盖，忽视了偏差本身的因果起源，这种浅层理解限制了去偏效果和推荐准确性。

Method: 提出FairLRM框架，利用大语言模型增强对流行度偏差的语义理解，将偏差分解为物品侧和用户侧组件，使用结构化指令提示来增强模型对全局物品分布和个体用户偏好的理解。

Result: 实证评估显示FairLRM显著提升了公平性和推荐准确性，提供了更具语义感知和可信赖的流行度偏差理解方法。

Conclusion: FairLRM通过大语言模型增强对流行度偏差的深层语义理解，为推荐系统提供了一种更有效、更公平的去偏方法，超越了传统基于表面特征的方法。

Abstract: Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as "diversity" or "debiasing", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.

</details>


### [9] [Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning](https://arxiv.org/abs/2601.09496)
*Jujia Zhao,Zihan Wang,Shuaiqun Pan,Suzan Verberne,Zhaochun Ren*

Main category: cs.IR

TL;DR: GEMS：一个统一搜索与推荐的LLM框架，通过多子空间分解和零空间投影解决梯度冲突和知识偏移问题


<details>
  <summary>Details</summary>
Motivation: 搜索和推荐在在线平台中具有互补作用，统一建模有重要意义。现有方法要么采用共享编码器，要么依赖LLM全微调，但全微调计算成本高且可扩展性差。参数高效微调(PEFT)更实用，但在统一S&R时面临梯度冲突和用户意图理解偏移两大挑战。

Method: 提出GEMS框架：1) 多子空间分解：将共享和任务特定的优化信号解耦到互补的低秩子空间，减少破坏性梯度干扰；2) 零空间投影：将参数更新约束到与通用知识空间正交的子空间，减轻用户意图理解的偏移。

Result: 在基准数据集上的广泛实验表明，GEMS在搜索和推荐任务上始终优于最先进的基线方法，实现了卓越的有效性。

Conclusion: GEMS成功解决了统一搜索与推荐时PEFT面临的梯度冲突和知识偏移问题，通过创新的多子空间分解和零空间投影技术，在保持LLM通用知识的同时实现了任务间的有效协同。

Abstract: Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.

</details>


### [10] [TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09523)
*Abdelrahman Abdallah,Mohammed Ali,Muhammad Abdul-Mageed,Adam Jatowt*

Main category: cs.IR

TL;DR: TEMPO是首个结合时序推理与推理密集型检索的基准测试，包含1,730个复杂查询、3,976个分解步骤和新的时序评估指标，评估显示现有检索系统在获取完整时序证据方面面临重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有时序QA基准主要关注新闻语料中的简单事实查询，而推理密集型检索基准缺乏时序基础。现实世界的信息需求通常需要理解时间演变和跨时期证据综合。

Method: 构建包含13个领域的TEMPO基准，具有：(1) 1,730个需要深度时序推理的复杂查询；(2) 逐步检索规划，包含3,976个分解步骤和每个步骤的黄金文档映射；(3) 新的时序评估指标Temporal Coverage@k和Temporal Precision@k。

Result: 评估12个检索系统显示显著挑战：最佳模型(DiVeR)仅达到32.0 NDCG@10和71.4% Temporal Coverage@10，表明在检索完整时序证据方面存在困难。

Conclusion: TEMPO为改进检索和RAG系统中的时序推理提供了一个具有挑战性的基准，现有系统在获取跨时间段的完整证据方面仍有很大提升空间。

Abstract: Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.

</details>


### [11] [SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval](https://arxiv.org/abs/2601.09530)
*Bingde Hu,Enhao Pan,Wanjing Zhou,Yang Gao,Zunlei Feng,Hao Zhong*

Main category: cs.IR

TL;DR: 提出统一时空向量检索框架，通过旋转编码、增量更新和加权检索算法，在单一相似空间集成时空语义信息，显著提升检索精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有时空检索方法多为传统向量搜索的扩展，依赖外部过滤器或专用索引处理时空约束，导致效率低下、架构复杂，且难以处理异构模态数据。

Method: 1) 基于旋转的统一编码方法：将时间和位置嵌入旋转位置向量；2) 循环增量更新机制：支持滑动窗口更新而无需全局重编码；3) 加权兴趣检索算法：自适应平衡模态权重实现上下文感知检索。

Result: 在多个真实数据集上的实验表明，该框架在检索准确性和效率上显著优于现有方法，并在动态数据演化下保持鲁棒性。

Conclusion: 该统一框架为智能系统中的可扩展时空信息检索提供了有效实用的解决方案，克服了现有方法的局限性。

Abstract: Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.

</details>


### [12] [Examining DOM Coordinate Effectiveness For Page Segmentation](https://arxiv.org/abs/2601.09543)
*Jason Carpenter,Faaiq Bilal,Eman Ramadan,Zhi-Li Zhang*

Main category: cs.IR

TL;DR: 该研究分析了网页DOM坐标对页面分割的影响，发现视觉坐标表现不如DOM坐标，简单向量优于复杂向量，最佳匹配可实现74%的分割准确率。


<details>
  <summary>Details</summary>
Motivation: 网页数据规模庞大且非结构化，现有基于DOM的方法常使用视觉位置或树结构等坐标信息，但这些向量的构建和组件价值缺乏深入分析。研究旨在深入理解DOM坐标对网页分割的影响。

Method: 提出并详细检验DOM坐标，比较视觉坐标与DOM坐标的性能，分析简单向量与复杂向量的效果，探索向量、聚类算法和页面的最佳匹配。

Result: 视觉坐标平均表现比DOM坐标差20-30%；简单向量（单坐标）在68.2%的页面中表现优于复杂向量；通过最佳匹配可实现74%的整体分割准确率，比朴素应用向量方法提升20%。

Conclusion: 研究挑战了当前分割向量构建的正统观念，表明无需包含视觉坐标，简单DOM坐标向量效果更好，强调需要寻找机制来匹配最佳网页分割方法。

Abstract: Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.

</details>


### [13] [MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.09562)
*Abdelrahman Abdallah,Mohamed Darwish Mounis,Mahmoud Abdalla,Mahmoud SalahEldin Kasem,Mostafa Farouk Senussi,Mohamed Mahmoud,Mohammed Ali,Adam Jatowt,Hyun-Soo Kang*

Main category: cs.IR

TL;DR: MM-BRIGHT是首个针对推理密集型检索的多模态基准测试，包含2,803个真实世界查询，涵盖29个技术领域和四种复杂度递增的任务，现有最先进模型在所有任务上都表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有检索基准主要基于文本查询，依赖关键词或语义匹配，而现实世界查询常包含图表、截图等多模态元素，需要深度推理才能找到相关文档，目前缺乏针对此类推理密集型检索的评估基准。

Method: 构建包含2,803个真实世界查询的多模态数据集，涵盖29个技术领域，设计四种复杂度递增的任务：文本到文本、多模态到文本、多模态到图像、多模态到多模态检索。

Result: 现有最先进模型在所有任务上都表现不佳：BM25在纯文本检索上仅获得8.5 nDCG@10，最佳多模态模型Nomic-Vision在多模态到文本检索上仅达到27.6 nDCG@10，甚至低于最佳纯文本模型DiVeR的32.2。

Conclusion: MM-BRIGHT揭示了当前检索模型在视觉推理集成方面的重大不足，为下一代更好地整合视觉推理的检索模型提供了测试平台，展示了显著的改进空间。

Abstract: Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.

</details>

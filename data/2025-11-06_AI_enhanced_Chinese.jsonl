{"id": "2511.03155", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03155", "abs": "https://arxiv.org/abs/2511.03155", "authors": ["Zhefan Wang", "Guokai Yan", "Jinbei Yu", "Siyu Gu", "Jingyan Chen", "Peng Jiang", "Zhiqiang Guo", "Min Zhang"], "title": "Generative Sequential Recommendation via Hierarchical Behavior Modeling", "comment": null, "summary": "Recommender systems in multi-behavior domains, such as advertising and\ne-commerce, aim to guide users toward high-value but inherently sparse\nconversions. Leveraging auxiliary behaviors (e.g., clicks, likes, shares) is\ntherefore essential. Recent progress on generative recommendations has brought\nnew possibilities for multi-behavior sequential recommendation. However,\nexisting generative approaches face two significant challenges: 1) Inadequate\nSequence Modeling: capture the complex, cross-level dependencies within user\nbehavior sequences, and 2) Lack of Suitable Datasets: publicly available\nmulti-behavior recommendation datasets are almost exclusively derived from\ne-commerce platforms, limiting the validation of feasibility in other domains,\nwhile also lacking sufficient side information for semantic ID generation. To\naddress these issues, we propose a novel generative framework, GAMER\n(Generative Augmentation and Multi-lEvel behavior modeling for Recommendation),\nbuilt upon a decoder-only backbone. GAMER introduces a cross-level interaction\nlayer to capture hierarchical dependencies among behaviors and a sequential\naugmentation strategy that enhances robustness in training. To further advance\nthis direction, we collect and release ShortVideoAD, a large-scale\nmulti-behavior dataset from a mainstream short-video platform, which differs\nfundamentally from existing e-commerce datasets and provides pretrained\nsemantic IDs for research on generative methods. Extensive experiments show\nthat GAMER consistently outperforms both discriminative and generative\nbaselines across multiple metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86GAMER\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u751f\u6210\u5f0f\u65b9\u6cd5\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u5e8f\u5217\u5efa\u6a21\u4e0d\u8db3\u548c\u7f3a\u4e4f\u5408\u9002\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u591a\u884c\u4e3a\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5f15\u5bfc\u7528\u6237\u5b8c\u6210\u9ad8\u4ef7\u503c\u4f46\u7a00\u758f\u7684\u8f6c\u5316\u884c\u4e3a\uff0c\u5229\u7528\u8f85\u52a9\u884c\u4e3a\uff08\u5982\u70b9\u51fb\u3001\u70b9\u8d5e\u3001\u5206\u4eab\uff09\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u590d\u6742\u8de8\u7ea7\u4f9d\u8d56\u5efa\u6a21\u548c\u6570\u636e\u96c6\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u57fa\u4e8e\u89e3\u7801\u5668\u67b6\u6784\u7684GAMER\u6846\u67b6\uff0c\u5f15\u5165\u8de8\u7ea7\u4ea4\u4e92\u5c42\u6355\u83b7\u884c\u4e3a\u95f4\u7684\u5c42\u6b21\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u5e8f\u5217\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u8bad\u7ec3\u9c81\u68d2\u6027\u3002\u540c\u65f6\u6536\u96c6\u5e76\u53d1\u5e03\u4e86ShortVideoAD\u77ed\u89c6\u9891\u5e7f\u544a\u6570\u636e\u96c6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGAMER\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GAMER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u884c\u4e3a\u63a8\u8350\u4e2d\u7684\u5e8f\u5217\u5efa\u6a21\u548c\u6570\u636e\u96c6\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.03298", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03298", "abs": "https://arxiv.org/abs/2511.03298", "authors": ["Oleg Senkevich", "Siyang Xu", "Tianyi Jiang", "Alexander Radionov", "Jan Tabaszewski", "Dmitriy Malyshev", "Zijian Li", "Daihao Xue", "Licheng Yu", "Weidi Zeng", "Meiling Wang", "Xin Yao", "Siyu Huang", "Gleb Neshchetkin", "Qiuling Pan", "Yaoyao Fu"], "title": "KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng", "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for\ninformation retrieval, recommendation systems, and machine learning\napplications. While x86-based architectures have historically dominated this\ndomain, the increasing adoption of ARM-based servers in industry presents a\ncritical need for ANNS solutions optimized on ARM architectures. A naive port\nof existing x86 ANNS algorithms to ARM platforms results in a substantial\nperformance deficit, failing to leverage the unique capabilities of the\nunderlying hardware. To address this challenge, we introduce KScaNN, a novel\nANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN\nembodies a holistic approach that synergizes sophisticated, data aware\nalgorithmic refinements with carefully-designed hardware specific\noptimizations. Its core contributions include: 1) novel algorithmic techniques,\nincluding a hybrid intra-cluster search strategy and an improved PQ residual\ncalculation method, which optimize the search process at a higher level; 2) an\nML-driven adaptive search module that provides adaptive, per-query tuning of\nsearch parameters, eliminating the inefficiencies of static configurations; and\n3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for\nthe critical distance computation workloads. The experimental results\ndemonstrate that KScaNN not only closes the performance gap but establishes a\nnew standard, achieving up to a 1.63x speedup over the fastest x86-based\nsolution. This work provides a definitive blueprint for achieving\nleadership-class performance for vector search on modern ARM architectures and\nunderscores", "AI": {"tldr": "KScaNN\u662f\u4e00\u4e2a\u4e13\u4e3aARM Kunpeng 920\u67b6\u6784\u8bbe\u8ba1\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b97\u6cd5\u4f18\u5316\u548c\u786c\u4ef6\u7279\u5b9a\u4f18\u5316\uff0c\u5728ARM\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u6700\u5febx86\u89e3\u51b3\u65b9\u68481.63\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u968f\u7740ARM\u670d\u52a1\u5668\u5728\u5de5\u4e1a\u754c\u7684\u666e\u53ca\uff0c\u9700\u8981\u9488\u5bf9ARM\u67b6\u6784\u4f18\u5316\u7684ANNS\u89e3\u51b3\u65b9\u6848\u3002\u7b80\u5355\u79fb\u690d\u73b0\u6709x86\u7b97\u6cd5\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5e95\u5c42\u786c\u4ef6\u80fd\u529b\u3002", "method": "1) \u6df7\u5408\u96c6\u7fa4\u5185\u641c\u7d22\u7b56\u7565\u548c\u6539\u8fdb\u7684PQ\u6b8b\u5dee\u8ba1\u7b97\u65b9\u6cd5\uff1b2) ML\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u641c\u7d22\u6a21\u5757\uff0c\u5b9e\u73b0\u6309\u67e5\u8be2\u81ea\u9002\u5e94\u7684\u53c2\u6570\u8c03\u4f18\uff1b3) \u9488\u5bf9ARM\u7684\u9ad8\u5ea6\u4f18\u5316\u7684SIMD\u5185\u6838\uff0c\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\u3002", "result": "KScaNN\u4e0d\u4ec5\u5f25\u8865\u4e86\u6027\u80fd\u5dee\u8ddd\uff0c\u8fd8\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u76f8\u6bd4\u6700\u5feb\u7684x86\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u6700\u9ad81.63\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u73b0\u4ee3ARM\u67b6\u6784\u4e0a\u5b9e\u73b0\u9886\u5148\u7684\u5411\u91cf\u641c\u7d22\u6027\u80fd\u63d0\u4f9b\u4e86\u660e\u786e\u84dd\u56fe\uff0c\u5f3a\u8c03\u4e86\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.03330", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.03330", "abs": "https://arxiv.org/abs/2511.03330", "authors": ["Shenghua Wang", "Zhen Yin"], "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning", "comment": null, "summary": "The rapid growth of open-access (OA) publications has intensified the\nchallenge of identifying relevant scientific papers. Due to privacy constraints\nand limited access to user interaction data, recent efforts have shifted toward\ncontent-based recommendation, which relies solely on textual information.\nHowever, existing models typically treat papers as unstructured text,\nneglecting their discourse organization and thereby limiting semantic\ncompleteness and interpretability. To address these limitations, we propose\nOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,\nMethod, Result, Conclusion) summarization, multi-level contrastive learning,\nand structure-aware re-ranking for scholarly recommendation. The QA-style\nsummarization module converts raw papers into structured and\ndiscourse-consistent representations, while multi-level contrastive objectives\nalign semantic representations across metadata, section, and document levels.\nThe final re-ranking stage further refines retrieval precision through\ncontextual similarity calibration. Experiments on DBLP, S2ORC, and the newly\nconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses\nstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in\nPrecision@10 and Recall@10, respectively. Additional evaluations confirm that\nQA-style summarization produces more coherent and factually complete\nrepresentations. Overall, OMRC-MR provides a unified and interpretable\ncontent-based paradigm for scientific paper recommendation, advancing\ntrustworthy and privacy-aware scholarly information retrieval.", "AI": {"tldr": "OMRC-MR\u662f\u4e00\u4e2a\u57fa\u4e8e\u5185\u5bb9\u7684\u79d1\u5b66\u8bba\u6587\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7QA\u5f0fOMRC\u6458\u8981\u3001\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u548c\u7ed3\u6784\u611f\u77e5\u91cd\u6392\u6765\u63d0\u5347\u63a8\u8350\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5f00\u653e\u83b7\u53d6\u51fa\u7248\u7269\u7684\u5feb\u901f\u589e\u957f\u52a0\u5267\u4e86\u8bc6\u522b\u76f8\u5173\u79d1\u5b66\u8bba\u6587\u7684\u6311\u6218\u3002\u7531\u4e8e\u9690\u79c1\u9650\u5236\u548c\u7528\u6237\u4ea4\u4e92\u6570\u636e\u6709\u9650\uff0c\u7814\u7a76\u8f6c\u5411\u57fa\u4e8e\u5185\u5bb9\u7684\u63a8\u8350\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5c06\u8bba\u6587\u89c6\u4e3a\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u5176\u8bdd\u8bed\u7ec4\u7ec7\uff0c\u9650\u5236\u4e86\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faOMRC-MR\u5206\u5c42\u6846\u67b6\uff1a1\uff09QA\u5f0fOMRC\u6458\u8981\u6a21\u5757\u5c06\u539f\u59cb\u8bba\u6587\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff1b2\uff09\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5728\u5143\u6570\u636e\u3001\u7ae0\u8282\u548c\u6587\u6863\u7ea7\u522b\u5bf9\u9f50\u8bed\u4e49\u8868\u793a\uff1b3\uff09\u7ed3\u6784\u611f\u77e5\u91cd\u6392\u9636\u6bb5\u901a\u8fc7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u6821\u51c6\u4f18\u5316\u68c0\u7d22\u7cbe\u5ea6\u3002", "result": "\u5728DBLP\u3001S2ORC\u548c\u65b0\u6784\u5efa\u7684Sci-OMRC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOMRC-MR\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Precision@10\u548cRecall@10\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.2%\u548c3.8%\u7684\u6539\u8fdb\u3002\u989d\u5916\u8bc4\u4f30\u8bc1\u5b9eQA\u5f0f\u6458\u8981\u80fd\u4ea7\u751f\u66f4\u8fde\u8d2f\u548c\u4e8b\u5b9e\u5b8c\u6574\u7684\u8868\u793a\u3002", "conclusion": "OMRC-MR\u4e3a\u79d1\u5b66\u8bba\u6587\u63a8\u8350\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5185\u5bb9\u8303\u5f0f\uff0c\u63a8\u8fdb\u4e86\u53ef\u4fe1\u8d56\u548c\u9690\u79c1\u611f\u77e5\u7684\u5b66\u672f\u4fe1\u606f\u68c0\u7d22\u3002"}}
{"id": "2511.03351", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.03351", "abs": "https://arxiv.org/abs/2511.03351", "authors": ["Saba Latif", "Fajar J. Ekaputra", "Maxim Vidgof", "Sabrina Kirrane", "Claudio Di Ciccio"], "title": "A Semantic Encoding of Object Centric Event Data", "comment": "12 pages, 3 figures, Wil60", "summary": "The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing\na common ground for process data records centered around events and objects.\nOne of its objectives is to foster interoperability and process information\nexchange. In this context, the integration of data from different providers,\nthe combination of multiple processes, and the enhancement of knowledge\ninference are novel challenges. Semantic Web technologies can enable the\ncreation of a machine-readable OCED description enriched through ontology-based\nrelationships and entity categorization. In this paper, we introduce an\napproach built upon Semantic Web technologies for the realization of\nsemantic-enhanced OCED, with the aim to strengthen process data reasoning,\ninterconnect information sources, and boost expressiveness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u7f51\u6280\u672f\u7684\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u6570\u636e(OCED)\u8bed\u4e49\u589e\u5f3a\u65b9\u6cd5\uff0c\u65e8\u5728\u52a0\u5f3a\u8fc7\u7a0b\u6570\u636e\u63a8\u7406\u3001\u4e92\u8054\u4fe1\u606f\u6e90\u5e76\u63d0\u5347\u8868\u8fbe\u80fd\u529b", "motivation": "OCED\u4f5c\u4e3a\u9762\u5411\u4e8b\u4ef6\u548c\u5bf9\u8c61\u7684\u8fc7\u7a0b\u6570\u636e\u8bb0\u5f55\u5143\u6a21\u578b\uff0c\u9700\u8981\u89e3\u51b3\u6765\u81ea\u4e0d\u540c\u63d0\u4f9b\u8005\u7684\u6570\u636e\u96c6\u6210\u3001\u591a\u8fc7\u7a0b\u7ec4\u5408\u4ee5\u53ca\u77e5\u8bc6\u63a8\u7406\u589e\u5f3a\u7b49\u65b0\u6311\u6218", "method": "\u5229\u7528\u8bed\u4e49\u7f51\u6280\u672f\u521b\u5efa\u673a\u5668\u53ef\u8bfb\u7684OCED\u63cf\u8ff0\uff0c\u901a\u8fc7\u57fa\u4e8e\u672c\u4f53\u7684\u5173\u7cfb\u548c\u5b9e\u4f53\u5206\u7c7b\u8fdb\u884c\u4e30\u5bcc", "result": "\u5b9e\u73b0\u4e86\u8bed\u4e49\u589e\u5f3a\u7684OCED\u65b9\u6cd5\uff0c\u80fd\u591f\u52a0\u5f3a\u8fc7\u7a0b\u6570\u636e\u63a8\u7406\u80fd\u529b", "conclusion": "\u8bed\u4e49\u7f51\u6280\u672f\u80fd\u591f\u6709\u6548\u5b9e\u73b0OCED\u7684\u8bed\u4e49\u589e\u5f3a\uff0c\u4fc3\u8fdb\u8fc7\u7a0b\u4fe1\u606f\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u77e5\u8bc6\u63a8\u7406"}}
{"id": "2511.03620", "categories": ["cs.IR", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.03620", "abs": "https://arxiv.org/abs/2511.03620", "authors": ["Philipp Hager", "Onno Zoeter", "Maarten de Rijke"], "title": "CLAX: Fast and Flexible Neural Click Models in JAX", "comment": null, "summary": "CLAX is a JAX-based library that implements classic click models using modern\ngradient-based optimization. While neural click models have emerged over the\npast decade, complex click models based on probabilistic graphical models\n(PGMs) have not systematically adopted gradient-based optimization, preventing\npractitioners from leveraging modern deep learning frameworks while preserving\nthe interpretability of classic models. CLAX addresses this gap by replacing\nEM-based optimization with direct gradient-based optimization in a numerically\nstable manner. The framework's modular design enables the integration of any\ncomponent, from embeddings and deep networks to custom modules, into classic\nclick models for end-to-end optimization. We demonstrate CLAX's efficiency by\nrunning experiments on the full Baidu-ULTR dataset comprising over a billion\nuser sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster\nthan traditional EM approaches. CLAX implements ten classic click models,\nserving both industry practitioners seeking to understand user behavior and\nimprove ranking performance at scale and researchers developing new click\nmodels. CLAX is available at: https://github.com/philipphager/clax", "AI": {"tldr": "CLAX\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u5e93\uff0c\u4f7f\u7528\u73b0\u4ee3\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\u7ecf\u5178\u70b9\u51fb\u6a21\u578b\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684EM\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7ecf\u5178\u6982\u7387\u56fe\u6a21\u578b\u70b9\u51fb\u6a21\u578b\u672a\u80fd\u7cfb\u7edf\u91c7\u7528\u68af\u5ea6\u4f18\u5316\u7684\u95ee\u9898\uff0c\u8ba9\u4ece\u4e1a\u8005\u80fd\u591f\u5728\u4fdd\u6301\u7ecf\u5178\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u5229\u7528\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u6570\u503c\u7a33\u5b9a\u7684\u76f4\u63a5\u68af\u5ea6\u4f18\u5316\u66ff\u4ee3EM\u4f18\u5316\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u96c6\u6210\u5d4c\u5165\u3001\u6df1\u5ea6\u7f51\u7edc\u7b49\u7ec4\u4ef6\u5230\u7ecf\u5178\u70b9\u51fb\u6a21\u578b\u4e2d\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5728\u5305\u542b\u8d85\u8fc710\u4ebf\u7528\u6237\u4f1a\u8bdd\u7684Baidu-ULTR\u6570\u636e\u96c6\u4e0a\uff0c\u5355GPU\u7ea62\u5c0f\u65f6\u5b8c\u6210\u5b9e\u9a8c\uff0c\u6bd4\u4f20\u7edfEM\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "CLAX\u5b9e\u73b0\u4e8610\u4e2a\u7ecf\u5178\u70b9\u51fb\u6a21\u578b\uff0c\u4e3a\u884c\u4e1a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u70b9\u51fb\u6a21\u578b\u6846\u67b6\u3002"}}

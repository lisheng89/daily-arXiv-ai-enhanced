<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 15]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Atomic Information Flow: A Network Flow Model for Tool Attributions in RAG Systems](https://arxiv.org/abs/2602.04912)
*James Gao,Josh Zhou,Qi Sun,Ryan Huang,Steven Yoo*

Main category: cs.IR

TL;DR: 提出Atomic Information Flow (AIF)框架，将工具RAG系统分解为原子信息单元，通过图网络流模型实现细粒度溯源，并训练轻量级Gemma3模型进行信息压缩，显著提升关键信息识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于工具的检索增强生成(RAG)系统缺乏将最终响应追溯到特定工具组件的精确机制，这在系统扩展到复杂多智能体架构时成为关键缺陷。

Method: 提出原子信息流(AIF)图网络流模型，将工具输出和LLM调用分解为不可分割的自包含信息原子。基于最大流最小割定理，训练轻量级Gemma3(4B参数)模型作为上下文压缩器，利用AIF离线计算的流信号近似最小割。

Result: 基础Gemma3-4B模型在HotpotQA上识别关键信息准确率仅54.7%，略优于词法基线(BM25)。使用AIF信号微调后准确率提升至82.71%(+28.01点)，同时实现87.52%(+1.85%)的上下文token压缩，性能接近7倍大的Gemma3-27B变体。

Conclusion: AIF框架为工具RAG系统提供了细粒度溯源能力，通过轻量级模型训练实现了高效的信息压缩和关键信息识别，为AI可解释性提供了新的解决方案。

Abstract: Many tool-based Retrieval Augmented Generation (RAG) systems lack precise mechanisms for tracing final responses back to specific tool components -- a critical gap as systems scale to complex multi-agent architectures. We present \textbf{Atomic Information Flow (AIF)}, a graph-based network flow model that decomposes tool outputs and LLM calls into atoms: indivisible, self-contained units of information. By modeling LLM orchestration as a directed flow of atoms from tool and LLM nodes to a response super-sink, AIF enables granular attribution metrics for AI explainability.
  Motivated by the max-flow min-cut theorem in network flow theory, we train a lightweight Gemma3 (4B parameter) language model as a context compressor to approximate the minimum cut of tool atoms using flow signals computed offline by AIF. We note that the base Gemma3-4B model struggles to identify critical information with \textbf{54.7\%} accuracy on HotpotQA, barely outperforming lexical baselines (BM25). However, post-training on AIF signals boosts accuracy to \textbf{82.71\%} (+28.01 points) while achieving \textbf{87.52\%} (+1.85\%) context token compression -- bridging the gap with the Gemma3-27B variant, a model nearly $7\times$ larger.

</details>


### [2] [Scaling Laws for Embedding Dimension in Information Retrieval](https://arxiv.org/abs/2602.05062)
*Julian Killingback,Mahta Rafiee,Madine Manas,Hamed Zamani*

Main category: cs.IR

TL;DR: 本文对稠密检索中嵌入维度与检索性能的关系进行了全面分析，发现性能随维度增加呈幂律增长，但存在收益递减现象，且任务对齐程度影响性能可预测性。


<details>
  <summary>Details</summary>
Motivation: 随着稠密检索任务复杂度增加，基于单向量和内积的底层数据结构与相似度度量的局限性日益明显。理解嵌入维度如何影响检索性能对于构建平衡效果与效率的下一代检索模型至关重要。

Method: 使用两个模型系列和一系列模型尺寸进行综合实验，构建详细的嵌入缩放行为图景。通过实验数据分析嵌入维度与检索性能的关系，并推导缩放规律。

Result: 发现缩放行为符合幂律，可以仅根据嵌入维度推导性能缩放规律，以及考虑嵌入维度和模型尺寸的联合规律。对于与训练任务对齐的评估任务，性能随嵌入尺寸增加持续改善但收益递减；对于与训练任务对齐度较低的评估数据，性能可预测性较差，某些任务中较大嵌入维度会导致性能下降。

Conclusion: 本文提供了对嵌入局限性及其行为的深入理解，并为选择模型和嵌入维度以实现最佳性能同时降低存储和计算成本提供了实用指南。研究表明嵌入维度缩放存在复杂的行为模式，需要根据具体任务特性进行优化。

Abstract: Dense retrieval, which encodes queries and documents into a single dense vector, has become the dominant neural retrieval approach due to its simplicity and compatibility with fast approximate nearest neighbor algorithms. As the tasks dense retrieval performs grow in complexity, the fundamental limitations of the underlying data structure and similarity metric -- namely vectors and inner-products -- become more apparent. Prior recent work has shown theoretical limitations inherent to single vectors and inner-products that are generally tied to the embedding dimension. Given the importance of embedding dimension for retrieval capacity, understanding how dense retrieval performance changes as embedding dimension is scaled is fundamental to building next generation retrieval models that balance effectiveness and efficiency. In this work, we conduct a comprehensive analysis of the relationship between embedding dimension and retrieval performance. Our experiments include two model families and a range of model sizes from each to construct a detailed picture of embedding scaling behavior. We find that the scaling behavior fits a power law, allowing us to derive scaling laws for performance given only embedding dimension, as well as a joint law accounting for embedding dimension and model size. Our analysis shows that for evaluation tasks aligned with the training task, performance continues to improve as embedding size increases, though with diminishing returns. For evaluation data that is less aligned with the training task, we find that performance is less predictable, with performance degrading with larger embedding dimensions for certain tasks. We hope our work provides additional insight into the limitations of embeddings and their behavior as well as offers a practical guide for selecting model and embedding dimension to achieve optimal performance with reduced storage and compute costs.

</details>


### [3] [RAG without Forgetting: Continual Query-Infused Key Memory](https://arxiv.org/abs/2602.05152)
*Yuntong Hu,Sha Li,Naren Ramakrishnan,Liang Zhao*

Main category: cs.IR

TL;DR: ERM框架将瞬时的查询扩展增益转化为持久的检索改进，通过正确性门控反馈更新检索索引，实现零推理开销的持续学习


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在查询时进行扩展和迭代检索，但这些改进是瞬态的，每次查询都需要重新计算，无法累积学习，且重复产生推理成本。索引端方法虽然持久但依赖离线预处理或启发式更新，与下游任务效用对齐弱，导致语义漂移和噪声累积

Method: 提出Evolving Retrieval Memory (ERM)框架：1) 通过正确性门控反馈更新检索索引；2) 选择性地将原子扩展信号归因于受益的文档键；3) 通过稳定、范数有界的更新渐进演化键。理论证明查询扩展和键扩展在标准相似度函数下等价，并证明ERM选择性更新的收敛性

Result: 在BEIR和BRIGHT的13个领域实验中，ERM在检索和生成方面均取得一致提升，特别是在推理密集型任务上，同时保持原生检索速度

Conclusion: ERM成功将最优查询扩展摊销到稳定索引中，实现零推理时间开销的持续学习，解决了现有RAG系统无法累积学习和重复计算的问题

Abstract: Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.

</details>


### [4] [Semantic Search over 9 Million Mathematical Theorems](https://arxiv.org/abs/2602.05216)
*Luke Alexander,Eric Leonen,Sophie Szeto,Artemii Remizov,Ignacio Tejeda,Giovanni Inchiostro,Vasily Ilin*

Main category: cs.IR

TL;DR: 该论文构建了包含920万定理的数学定理检索系统，通过语义搜索显著提升了定理和论文检索效果


<details>
  <summary>Details</summary>
Motivation: 现有数学检索工具只能检索整篇论文，而数学家和定理证明代理通常需要查找特定的定理、引理或命题。在大型技术性数学文献库上的语义搜索效果尚不清楚

Method: 从arXiv等8个来源提取920万定理构建最大公开数学定理语料库，使用自然语言描述作为检索表示，系统分析表示上下文、语言模型选择、嵌入模型和提示策略对检索质量的影响

Result: 在专业数学家编写的定理搜索查询评估集上，该方法在定理级和论文级检索方面显著优于现有基线，证明语义定理搜索在web规模上是可行且有效的

Conclusion: 语义定理搜索在大型数学文献库中是可行的，构建的定理检索工具和数据集已公开可用，为数学研究和定理证明提供了重要支持

Abstract: Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of $9.2$ million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at \href{https://huggingface.co/spaces/uw-math-ai/theorem-search}{this link}, and the dataset is available at \href{https://huggingface.co/datasets/uw-math-ai/TheoremSearch}{this link}.

</details>


### [5] [NeuCLIRTech: Chinese Monolingual and Cross-Language Information Retrieval Evaluation in a Challenging Domain](https://arxiv.org/abs/2602.05334)
*Dawn Lawrie,James Mayfield,Eugene Yang,Andrew Yates,Sean MacAvaney,Ronak Pradeep,Scott Miller,Paul McNamee,Luca Soldaini*

Main category: cs.IR

TL;DR: NeuCLIRTech是一个用于跨语言技术信息检索的评估数据集，包含中文技术文档及其英文机器翻译版本，支持中文单语检索和英文查询的跨语言检索场景。


<details>
  <summary>Details</summary>
Motivation: 为了准确评估检索系统的进展，需要能够可靠区分系统性能的测试集。当前缺乏专门针对技术信息跨语言检索的高质量评估数据集。

Method: 构建了包含110个查询和35,962个文档相关性标注的数据集，结合了TREC NeuCLIR 2023和2024的主题，包含中文原生技术文档及其英文机器翻译版本。

Result: 创建了NeuCLIRTech数据集，支持两种检索场景：中文单语检索和英文查询的跨语言检索，提供了强大的统计区分能力，并包含基于神经检索系统的融合基线。

Conclusion: NeuCLIRTech为技术信息跨语言检索提供了高质量的评估资源，数据集已在Huggingface Datasets上发布，有助于推动检索算法的研究和比较。

Abstract: Measuring advances in retrieval requires test collections with relevance judgments that can faithfully distinguish systems. This paper presents NeuCLIRTech, an evaluation collection for cross-language retrieval over technical information. The collection consists of technical documents written natively in Chinese and those same documents machine translated into English. It includes 110 queries with relevance judgments. The collection supports two retrieval scenarios: monolingual retrieval in Chinese, and cross-language retrieval with English as the query language. NeuCLIRTech combines the TREC NeuCLIR track topics of 2023 and 2024. The 110 queries with 35,962 document judgments provide strong statistical discriminatory power when trying to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included so that developers of reranking algorithms are not reliant on BM25 as their first stage retriever. The dataset and artifacts are released on Huggingface Datasets

</details>


### [6] [Multi-Field Tool Retrieval](https://arxiv.org/abs/2602.05366)
*Yichen Tang,Weihang Su,Yiqun Liu,Qingyao Ai*

Main category: cs.IR

TL;DR: 提出多字段工具检索框架，解决LLM工具检索中的文档不完整、语义不匹配和多维度效用问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法将工具检索视为传统ad-hoc检索任务，匹配用户查询与原始工具文档，存在三个根本挑战：1) 工具文档不完整和结构不一致；2) 用户查询与技术工具文档之间的语义和粒度不匹配；3) 工具效用的多维度特性（功能、输入约束、输出格式等）。

Method: 提出多字段工具检索框架，通过细粒度、多字段建模来对齐用户意图与工具表示。该方法对工具的不同维度（功能、输入约束、输出格式等）进行单独建模，解决传统方法的局限性。

Result: 实验结果显示，该框架在五个数据集和一个混合基准测试中达到最先进的性能，表现出优异的泛化能力和鲁棒性。

Conclusion: 多字段工具检索框架有效解决了传统工具检索方法的三个核心挑战，通过细粒度多字段建模显著提升了工具检索的性能和实用性。

Abstract: Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness.

</details>


### [7] [Rich-Media Re-Ranker: A User Satisfaction-Driven LLM Re-ranking Framework for Rich-Media Search](https://arxiv.org/abs/2602.05408)
*Zihao Guo,Ligang Zhou,Zeyang Tang,Feicheng Li,Ying Nie,Zhiming Peng,Qingyun Sun,Jianxin Li*

Main category: cs.IR

TL;DR: 提出Rich-Media Re-Ranker框架，通过多维度细粒度建模提升搜索满意度，整合查询分析、视觉内容、多任务强化学习，在工业搜索系统中显著提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法存在两个主要局限：1) 对多面用户意图建模不足；2) 忽视丰富的侧信息（如视觉感知信号）。这限制了用户搜索满意度的提升。

Method: 1) Query Planner分析会话中的查询细化序列，将查询分解为互补子查询；2) 整合候选结果的丰富侧信息，包括VLM生成的视觉内容信号；3) 设计考虑内容相关性、质量、信息增益、新颖性和封面图像视觉呈现的重排序原则；4) LLM-based re-ranker基于这些原则和整合信号进行整体评估；5) 通过多任务强化学习增强VLM评估器和LLM重排序器的场景适应性。

Result: 实验表明该方法显著优于最先进的基线方法。该框架已部署在大型工业搜索系统中，在线用户参与率和满意度指标均有显著提升。

Conclusion: Rich-Media Re-Ranker框架通过多维度细粒度建模有效解决了现有重排序方法的局限性，整合了查询意图分析、视觉内容信号和多任务强化学习，显著提升了用户搜索满意度和系统性能。

Abstract: Re-ranking plays a crucial role in modern information search systems by refining the ranking of initial search results to better satisfy user information needs. However, existing methods show two notable limitations in improving user search satisfaction: inadequate modeling of multifaceted user intents and neglect of rich side information such as visual perception signals. To address these challenges, we propose the Rich-Media Re-Ranker framework, which aims to enhance user search satisfaction through multi-dimensional and fine-grained modeling. Our approach begins with a Query Planner that analyzes the sequence of query refinements within a session to capture genuine search intents, decomposing the query into clear and complementary sub-queries to enable broader coverage of users' potential intents. Subsequently, moving beyond primary text content, we integrate richer side information of candidate results, including signals modeling visual content generated by the VLM-based evaluator. These comprehensive signals are then processed alongside carefully designed re-ranking principle that considers multiple facets, including content relevance and quality, information gain, information novelty, and the visual presentation of cover images. Then, the LLM-based re-ranker performs the holistic evaluation based on these principles and integrated signals. To enhance the scenario adaptability of the VLM-based evaluator and the LLM-based re-ranker, we further enhance their capabilities through multi-task reinforcement learning. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines. Notably, the proposed framework has been deployed in a large-scale industrial search system, yielding substantial improvements in online user engagement rates and satisfaction metrics.

</details>


### [8] [SciDef: Automating Definition Extraction from Academic Literature with Large Language Models](https://arxiv.org/abs/2602.05413)
*Filip Kučera,Christoph Mandl,Isao Echizen,Radu Timofte,Timo Spinde*

Main category: cs.IR

TL;DR: SciDef是一个基于LLM的自动化定义提取管道，在科学文献中提取定义，使用多步和DSPy优化提示策略，在测试集上达到86.4%的提取准确率。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量激增，手动收集相关定义变得困难，需要自动化工具来提取科学文献中的定义。

Method: 开发了基于LLM的SciDef管道，使用多步提示和DSPy优化策略，在DefExtra和DefSim数据集上评估了16种语言模型，采用NLI-based方法作为评估指标。

Result: LLM能够从科学文献中提取86.4%的定义，多步和DSPy优化提示能提升性能，NLI-based评估方法最可靠，但模型倾向于过度生成定义。

Conclusion: LLM在定义提取方面表现良好，但未来工作应关注识别相关定义而非仅仅找到定义，因为模型容易过度生成。

Abstract: Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.
  Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.

</details>


### [9] [Forward Index Compression for Learned Sparse Retrieval](https://arxiv.org/abs/2602.05445)
*Sebastian Bruch,Martino Fontana,Franco Maria Nardini,Cosimo Rulli,Rossano Venturini*

Main category: cs.IR

TL;DR: 本文提出DotVByte算法，针对稀疏检索中的前向索引进行压缩优化，在保持检索质量和延迟的同时显著减少存储空间


<details>
  <summary>Details</summary>
Motivation: 稀疏检索已成为有效的搜索方法，但前向索引占用了大量存储空间。需要在不影响检索质量和计算延迟的情况下压缩前向索引

Method: 首先评估现有整数压缩技术，发现StreamVByte效果最佳，然后提出专门针对内积计算的DotVByte算法进行改进

Result: 在MsMarco数据集上实验显示，改进后的方法实现了显著的空间节省，同时保持了检索效率

Conclusion: DotVByte算法为稀疏检索的前向索引压缩提供了有效的解决方案，在存储效率、检索准确性和延迟之间取得了良好平衡

Abstract: Text retrieval using learned sparse representations of queries and documents has, over the years, evolved into a highly effective approach to search. It is thanks to recent advances in approximate nearest neighbor search-with the emergence of highly efficient algorithms such as the inverted index-based Seismic and the graph-based Hnsw-that retrieval with sparse representations became viable in practice. In this work, we scrutinize the efficiency of sparse retrieval algorithms and focus particularly on the size of a data structure that is common to all algorithmic flavors and that constitutes a substantial fraction of the overall index size: the forward index. In particular, we seek compression techniques to reduce the storage footprint of the forward index without compromising search quality or inner product computation latency. In our examination with various integer compression techniques, we report that StreamVByte achieves the best trade-off between memory footprint, retrieval accuracy, and latency. We then improve StreamVByte by introducing DotVByte, a new algorithm tailored to inner product computation. Experiments on MsMarco show that our improvements lead to significant space savings while maintaining retrieval efficiency.

</details>


### [10] [LMMRec: LLM-driven Motivation-aware Multimodal Recommendation](https://arxiv.org/abs/2602.05474)
*Yicheng Di,Zhanjie Zhang,Yun Wangc,Jinren Liue,Jiaqi Yanf,Jiyu Wei,Xiangyu Chend,Yuan Liu*

Main category: cs.IR

TL;DR: LMMRec是一个基于大语言模型的动机感知多模态推荐框架，通过链式思维提示提取细粒度动机，采用双编码器架构进行跨模态对齐，在三个数据集上实现最高4.98%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统通常将用户动机作为交互数据中的潜在变量处理，忽略了评论文本等多模态信息。在多模态动机融合中存在两个关键挑战：1) 在噪声中实现稳定的跨模态对齐；2) 识别反映相同底层动机的跨模态特征。

Method: 提出LMMRec框架：1) 使用链式思维提示从文本中提取细粒度用户和物品动机；2) 采用双编码器架构分别建模文本动机和交互动机；3) 通过动机协调策略和交互-文本对应方法，利用对比学习和动量更新缓解噪声和语义漂移问题。

Result: 在三个数据集上的实验表明，LMMRec实现了最高4.98%的性能提升，验证了该框架在动机感知多模态推荐中的有效性。

Conclusion: LMMRec成功解决了多模态动机融合中的跨模态对齐和特征识别挑战，通过大语言模型的语义先验和动机理解能力，提升了推荐系统的性能，为动机建模提供了新的解决方案。

Abstract: Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\% performance improvement.

</details>


### [11] [GLASS: A Generative Recommender for Long-sequence Modeling via SID-Tier and Semantic Search](https://arxiv.org/abs/2602.05663)
*Shiteng Cao,Junda She,Ji Liu,Bin Zeng,Chengcheng Guo,Kuo Cai,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Zhiheng Li,Cheng Yang*

Main category: cs.IR

TL;DR: GLASS是一个生成式推荐框架，通过SID-Tier和语义搜索整合长期用户兴趣，解决了生成式推荐系统难以建模长历史序列的问题。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统虽然具有变革性潜力，但在有效建模用户长期历史行为序列方面面临挑战。传统检索模型在处理大规模物品空间时存在困难，需要一种能够有效整合长期用户兴趣的方法。

Method: 1. SID-Tier模块：将长期交互映射为统一兴趣向量，增强初始SID令牌预测；利用语义码本的紧凑特性整合用户历史与候选语义代码的交叉特征。
2. 语义硬搜索：使用生成的粗粒度语义ID作为动态键提取相关历史行为，通过自适应门控融合模块重新校准后续细粒度令牌轨迹。
3. 针对数据稀疏性的策略：语义邻居增强和码本大小调整。

Result: 在TAOBAO-MM和KuaiRec两个大规模真实数据集上的实验表明，GLASS优于现有最先进的基线方法，在推荐质量上取得了显著提升。

Conclusion: GLASS通过有效整合长期用户兴趣到生成过程中，解决了生成式推荐系统在建模长历史序列方面的挑战，为生成式推荐研究提供了新的框架和思路。

Abstract: Leveraging long-term user behavioral patterns is a key trajectory for enhancing the accuracy of modern recommender systems. While generative recommender systems have emerged as a transformative paradigm, they face hurdles in effectively modeling extensive historical sequences. To address this challenge, we propose GLASS, a novel framework that integrates long-term user interests into the generative process via SID-Tier and Semantic Search. We first introduce SID-Tier, a module that maps long-term interactions into a unified interest vector to enhance the prediction of the initial SID token. Unlike traditional retrieval models that struggle with massive item spaces, SID-Tier leverages the compact nature of the semantic codebook to incorporate cross features between the user's long-term history and candidate semantic codes. Furthermore, we present semantic hard search, which utilizes generated coarse-grained semantic ID as dynamic keys to extract relevant historical behaviors, which are then fused via an adaptive gated fusion module to recalibrate the trajectory of subsequent fine-grained tokens. To address the inherent data sparsity in semantic hard search, we propose two strategies: semantic neighbor augmentation and codebook resizing. Extensive experiments on two large-scale real-world datasets, TAOBAO-MM and KuaiRec, demonstrate that GLASS outperforms state-of-the-art baselines, achieving significant gains in recommendation quality. Our codes are made publicly available to facilitate further research in generative recommendation.

</details>


### [12] [Evaluating the impact of word embeddings on similarity scoring in practical information retrieval](https://arxiv.org/abs/2602.05734)
*Niall McCarroll,Kevin Curran,Eugene McNamee,Angela Clist,Andrew Brammer*

Main category: cs.IR

TL;DR: 本文评估了一种替代的查询-陈述相似度测量方法，使用词移距离（WMD）结合词嵌入技术，相比传统的词嵌入质心相似度方法，在检索准确率上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于词嵌入质心的相似度测量方法可能无法充分捕捉查询和陈述之间的语义关联。受词移距离（WMD）模型的启发，研究者希望探索通过计算查询和陈述中单个词语之间的距离来评估相似性的替代方法，以更好地反映语义相关性。

Method: 提出了一种结合词移距离（WMD）和词嵌入技术的相似度排名方法。WMD通过计算查询和陈述中单个词语之间的距离来评估相似性，而不是使用词嵌入向量的质心。该方法使用预训练的词嵌入（如GloVe）来表示词语，然后在信息检索系统中应用WMD进行相似度计算和排名。

Result: WMD+GloVe组合在查询和响应陈述的排名任务中表现最佳，显著超越了所有其他最先进的检索模型，包括Doc2Vec和基线LSA模型。该方法在相似度排名准确率上取得了显著提升。

Conclusion: 基于大规模数据预训练的词嵌入结合WMD方法，能够产生领域无关的语言处理解决方案，可移植到多样化的商业应用场景中。这种方法在语义相似度测量方面具有优越性能。

Abstract: Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.
  This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.

</details>


### [13] [Bagging-Based Model Merging for Robust General Text Embeddings](https://arxiv.org/abs/2602.05787)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Wenbo Yang,Daiting Shi,Xueqi Cheng*

Main category: cs.IR

TL;DR: 该论文系统研究了文本嵌入模型的多任务训练策略，发现简单的批量级混洗效果最佳，但存在域外泛化不足和增量学习成本高的问题。作者提出Bagging-based rObust mOdel Merging (BROOM)方法，通过训练多个子集模型并合并来提升鲁棒性，同时支持高效的增量更新。


<details>
  <summary>Details</summary>
Motivation: 通用文本嵌入模型广泛应用于NLP和信息检索，通常通过大规模多任务语料训练以获得广泛泛化能力。然而，不同多任务训练策略的实际效果比较不明确，且随着新领域和数据类型的不断出现，如何高效适应嵌入模型成为挑战。现有方法在域外泛化和增量学习方面存在局限性。

Method: 论文从数据调度和模型合并两个角度系统研究多任务训练：比较批量级混洗、顺序训练变体、两阶段训练和多种合并粒度。为解决批量级混洗的局限性，提出Bagging-based rObust mOdel Merging (BROOM)方法：1）训练多个嵌入模型于采样子集；2）合并为单一模型以保持推理效率；3）支持高效增量更新：在新数据上训练轻量更新模型并与历史子集合并。

Result: 实验表明：1）批量级混洗在整体性能上表现最佳，说明任务冲突有限且训练数据互补；2）BROOM方法在多个嵌入基准测试中，相比全语料批量级混洗，持续提升域内和域外性能；3）在增量学习设置中，BROOM显著降低训练成本。

Conclusion: 批量级混洗是有效的多任务训练策略，但存在域外泛化和增量学习限制。BROOM方法通过模型合并和bagging策略解决了这些问题，在保持单模型推理效率的同时提升鲁棒性，并支持高效的增量更新，为文本嵌入模型的持续适应提供了实用解决方案。

Abstract: General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.

</details>


### [14] [AgenticTagger: Structured Item Representation for Recommendation with LLM Agents](https://arxiv.org/abs/2602.05945)
*Zhouhang Xie,Bo Peng,Zhankui He,Ziqi Chen,Alice Han,Isabella Ye,Benjamin Coleman,Noveen Sachdeva,Fernando Pereira,Julian McAuley,Wang-Cheng Kang,Derek Zhiyuan Cheng,Beidou Wang,Randolph Brown*

Main category: cs.IR

TL;DR: 提出AgenticTagger框架，通过LLM生成分层、低基数、高质量的描述符来改进推荐系统中的物品表示


<details>
  <summary>Details</summary>
Motivation: 高质量的表示对推荐系统至关重要，但现有的LLM描述符生成方法存在生成空间控制不足、基数高、性能低的问题，导致下游建模困难

Method: 提出AgenticTagger框架，包含两个核心阶段：(1)词汇构建阶段：通过多智能体反思机制，由架构师LLM迭代优化分层、低基数、高质量的描述符词汇表；(2)词汇分配阶段：LLM将词汇表中的描述符分配给物品

Result: 在公开和私有数据上的实验表明，AgenticTagger在多种推荐场景中带来一致改进，包括生成式和基于术语的检索、排序以及面向可控性的基于评论的推荐

Conclusion: AgenticTagger通过结构化描述符生成框架，解决了LLM生成描述符的基数控制问题，为推荐系统提供了更有效的物品表示方法

Abstract: High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation.

</details>


### [15] [SAGE: Benchmarking and Improving Retrieval for Deep Research Agents](https://arxiv.org/abs/2602.05975)
*Tiansheng Hu,Yilun Zhao,Canyu Zhang,Arman Cohan,Chen Zhao*

Main category: cs.IR

TL;DR: SAGE基准测试显示，现有深度研究代理在科学文献检索中表现不佳，BM25检索器优于LLM检索器约30%，通过文档增强框架可提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLM检索器能否有效支持深度研究代理工作流，特别是在科学文献检索任务中。

Method: 引入SAGE基准（1,200个查询，4个科学领域，20万篇论文），评估6个深度研究代理，比较BM25和LLM检索器，并提出基于LLM的文档增强框架。

Result: BM25显著优于LLM检索器约30%，现有代理生成关键词导向的子查询。文档增强框架在短式和开放式问题上分别带来8%和2%的性能提升。

Conclusion: LLM检索器在当前深度研究代理工作流中效果有限，需要改进检索策略，文档增强是提升检索性能的有效方法。

Abstract: Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.

</details>

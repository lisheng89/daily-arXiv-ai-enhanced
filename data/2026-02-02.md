<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning](https://arxiv.org/abs/2601.22498)
*Wei Yang,Rui Zhong,Yiqun Chen,Shixuan Li,Heng Ping,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: FITMM是一个基于频率感知信息论的多模态推荐框架，通过谱分解将模态分离到不同频带，减少冗余和对齐问题，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统主要在空间域融合模态，这会模糊信号的频率结构，放大模态间的错位和冗余问题。需要一种更有效的方法来处理多模态信号。

Method: 提出FITMM框架：1)构建图增强的物品表示；2)进行模态级谱分解获得正交频带；3)构建轻量级带内多模态组件；4)使用残差任务自适应门聚合频带；5)引入频率域信息瓶颈正则化控制冗余；6)添加跨模态谱一致性损失对齐频带。

Result: 在三个真实世界数据集上的广泛实验表明，FITMM始终且显著优于先进的基线方法。

Conclusion: 通过频率感知信息论视角处理多模态推荐问题，FITMM提供了一种分离后融合的范式，有效解决了模态错位和冗余问题，提升了推荐性能。

Abstract: Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.

</details>


### [2] [SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation](https://arxiv.org/abs/2601.22543)
*Ruiqi Zheng,Jinli Cao,Jiao Yin,Hongzhi Yin*

Main category: cs.IR

TL;DR: SCaLRec通过设备端语义校准模块，在无法实时调用云端LLM时，调整缓存的语义嵌入以适应用户最新交互，解决云端语义陈旧化问题


<details>
  <summary>Details</summary>
Motivation: 现有云-设备协同推荐系统面临云端语义陈旧化问题：由于无法为每个请求重新生成LLM语义嵌入，重用缓存的嵌入会导致与用户最新交互不一致，造成推荐质量下降

Method: 提出SCaLRec框架：1) 评估缓存语义在用户最新交互下的可靠性；2) 设备端语义校准模块，利用最新交互证据调整缓存语义嵌入，无需每次请求调用云端LLM

Result: 在真实世界数据集上的实验表明，SCaLRec在云端语义陈旧化情况下，相比强基线方法能持续提升推荐性能

Conclusion: SCaLRec有效解决了云-设备协同推荐中缓存语义陈旧化问题，通过设备端校准机制在不增加云端计算负担的情况下保持推荐质量

Abstract: Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.
  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.

</details>


### [3] [PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing](https://arxiv.org/abs/2601.22547)
*Shilong Zhao,Qinggang Yang,Zhiyi Yin,Xiaoshi Wang,Zhenxing Chen,Du Su,Xueqi Cheng*

Main category: cs.IR

TL;DR: 提出PersonaAct框架，通过基于真实行为轨迹训练的多模态智能体模拟短视频用户，用于审计推荐系统中的过滤气泡现象。


<details>
  <summary>Details</summary>
Motivation: 短视频平台依赖个性化推荐，引发过滤气泡担忧，但大规模审计面临挑战：真实用户研究成本高且涉及隐私，现有模拟器依赖文本信号且个性化弱，无法复现真实行为。

Method: 1) 通过结合行为分析和结构化提问的自动化访谈合成可解释的用户画像；2) 在多模态观察上使用监督微调和强化学习训练智能体；3) 部署智能体进行过滤气泡审计，从内容多样性（广度）和逃离潜力（深度）两个维度评估。

Result: 1) 相比通用LLM基线，PersonaAct在保真度上有显著提升，能复现真实行为；2) 交互过程中内容窄化现象明显；3) Bilibili平台展现出最强的逃离潜力。

Conclusion: PersonaAct框架能有效模拟短视频用户行为，为推荐系统审计提供可行方案，并发布了首个开源多模态短视频数据集和代码，支持可复现的推荐系统审计研究。

Abstract: Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.

</details>


### [4] [Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens](https://arxiv.org/abs/2601.22694)
*Zhen Zhao,Tong Zhang,Jie Xu,Qingliang Cai,Qile Zhang,Leyuan Yang,Daorui Xiao,Xiaojia Chang*

Main category: cs.IR

TL;DR: TRM框架用语义token替代item ID，解决了大规模排序系统中item ID嵌入训练和维护困难的问题，在减少存储的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 传统大规模排序系统依赖item ID，将每个item视为独立分类符号并映射到学习嵌入。随着item快速出现和消失，这些嵌入难以训练和维护，阻碍神经网络参数的有效学习并限制排序模型的可扩展性

Method: 提出TRM框架，用语义token替代item ID，改进token生成和应用流程。语义token相比item ID具有更好的扩展潜力

Result: TRM实现稀疏存储减少33%，AUC提升0.85%。在模型容量扩展时持续优于SOTA模型。在大规模个性化搜索引擎部署中，A/B测试显示用户活跃天数提升0.26%，查询变化率提升0.75%

Conclusion: 语义token比item ID具有更大的扩展潜力，TRM框架能有效解决大规模排序系统中item ID嵌入的稳定性问题，提升模型性能和可扩展性

Abstract: Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.

</details>


### [5] [Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval](https://arxiv.org/abs/2601.22783)
*Ilyass Moummad,Marius Miron,David Robinson,Kawtar Zaher,Hervé Goëau,Olivier Pietquin,Pierre Bonnet,Emmanuel Chemla,Matthieu Geist,Alexis Joly*

Main category: cs.IR

TL;DR: 提出紧凑超立方体嵌入用于快速文本驱动的野生动物观测检索，通过二进制表示实现大规模图像和音频数据库的高效搜索，显著降低内存和搜索成本。


<details>
  <summary>Details</summary>
Motivation: 大规模生物多样性监测平台依赖多模态野生动物观测，但现有基础模型的高维相似性搜索计算成本高，难以从海量档案中检索相关观测。

Method: 基于跨视图代码对齐哈希框架，将轻量级哈希扩展到多模态设置，在共享汉明空间中对齐自然语言描述与视觉/听觉观测。利用预训练的野生动物基础模型（BioCLIP和BioLingual），通过参数高效微调适应哈希任务。

Result: 在iNaturalist2024（文本到图像）和iNatSounds2024（文本到音频）等大规模基准测试中，离散超立方体嵌入相比连续嵌入实现竞争性甚至更优性能，同时大幅减少内存和搜索成本。哈希目标持续改进底层编码器表示，增强检索和零样本泛化能力。

Conclusion: 二进制、基于语言的检索方法为生物多样性监测系统提供了可扩展且高效的大规模野生动物档案搜索方案。

Abstract: Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.

</details>


### [6] [BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models](https://arxiv.org/abs/2601.22925)
*Weiqin Yang,Bohao Wang,Zhenxiang Xu,Jiawei Chen,Shengjia Zhang,Jingbang Chen,Canghong Jin,Can Wang*

Main category: cs.IR

TL;DR: BEAR提出一种新的微调目标，通过考虑beam search的行为来解决推荐系统中LLM训练与推理不一致的问题，确保正样本在beam search中不会被过早剪枝。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的推荐方法存在训练与推理不一致的问题：监督微调优化正样本的整体概率，但beam search的贪心剪枝机制可能导致高整体概率的正样本因前缀概率不足而被过早丢弃。

Method: 提出BEAR（Beam-SEarch-Aware Regularization）微调目标，强制正样本的每个token在每个解码步骤中排名都在前B个候选token内，这是一个宽松的必要条件，避免beam search中的错误剪枝。

Result: 在四个真实世界数据集上的广泛实验表明，BEAR显著优于强基线方法，且相比标准监督微调只带来可忽略的计算开销。

Conclusion: BEAR通过显式考虑beam search行为来解决训练-推理不一致问题，有效提升基于LLM的推荐系统性能，代码将在接受后发布。

Abstract: Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.
  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.

</details>


### [7] [OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning](https://arxiv.org/abs/2601.23085)
*Mohanna Hoveyda,Jelle Piepenbrock,Arjen P de Vries,Maarten de Rijke,Faegheh Hasibi*

Main category: cs.IR

TL;DR: OrLog是一个神经符号检索框架，将谓词级可能性估计与逻辑推理解耦，使用LLM提供原子谓词的可能性分数，然后通过概率推理引擎计算查询满足的后验概率，实现约束感知检索。


<details>
  <summary>Details</summary>
Motivation: 当前检索系统要么忽略查询中的逻辑约束，要么在生成推理过程中近似处理，导致不一致和不可靠。现有的神经符号方法虽然适合结构化推理，但局限于形式逻辑或数学问题，假设查询明确且证据完整，这在信息检索中很少满足。

Method: OrLog框架将谓词级可能性估计与逻辑推理解耦：1) LLM在一次无解码前向传递中为原子谓词提供可能性分数；2) 概率推理引擎根据这些分数推导查询满足的后验概率。

Result: 在多个骨干LLM、不同外部知识访问级别和各种逻辑约束下评估，OrLog显著提升了top-rank精度，特别是在析取查询上表现更好。效率方面，每个查询-实体对的平均token数减少了约90%。

Conclusion: 无生成的谓词可能性估计结合概率推理，能够实现约束感知检索，在性能上优于整体推理方法，同时使用更少的token，为复杂信息需求检索提供了有效解决方案。

Abstract: Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\sim$90\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.

</details>

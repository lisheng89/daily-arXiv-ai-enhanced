{"id": "2510.26095", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.26095", "abs": "https://arxiv.org/abs/2510.26095", "authors": ["Jingyuan He", "Jiongnan Liu", "Vishan Vishesh Oberoi", "Bolin Wu", "Mahima Jagadeesh Patel", "Kangrui Mao", "Chuning Shi", "I-Ta Lee", "Arnold Overwijk", "Chenyan Xiong"], "title": "ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Tests", "comment": "Accepted to NeurIPS 2025 Datasets & Benchmarks track", "summary": "Recommender systems are among the most impactful AI applications, interacting\nwith billions of users every day, guiding them to relevant products, services,\nor information tailored to their preferences. However, the research and\ndevelopment of recommender systems are hindered by existing datasets that fail\nto capture realistic user behaviors and inconsistent evaluation settings that\nlead to ambiguous conclusions. This paper introduces the Open Recommendation\nBenchmark for Reproducible Research with HIdden Tests (ORBIT), a unified\nbenchmark for consistent and realistic evaluation of recommendation models.\nORBIT offers a standardized evaluation framework of public datasets with\nreproducible splits and transparent settings for its public leaderboard.\nAdditionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,\nfeaturing web browsing sequences from 87 million public, high-quality webpages.\nClueWeb-Reco is a synthetic dataset derived from real, user-consented, and\nprivacy-guaranteed browsing data. It aligns with modern recommendation\nscenarios and is reserved as the hidden test part of our leaderboard to\nchallenge recommendation models' generalization ability. ORBIT measures 12\nrepresentative recommendation models on its public benchmark and introduces a\nprompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results\nreflect general improvements of recommender systems on the public datasets,\nwith variable individual performances. The results on the hidden test reveal\nthe limitations of existing approaches in large-scale webpage recommendation\nand highlight the potential for improvements with LLM integrations. ORBIT\nbenchmark, leaderboard, and codebase are available at\nhttps://www.open-reco-bench.ai."}
{"id": "2510.26104", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26104", "abs": "https://arxiv.org/abs/2510.26104", "authors": ["Zhaoqi Zhang", "Haolei Pei", "Jun Guo", "Tianyu Wang", "Yufei Feng", "Hui Sun", "Shaowei Liu", "Aixin Sun"], "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender", "comment": null, "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."}
{"id": "2510.26178", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26178", "abs": "https://arxiv.org/abs/2510.26178", "authors": ["Yanran Tang", "Ruihong Qiu", "Xue Li", "Zi Huang"], "title": "ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs", "comment": null, "summary": "Legal case retrieval (LCR) is a cornerstone of real-world legal decision\nmaking, as it enables practitioners to identify precedents for a given query\ncase. Existing approaches mainly rely on traditional lexical models and\npretrained language models to encode the texts of legal cases. Yet there are\nrich information in the relations among different legal entities as well as the\ncrucial reasoning process that uncovers how legal facts and legal issues can\nlead to judicial decisions. Such relational reasoning process reflects the\ndistinctive characteristics of each case that can distinguish one from another,\nmirroring the real-world judicial process. Naturally, incorporating such\ninformation into the precise case embedding could further enhance the accuracy\nof case retrieval. In this paper, a novel ReaKase-8B framework is proposed to\nleverage extracted legal facts, legal issues, legal relation triplets and legal\nreasoning for effective legal case retrieval. ReaKase-8B designs an in-context\nlegal case representation learning paradigm with a fine-tuned large language\nmodel. Extensive experiments on two benchmark datasets from COLIEE 2022 and\nCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings\nsubstantially improve retrieval performance over baseline models, highlighting\nthe potential of integrating legal reasoning into legal case retrieval systems.\nThe code has been released on https://github.com/yanran-tang/ReaKase-8B."}
{"id": "2510.26231", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26231", "abs": "https://arxiv.org/abs/2510.26231", "authors": ["Haochen Chen", "Qi Huang", "Anan Wu", "Wenhao Zhang", "Jianliang Ye", "Jianming Wu", "Kai Tan", "Xin Lu", "Xin Xu"], "title": "DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds", "comment": null, "summary": "Automatic structure elucidation is essential for self-driving laboratories as\nit enables the system to achieve truly autonomous. This capability closes the\nexperimental feedback loop, ensuring that machine learning models receive\nreliable structure information for real-time decision-making and optimization.\nHerein, we present DiSE, an end-to-end diffusion-based generative model that\nintegrates multiple spectroscopic modalities, including MS, 13C and 1H chemical\nshifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation\nof organic compounds. By learning inherent correlations among spectra through\ndata-driven approaches, DiSE achieves superior accuracy, strong generalization\nacross chemically diverse datasets, and robustness to experimental data despite\nbeing trained on calculated spectra. DiSE thus represents a significant advance\ntoward fully automated structure elucidation, with broad potential in natural\nproduct research, drug discovery, and self-driving laboratories."}
{"id": "2510.26407", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26407", "abs": "https://arxiv.org/abs/2510.26407", "authors": ["Ivan Razvorotnev", "Marina Munkhoeva", "Evgeny Frolov"], "title": "Barlow Twins for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation models must navigate sparse interaction data\npopularity bias and conflicting objectives like accuracy versus diversity While\nrecent contrastive selfsupervised learning SSL methods offer improved accuracy\nthey come with tradeoffs large batch requirements reliance on handcrafted\naugmentations and negative sampling that can reinforce popularity bias In this\npaper we introduce BT-SR a novel noncontrastive SSL framework that integrates\nthe Barlow Twins redundancyreduction principle into a Transformerbased nextitem\nrecommender BTSR learns embeddings that align users with similar shortterm\nbehaviors while preserving longterm distinctionswithout requiring negative\nsampling or artificial perturbations This structuresensitive alignment allows\nBT-SR to more effectively recognize emerging user intent and mitigate the\ninfluence of noisy historical context Our experiments on five public benchmarks\ndemonstrate that BTSR consistently improves nextitem prediction accuracy and\nsignificantly enhances longtail item coverage and recommendation calibration\nCrucially we show that a single hyperparameter can control the\naccuracydiversity tradeoff enabling practitioners to adapt recommendations to\nspecific application needs"}
{"id": "2510.26461", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26461", "abs": "https://arxiv.org/abs/2510.26461", "authors": ["Danial Ebrat", "Sepideh Ahmadian", "Luis Rueda"], "title": "Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering", "comment": null, "summary": "Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther."}
{"id": "2510.26546", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26546", "abs": "https://arxiv.org/abs/2510.26546", "authors": ["Min Hou", "Xin Liu", "Le Wu", "Chenyi He", "Hao Liu", "Zhi Li", "Xin Li", "Si Wei"], "title": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging", "comment": null, "summary": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks."}
{"id": "2510.26750", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.26750", "abs": "https://arxiv.org/abs/2510.26750", "authors": ["Martim Afonso", "Nuno Saavedra", "Bruno Lourenço", "Alexandra Mendes", "João Ferreira"], "title": "ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews", "comment": "4 pages, 1 Figure, 2 tables", "summary": "Systematic reviews and mapping studies are critical for synthesizing\nresearch, identifying gaps, and guiding future work, but they are often\nlabor-intensive and time-consuming. Existing tools provide partial support for\nspecific steps, leaving much of the process manual and error-prone. We present\nProfOlaf, a semi-automated tool designed to streamline systematic reviews while\nmaintaining methodological rigor. ProfOlaf supports iterative snowballing for\narticle collection with human-in-the-loop filtering and uses large language\nmodels to assist in analyzing articles, extracting key topics, and answering\nqueries about the content of papers. By combining automation with guided manual\neffort, ProfOlaf enhances the efficiency, quality, and reproducibility of\nsystematic reviews across research fields. A video describing and demonstrating\nProfOlaf is available at: https://youtu.be/4noUXfcmxsE"}

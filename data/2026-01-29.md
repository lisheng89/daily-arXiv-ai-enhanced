<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation](https://arxiv.org/abs/2601.20083)
*Lee Xiong,Zhirong Chen,Rahul Mayuranath,Shangran Qiu,Arda Ozdemir,Lu Li,Yang Hu,Dave Li,Jingtao Ren,Howard Cheng,Fabian Souto Herrera,Ahmed Agiza,Baruch Epshtein,Anuj Aggarwal,Julia Ulziisaikhan,Chao Wang,Dinesh Ramasamy,Parshva Doshi,Sri Reddy,Arnold Overwijk*

Main category: cs.IR

TL;DR: LLaTTE：用于广告推荐的规模化Transformer架构，通过两阶段设计在严格延迟约束下实现LLM式的幂律扩展，在Meta部署带来4.3%转化提升


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的序列建模遵循类似LLM的可预测幂律扩展规律，但需要解决在严格延迟约束下实现规模化的问题，同时发现语义特征是扩展的前提条件

Method: 提出LLaTTE架构，采用两阶段设计：将大型长上下文模型的重计算卸载到异步上游用户模型，下游排序任务可预测地受益于上游改进

Result: 部署为Meta最大的用户模型，在Facebook Feed和Reels上实现4.3%的转化提升，服务开销最小，验证了工业推荐系统中扩展定律的实用性

Conclusion: 为工业推荐系统利用扩展定律提供了实用蓝图，证明语义特征是扩展前提，两阶段架构可在延迟约束下实现可预测的规模化改进

Abstract: We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.

</details>


### [2] [IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation](https://arxiv.org/abs/2601.20084)
*Yash Saxena,Ankur Padia,Kalpa Gunaratna,Manas Gaur*

Main category: cs.IR

TL;DR: IMRNNs是一种轻量级框架，通过动态双向调制增强稠密检索器的可解释性和检索效果，在推理时使用两个独立适配器分别基于查询调整文档嵌入和基于检索文档反馈调整查询嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决黑盒稠密检索器在RAG系统中缺乏可解释性的问题。现有方法使用静态嵌入，掩盖了查询和文档之间的双向语义关系，而事后重排序方法计算成本高且无法揭示底层语义对齐。

Method: 提出IMRNNs框架，包含两个独立适配器：一个基于当前查询条件化文档嵌入，另一个使用初始检索文档的语料级反馈精化查询嵌入。通过迭代调制过程动态调整表示，暴露查询和文档之间的可解释语义依赖关系。

Result: 在7个基准数据集上，将IMRNNs应用于标准稠密检索器，相比最先进基线平均提升：nDCG +6.35%、召回率 +7.14%、MRR +7.04%。同时增强了检索系统的可解释性。

Conclusion: 引入可解释性驱动的调制机制既能解释又能增强RAG系统中的检索性能，证明了可解释性和检索效果可以协同提升。

Abstract: Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.

</details>


### [3] [Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms](https://arxiv.org/abs/2601.20131)
*Deep Shah,Sanket Badhe,Nehal Kathrotia*

Main category: cs.IR

TL;DR: 论文提出了一个四层框架来系统化设计嵌入检索系统，通过表示层、粒度层、编排层和鲁棒性层来平衡效率与效果之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 设计嵌入检索系统需要在效率与效果之间进行复杂的权衡决策，目前缺乏系统化的设计框架来指导这些决策。

Method: 提出垂直遍历系统设计栈的四层框架：表示层（损失函数和架构）、粒度层（分割策略）、编排层（超越单向量范式的方法）、鲁棒性层（架构缓解措施）。

Result: 通过分类这些限制和设计选择，为从业者提供了一个全面的框架，用于优化现代神经搜索系统中效率与效果的前沿。

Conclusion: 该四层框架为嵌入检索系统的设计提供了系统化的方法论，帮助实践者在复杂的效率-效果权衡空间中做出明智的设计决策。

Abstract: Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.

</details>


### [4] [MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation](https://arxiv.org/abs/2601.20199)
*Jing Yan,Yimeng Bai,Zongyu Liu,Yahui Liu,Junwei Wang,Jingze Huang,Haoda Li,Sihao Ding,Shaohui Ruan,Yang Zhang*

Main category: cs.IR

TL;DR: MERGE提出了一种新的物品索引范式，通过自适应构建聚类、动态监控聚类占用率以及细到粗的层次合并，解决了传统VQ方法在流式推荐系统中处理高度偏斜和非平稳物品分布的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于向量量化(VQ)的物品索引方法在处理流式工业推荐系统中常见的高度偏斜和非平稳物品分布时，面临分配精度差、聚类占用不平衡和聚类分离不足的挑战，需要更有效的索引解决方案。

Method: MERGE采用自适应从零开始构建聚类、动态监控聚类占用率，并通过细到粗的层次合并形成分层索引结构的新范式。

Result: 实验表明MERGE在分配精度、聚类均匀性和聚类分离度方面显著优于现有索引方法，在线A/B测试显示关键业务指标有实质性提升。

Conclusion: MERGE有潜力成为大规模推荐系统的基础索引方法，解决了传统VQ方法在流式推荐环境中的局限性。

Abstract: Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.

</details>


### [5] [Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation](https://arxiv.org/abs/2601.20215)
*Na Li,Jiaqi Yu,Minzhi Xie,Tiantian He,Xiaoxiao Xu,Zixiu Wang,Lantao Hu,Yongqi Liu,Han Li,Kaiqiao Zhan,Kun Gai*

Main category: cs.IR

TL;DR: EASQ框架通过问卷反馈实现推荐系统的端到端用户满意度对齐，解决了稀疏满意度信号被海量行为数据淹没的问题。


<details>
  <summary>Details</summary>
Motivation: 传统短视频推荐系统依赖点击、观看时长等行为信号作为用户满意度的间接代理，但这些信号存在噪声和偏差。问卷收集的显式满意度反馈质量高但极其稀疏，难以融入实时推荐模型。

Method: 1) 构建独立参数通路：结合多任务架构和轻量级LoRA模块，分离稀疏满意度监督与密集行为信号；2) DPO优化目标：为主模型输出与稀疏满意度信号提供实时对齐；3) 端到端在线学习：模型持续适应新问卷反馈，同时保持主干稳定性。

Result: 离线和在线A/B测试表明，EASQ在多个场景下持续提升用户满意度指标。已在生产短视频推荐系统中成功部署，带来显著且稳定的业务收益。

Conclusion: EASQ框架有效解决了稀疏满意度信号融入实时推荐系统的挑战，通过参数隔离和实时对齐实现了用户满意度的端到端优化，为推荐系统提供了高质量的直接监督信号。

Abstract: Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.

</details>


### [6] [MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation](https://arxiv.org/abs/2601.20234)
*Qihang Yu,Kairui Fu,Zhaocheng Du,Yuxuan Si,Kaiyuan Li,Weihao Zhao,Zhicheng Zhang,Jieming Zhu,Quanyu Dai,Zhenhua Dong,Shengyu Zhang,Kun Kuang,Fei Wu*

Main category: cs.IR

TL;DR: MALLOC是一个用于内存感知长序列压缩的基准测试，旨在解决大规模推荐系统中因用户行为序列长依赖关系带来的巨大内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 随着推荐模型规模的扩大，计算成本急剧增加，特别是在处理用户行为的长序列依赖时。现有方法虽然通过预存储用户历史行为中间状态来减少计算开销，但忽视了由此带来的巨大内存空间成本，这在拥有数十亿用户、每个用户可能有数千次交互的真实推荐系统中成为关键挑战。

Method: 提出了MALLOC基准测试框架，系统性地分类和整合了适用于大规模序列推荐的内存管理技术，并将这些技术集成到最先进的推荐模型中，创建了一个可复现、易访问的评估平台。

Result: 通过广泛的实验评估，在准确性、效率和复杂性等多个维度上验证了MALLOC在推进大规模推荐系统发展方面的全面可靠性。

Conclusion: MALLOC填补了推荐系统中内存管理技术评估的空白，为大规模序列推荐的内存压缩问题提供了系统性的解决方案和评估基准，有助于推动更高效的大规模推荐系统发展。

Abstract: The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.

</details>


### [7] [One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking](https://arxiv.org/abs/2601.20283)
*Tanmay Karmakar,Sourav Saha,Debapriyo Majumdar,Surjyanee Halder*

Main category: cs.IR

TL;DR: 针对神经排序模型的单词语义攻击：通过插入或替换一个与查询语义对齐的"查询中心词"，就能显著提升目标文档排名，攻击成功率高达91%，平均每文档修改不到2个token。


<details>
  <summary>Details</summary>
Motivation: 虽然神经排序模型在检索效果上表现优异，但先前研究表明它们容易受到对抗性扰动攻击。本文重新审视这一鲁棒性问题，探索最小化的查询感知攻击方法。

Method: 提出了一种最小化的查询感知攻击方法，通过插入或替换单个语义对齐的"查询中心词"来提升目标文档排名。研究了启发式和梯度引导的变体，包括识别有影响力插入点的白盒方法。在TREC-DL 2019/2020数据集上使用BERT和monoT5重新排序器进行测试。

Result: 单词语义攻击在BERT和monoT5重新排序器上达到91%的成功率，平均每文档修改不到2个token。在可比的白盒设置下，以更少的编辑实现了竞争性的排名和分数提升。分析发现存在一个"Goldilocks区域"，中等排名的文档最为脆弱。

Conclusion: 这些发现展示了神经排序模型的实际风险，并激励未来开发更鲁棒的防御机制。研究还引入了新的诊断指标来分析攻击敏感性，超越了聚合成功率。

Abstract: Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.

</details>


### [8] [Less is More: Benchmarking LLM Based Recommendation Agents](https://arxiv.org/abs/2601.20316)
*Kargi Chauhan,Mahalakshmi Venkateswarlu*

Main category: cs.IR

TL;DR: LLM推荐系统中，增加用户历史长度（5-50项）并不会显著提升推荐质量，使用5-10项历史即可节省88%推理成本而不损失质量。


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM推荐系统中"更多上下文更好"的普遍假设，验证增加用户购买历史长度是否真的能提升推荐质量。

Method: 在REGEN数据集上，对GPT-4o-mini、DeepSeek-V3、Qwen2.5-72B和Gemini 2.5 Flash四种SOTA LLM进行系统基准测试，使用50名用户的受试者内设计，比较5-50项不同上下文长度下的表现。

Result: 所有模型在不同上下文长度（5-50项）下的质量评分保持稳定（0.17-0.23），没有显著改善。使用5-10项历史相比50项可节省约88%推理成本而不损失质量。

Conclusion: LLM推荐系统中"更多上下文更好"的范式被挑战，使用较短上下文（5-10项）即可实现成本效益高的推荐系统，为实际部署提供指导。

Abstract: Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.
  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.

</details>


### [9] [Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval](https://arxiv.org/abs/2601.20391)
*Zhuocheng Zhang,Kangheng Liang,Guanxuan Li,Paul Henderson,Richard Mccreadie,Zijun Long*

Main category: cs.IR

TL;DR: 提出了DMCL框架，通过扩散感知的多视图对比学习解决扩散增强交互式文本到图像检索中的幻觉问题，显著提升检索性能


<details>
  <summary>Details</summary>
Motivation: 扩散增强的交互式文本到图像检索(DAI-TIR)通过扩散模型生成查询图像作为用户意图的额外"视图"，但扩散生成可能引入与原始查询文本冲突的幻觉视觉线索，这会显著降低检索性能

Method: 提出扩散感知多视图对比学习(DMCL)框架，通过语义一致性和扩散感知对比目标，对齐文本和扩散生成的查询视图，同时抑制幻觉查询信号，使编码器充当语义过滤器

Result: 在五个标准基准测试中，DMCL在多轮Hits@10指标上持续改进，最高提升7.37%，超过先前微调和零样本基线，注意力可视化和几何嵌入空间分析证实了过滤行为

Conclusion: DMCL是一个通用且鲁棒的DAI-TIR训练框架，能够有效抑制扩散模型引入的幻觉线索，更好地表示用户意图，提高检索性能

Abstract: Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.

</details>


### [10] [When Vision Meets Texts in Listwise Reranking](https://arxiv.org/abs/2601.20623)
*Hongyi Cai*

Main category: cs.IR

TL;DR: Rank-Nexus：一个轻量级多模态图像-文本文档重排器，通过渐进式跨模态训练策略解决模态鸿沟问题，在仅使用2B参数的情况下在文本和图像重排基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息检索中视觉与文本信息整合潜力巨大，但图像-文本文档重排面临模态鸿沟和对齐数据稀缺的挑战。现有方法依赖大型模型（7B-32B参数）和推理蒸馏，计算开销大且主要关注文本模态。

Method: 提出Rank-Nexus多模态图像-文本文档重排器，采用列表式定性重排。引入渐进式跨模态训练策略：1）分别训练模态：利用丰富文本重排数据蒸馏到文本分支；2）对稀缺图像数据，从MLLM在图像检索基准上生成标题构建蒸馏对；3）蒸馏联合图像-文本重排数据集。

Result: 在文本重排基准（TREC, BEIR）和挑战性图像重排基准（INQUIRE, MMDocIR）上取得优异性能，仅使用轻量级2B预训练视觉语言模型。高效设计确保在多样化多模态场景中的强泛化能力，无需过多参数或推理开销。

Conclusion: Rank-Nexus通过渐进式跨模态训练有效解决模态鸿沟问题，以轻量级架构实现强大的多模态重排性能，为高效多模态信息检索提供了新思路。

Abstract: Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.

</details>


### [11] [Overview of the TREC 2025 Tip-of-the-Tongue track](https://arxiv.org/abs/2601.20671)
*Jaime Arguello,Fernando Diaz,Maik Fröebe,To Eun Kim,Bhaskar Mitra*

Main category: cs.IR

TL;DR: TREC 2025 ToT 跟踪扩展了领域范围，整合了来自多个来源的测试查询，共有9个团队提交了32个运行结果。


<details>
  <summary>Details</summary>
Motivation: 舌尖现象（ToT）已知项检索涉及重新查找用户无法可靠回忆标识符的项目。ToT信息请求通常冗长且包含多种复杂现象，这对现有信息检索系统提出了特别挑战。

Method: 扩展TREC ToT跟踪到通用领域，整合了来自三个不同来源的测试查询：MS-ToT数据集、手动主题开发和基于LLM的合成查询生成。

Result: 共有9个团队（包括跟踪协调者）提交了32个运行结果。

Conclusion: TREC 2025 ToT跟踪通过扩展领域范围和整合多样化查询来源，为研究舌尖现象检索提供了更全面的评估框架。

Abstract: Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.

</details>


### [12] [MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature](https://arxiv.org/abs/2601.20709)
*Huan He,Xueqing Peng,Yutong Xie,Qijia Liu,Chia-Hsuan Chang,Lingfei Qian,Brian Ondov,Qiaozhu Mei,Hua Xu*

Main category: cs.IR

TL;DR: MedViz是一个结合多AI代理与交互式可视化的生物医学文献分析系统，通过语义地图和智能代理功能支持大规模文献探索与知识发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学研究者面临海量多样化文献的导航挑战，传统搜索引擎仅提供排名文本列表，缺乏全局探索和深度分析支持。虽然生成式AI和大型语言模型在摘要、提取和问答方面有潜力，但其对话式实现与文献搜索工作流整合不足。

Method: MedViz整合多个AI代理与交互式可视化，结合数百万篇文章的语义地图，提供查询、摘要和假设生成等代理驱动功能，支持研究者迭代式问题精炼、趋势识别和隐藏连接发现。

Result: MedViz将生物医学文献搜索转变为动态的探索过程，通过智能代理与交互式可视化的结合，加速知识发现。

Conclusion: MedViz通过整合AI代理与可视化技术，解决了生物医学文献探索中的关键挑战，为研究者提供了更有效的知识发现工具。

Abstract: Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.

</details>

{"id": "2512.19958", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.19958", "abs": "https://arxiv.org/abs/2512.19958", "authors": ["Sneha Oommen", "Gabby Sanchez", "Cassandra T. Britto", "Di Wang", "Jordan Chiou", "Maria Spichkova"], "title": "Towards Analysing Invoices and Receipts with Amazon Textract", "comment": null, "summary": "This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.", "AI": {"tldr": "\u8bc4\u4f30AWS Textract\u5728\u6536\u636e\u6570\u636e\u63d0\u53d6\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u683c\u5f0f\u548c\u6761\u4ef6\u4e0b\u7684\u6536\u636e\u5904\u7406\u80fd\u529b\uff0c\u8bc6\u522b\u5178\u578b\u95ee\u9898\u5e76\u63d0\u51fa\u7f13\u89e3\u7b56\u7565", "motivation": "\u8bc4\u4f30AWS Textract\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5904\u7406\u6536\u636e\u6570\u636e\u63d0\u53d6\u7684\u5b9e\u9645\u6548\u679c\uff0c\u4e86\u89e3\u8be5\u670d\u52a1\u5728\u4e0d\u540c\u683c\u5f0f\u548c\u8d28\u91cf\u6536\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53c2\u8003", "method": "\u4f7f\u7528\u5305\u542b\u591a\u79cd\u683c\u5f0f\u548c\u6761\u4ef6\u7684\u6536\u636e\u6570\u636e\u96c6\uff0c\u5bf9AWS Textract\u529f\u80fd\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u5728\u6536\u636e\u6570\u636e\u63d0\u53d6\u4e2d\u7684\u8868\u73b0", "result": "Textract\u5728\u63d0\u53d6\u6536\u636e\u603b\u989d\u65b9\u9762\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u5b58\u5728\u53d7\u56fe\u50cf\u8d28\u91cf\u548c\u5e03\u5c40\u5f71\u54cd\u7684\u5178\u578b\u95ee\u9898\u548c\u5f02\u5e38\u60c5\u51b5\uff0c\u9700\u8981\u9488\u5bf9\u6027\u5904\u7406", "conclusion": "AWS Textract\u5728\u6536\u636e\u6570\u636e\u63d0\u53d6\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4f46\u56fe\u50cf\u8d28\u91cf\u548c\u5e03\u5c40\u4f1a\u5f71\u54cd\u63d0\u53d6\u6548\u679c\uff0c\u9700\u8981\u91c7\u7528\u9002\u5f53\u7684\u7f13\u89e3\u7b56\u7565\u6765\u5e94\u5bf9\u8bc6\u522b\u95ee\u9898"}}
{"id": "2512.19983", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.19983", "abs": "https://arxiv.org/abs/2512.19983", "authors": ["Ziyuan Guo", "Jie Guo", "Zhenghao Chen", "Bin Song", "Fei Richard Yu"], "title": "IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation", "comment": "12 pages, 6 figures. This paper has been accepted for publication in IEEE Transactions on Multimedia. The final published version will be available via IEEE Xplore", "summary": "Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \\LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.", "AI": {"tldr": "IGDMRec\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u884c\u4e3a\u4fe1\u606f\u6765\u53bb\u566a\u8bed\u4e49\u7269\u54c1\u56fe\uff0c\u63d0\u5347\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u6784\u7684\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u7269\u54c1\u56fe\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u56fe\u5b58\u5728\u566a\u58f0\u95ee\u9898\uff1a1\uff09\u591a\u6a21\u6001\u4fe1\u606f\u56fa\u6709\u566a\u58f0\uff1b2\uff09\u7269\u54c1\u8bed\u4e49\u4e0e\u7528\u6237-\u7269\u54c1\u5171\u73b0\u5173\u7cfb\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4\u865a\u5047\u94fe\u63a5\u548c\u6b21\u4f18\u63a8\u8350", "method": "\u63d0\u51faIGDMRec\u65b9\u6cd5\uff1a1\uff09\u884c\u4e3a\u6761\u4ef6\u56fe\u6269\u6563\u6a21\u5757\uff0c\u5c06\u4ea4\u4e92\u6570\u636e\u4f5c\u4e3a\u6761\u4ef6\u4fe1\u606f\u6307\u5bfc\u8bed\u4e49\u7269\u54c1\u56fe\u53bb\u566a\uff1b2\uff09\u6761\u4ef6\u53bb\u566a\u7f51\u7edc\u5b9e\u73b0\u53ef\u7ba1\u7406\u590d\u6742\u5ea6\u7684\u53bb\u566a\u8fc7\u7a0b\uff1b3\uff09\u5bf9\u6bd4\u8868\u793a\u589e\u5f3a\u65b9\u6848\uff0c\u5229\u7528\u53bb\u566a\u56fe\u548c\u539f\u59cb\u56fe\u589e\u5f3a\u7269\u54c1\u8868\u793a", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eIGDMRec\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\uff0c\u9c81\u68d2\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u53bb\u566a\u80fd\u529b\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5173\u952e\u7ec4\u4ef6\u7684\u6709\u6548\u6027", "conclusion": "IGDMRec\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6574\u5408\u7528\u6237\u884c\u4e3a\u4fe1\u606f\u6709\u6548\u53bb\u566a\u8bed\u4e49\u7269\u54c1\u56fe\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd"}}
{"id": "2512.20022", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20022", "abs": "https://arxiv.org/abs/2512.20022", "authors": ["Kian Godhwani", "David Benrimoh"], "title": "LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews", "comment": null, "summary": "Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.\n  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.\n  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.\n  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.", "AI": {"tldr": "OLIVER\u662f\u4e00\u4e2a\u7528\u4e8eLLM\u8f85\u52a9\u6587\u732e\u7b5b\u9009\u7684\u5f00\u6e90\u7ba1\u9053\uff0c\u8bc4\u4f30\u663e\u793a\u5355\u6a21\u578b\u6027\u80fd\u53d7\u7efc\u8ff0\u7279\u5f81\u548c\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u5927\u4e14\u6821\u51c6\u5dee\uff0c\u800c\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u8d28\u91cf\u548c\u6821\u51c6\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8f85\u52a9\u6587\u732e\u7b5b\u9009\u7814\u7a76\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4e3b\u8981\u8bc4\u4f30\u65e9\u671f\u6a21\u578b\u3001\u6807\u51c6\u5316Cochrane\u7efc\u8ff0\u3001\u5355\u6a21\u578b\u8bbe\u7f6e\uff0c\u4e14\u4ee5\u51c6\u786e\u6027\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6cdb\u5316\u6027\u3001\u914d\u7f6e\u6548\u679c\u548c\u6821\u51c6\u7684\u5168\u9762\u8003\u5bdf\u3002", "method": "\u5f00\u53d1OLIVER\u5f00\u6e90\u7ba1\u9053\uff0c\u8bc4\u4f30\u591a\u4e2a\u5f53\u4ee3LLM\u5728\u4e24\u4e2a\u975eCochrane\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u51c6\u786e\u6027\u3001AUC\u548c\u6821\u51c6\u6307\u6807\uff0c\u5e76\u6d4b\u8bd5\u7ed3\u5408\u4e24\u4e2a\u8f7b\u91cf\u6a21\u578b\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b5b\u9009\u6846\u67b6\u53ca\u4e09\u79cd\u805a\u5408\u89c4\u5219\u3002", "result": "\u5355\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u5927\uff1a\u5c0f\u7efc\u8ff0\u4e2d\u6a21\u578b\u654f\u611f\u6027\u9ad8\u4f46\u5047\u9633\u6027\u591a\u3001\u6821\u51c6\u5dee\uff1b\u5927\u7efc\u8ff0\u4e2d\u7279\u5f02\u6027\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u53ec\u56de\u3002\u5355\u6a21\u578b\u6821\u51c6\u666e\u904d\u5f31\u3002\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u663e\u8457\u63d0\u5347\u533a\u5206\u5ea6\u548c\u6821\u51c6\uff0cAUC\u66f4\u9ad8\u3002", "conclusion": "LLM\u6709\u6f5c\u529b\u52a0\u901f\u6587\u732e\u7b5b\u9009\uff0c\u4f46\u5355\u6a21\u578b\u6027\u80fd\u53d7\u7efc\u8ff0\u7279\u5f81\u548c\u63d0\u793a\u8bbe\u8ba1\u5f71\u54cd\u5927\u4e14\u6821\u51c6\u6709\u9650\u3002\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\u80fd\u63d0\u5347\u5206\u7c7b\u8d28\u91cf\u548c\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u5927\u89c4\u6a21\u7b5b\u9009\u3002"}}
{"id": "2512.20034", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20034", "abs": "https://arxiv.org/abs/2512.20034", "authors": ["Xian Wu", "Ming Zhang", "Zhiyu Fang", "Fei Li", "Bin Wang", "Yong Jiang", "Hao Zhou"], "title": "VSA:Visual-Structural Alignment for UI-to-Code", "comment": null, "summary": "The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \\textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.", "AI": {"tldr": "VSA\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u8303\u5f0f\uff0c\u901a\u8fc7\u89c6\u89c9-\u7ed3\u6784\u5bf9\u9f50\u5408\u6210\u6709\u7ec4\u7ec7\u7684\u524d\u7aef\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u65b9\u6cd5\u751f\u6210\u975e\u7ed3\u6784\u5316\u4ee3\u7801\u3001\u7f3a\u4e4f\u7ec4\u4ef6\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u65b9\u6cd5\u4e3b\u8981\u751f\u6210\u975e\u7ed3\u6784\u5316\u3001\u6241\u5e73\u5316\u7684\u4ee3\u7801\u5e93\uff0c\u7f3a\u4e4f\u4e0eReact\u6216Angular\u7b49\u7ec4\u4ef6\u5316\u5e93\u7684\u517c\u5bb9\u6027\uff0c\u5bfc\u81f4\u4ee3\u7801\u4f4e\u5185\u805a\u3001\u9ad8\u8026\u5408\uff0c\u957f\u671f\u7ef4\u62a4\u56f0\u96be\u3002", "method": "1) \u4f7f\u7528\u7a7a\u95f4\u611f\u77e5transformer\u5c06\u89c6\u89c9\u8f93\u5165\u91cd\u6784\u4e3a\u5c42\u6b21\u6811\u8868\u793a\uff1b2) \u96c6\u6210\u7b97\u6cd5\u6a21\u5f0f\u5339\u914d\u5c42\u8bc6\u522b\u91cd\u590dUI\u6a21\u5f0f\u5e76\u5c01\u88c5\u4e3a\u6a21\u5757\u5316\u6a21\u677f\uff1b3) \u901a\u8fc7\u6a21\u5f0f\u9a71\u52a8\u7684\u5408\u6210\u5f15\u64ce\u5904\u7406\u6a21\u677f\uff0c\u786e\u4fddLLM\u751f\u6210\u7c7b\u578b\u5b89\u5168\u3001\u652f\u6301\u5c5e\u6027\u4f20\u9012\u7684\u751f\u4ea7\u7ea7\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4ee3\u7801\u6a21\u5757\u5316\u548c\u67b6\u6784\u4e00\u81f4\u6027\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u6709\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u5f25\u5408\u4e86\u539f\u59cb\u50cf\u7d20\u4e0e\u53ef\u6269\u5c55\u8f6f\u4ef6\u5de5\u7a0b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "VSA\u901a\u8fc7\u89c6\u89c9-\u7ed3\u6784\u5bf9\u9f50\u7684\u591a\u9636\u6bb5\u8303\u5f0f\uff0c\u80fd\u591f\u751f\u6210\u7ec4\u7ec7\u826f\u597d\u7684\u524d\u7aef\u8d44\u4ea7\uff0c\u89e3\u51b3\u4e86\u8bbe\u8ba1\u5230\u4ee3\u7801\u8f6c\u6362\u4e2d\u7684\u7ec4\u4ef6\u5316\u548c\u53ef\u7ef4\u62a4\u6027\u95ee\u9898\u3002"}}
{"id": "2512.20172", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20172", "abs": "https://arxiv.org/abs/2512.20172", "authors": ["Yan Zhang", "Li Deng", "Lixin Duan", "Ivor W. Tsang", "Guowu Yang"], "title": "Collaborative Group-Aware Hashing for Fast Recommender Systems", "comment": null, "summary": "The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.", "AI": {"tldr": "\u63d0\u51faCGAH\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u548c\u7269\u54c1\u7684\u56fa\u6709\u7fa4\u7ec4\u4fe1\u606f\u6765\u7f13\u89e3\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u54c8\u5e0c\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u7387", "motivation": "\u5927\u89c4\u6a21\u6570\u636e\u5e93\u7684\u5feb\u901f\u5728\u7ebf\u63a8\u8350\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7a00\u758f\u573a\u666f\u4e0b\u51c6\u786e\u63a8\u8350\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u54c8\u5e0c\u63a8\u8350\u65b9\u6cd5\u5728\u7a00\u758f\u8bbe\u7f6e\u4e0b\u51c6\u786e\u7387\u4f4e\uff0c\u4e3b\u8981\u56e0\u4e3a\u6bcf\u4f4d\u8868\u793a\u80fd\u529b\u6709\u9650\u4e14\u5ffd\u7565\u4e86\u7528\u6237\u548c\u7269\u54c1\u95f4\u7684\u56fa\u6709\u5173\u7cfb", "method": "\u63d0\u51fa\u534f\u4f5c\u7fa4\u7ec4\u611f\u77e5\u54c8\u5e0c(CGAH)\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u5c06\u6f5c\u5728\u5411\u91cf\u5206\u7c7b\u5230\u4e0d\u540c\u7fa4\u7ec4\u6765\u63d0\u53d6\u7528\u6237\u548c\u7269\u54c1\u7684\u56fa\u6709\u7fa4\u7ec4\u4eb2\u548c\u6027\uff1b2) \u5c06\u504f\u597d\u5efa\u6a21\u4e3a\u7fa4\u7ec4\u4eb2\u548c\u6027\u4e0e\u54c8\u5e0c\u7801\u76f8\u4f3c\u5ea6\u7684\u5185\u79ef\uff1b3) \u5b66\u4e60\u5305\u542b\u56fa\u6709\u7fa4\u7ec4\u4fe1\u606f\u7684\u54c8\u5e0c\u7801", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCGAH\u548cCGAH-CF\u5728\u4e0d\u540c\u7a00\u758f\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u79bb\u6563\u534f\u4f5c\u8fc7\u6ee4\u65b9\u6cd5\u548c\u79bb\u6563\u5185\u5bb9\u611f\u77e5\u63a8\u8350\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6574\u5408\u56fa\u6709\u7fa4\u7ec4\u4fe1\u606f\uff0cCGAH\u65b9\u6cd5\u80fd\u591f\u4ece\u7a00\u758f\u4ea4\u4e92\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u6bd4\u5176\u4ed6\u79bb\u6563\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u54c8\u5e0c\u7801\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u54c8\u5e0c\u63a8\u8350\u7cfb\u7edf\u5728\u7a00\u758f\u573a\u666f\u4e0b\u7684\u51c6\u786e\u7387"}}
{"id": "2512.20458", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.20458", "abs": "https://arxiv.org/abs/2512.20458", "authors": ["Shuting Wang", "Qiaolin Xia", "Hao Wang", "Yu Lu", "Bobsimons", "Zhicheng Dou"], "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.", "AI": {"tldr": "Laser\u662f\u4e00\u4e2a\u7ed3\u6784\u5316\u4ee3\u7406\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u52a8\u4f5c\u534f\u8bae\u548c\u7d27\u51d1\u4e0a\u4e0b\u6587\u5bc4\u5b58\u5668\u89e3\u51b3\u73b0\u6709LLM\u4ee3\u7406\u641c\u7d22\u4e2d\u63a8\u7406\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u3001\u4e0a\u4e0b\u6587\u6ea2\u51fa\u7b49\u95ee\u9898\uff0c\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM/LRM\u7684\u4ee3\u7406\u641c\u7d22\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u5c06\u539f\u59cb\u4e2d\u95f4\u8f68\u8ff9\u7d2f\u79ef\u5728\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u3001\u4e0a\u4e0b\u6587\u6ea2\u51fa\uff0c\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u4e2d\u6027\u80fd\u4e0b\u964d\u3002", "method": "Laser\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7b26\u53f7\u5316\u52a8\u4f5c\u534f\u8bae\uff0c\u5c06\u4ee3\u7406\u884c\u4e3a\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u7a7a\u95f4\uff1a\u89c4\u5212\u3001\u4efb\u52a1\u89e3\u51b3\u548c\u53cd\u601d\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u6709\u660e\u786e\u7684\u8bed\u4e49\u548c\u786e\u5b9a\u6027\u6267\u884c\u683c\u5f0f\uff0c\u914d\u5408\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u5bc4\u5b58\u5668\u53ea\u5b58\u50a8\u63a8\u7406\u8fc7\u7a0b\u7684\u5173\u952e\u72b6\u6001\u3002", "result": "\u5728Qwen2.5/3\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLaser\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u8df3QA\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u7406\u641c\u7d22\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u4ec5\u63d0\u793a\u8fd8\u662f\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u3002", "conclusion": "Laser\u4e3a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u6709\u6548\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u548c\u53ef\u63a7\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.20612", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.20612", "abs": "https://arxiv.org/abs/2512.20612", "authors": ["Yibin Lei", "Shwai He", "Ang Li", "Andrew Yates"], "title": "Making Large Language Models Efficient Dense Retrievers", "comment": null, "summary": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.", "AI": {"tldr": "EffiR\u6846\u67b6\u901a\u8fc7\u5206\u6790LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u5c42\u5197\u4f59\uff0c\u53d1\u73b0MLP\u5c42\u6bd4\u6ce8\u610f\u529b\u5c42\u66f4\u53ef\u526a\u679d\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\u538b\u7f29MLP\u5c42\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u5668", "motivation": "\u73b0\u6709\u76f4\u63a5\u5fae\u8c03LLM\u8fdb\u884c\u7a20\u5bc6\u68c0\u7d22\u7684\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u5f3a\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u663e\u793aLLM\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u5c42\u5197\u4f59\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5728\u9700\u8981\u5c06\u6574\u4e2a\u5e8f\u5217\u7f16\u7801\u4e3a\u56fa\u5b9a\u8868\u793a\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\u662f\u5426\u5b58\u5728\u7c7b\u4f3c\u5197\u4f59", "method": "\u63d0\u51faEffiR\u6846\u67b6\uff1a1) \u5206\u6790LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u5c42\u5197\u4f59\u7279\u6027\uff1b2) \u53d1\u73b0MLP\u5c42\u6bd4\u6ce8\u610f\u529b\u5c42\u66f4\u53ef\u526a\u679d\uff1b3) \u91c7\u7528\u7c97\u5230\u7ec6\u7b56\u7565\u8fdb\u884c\u5927\u89c4\u6a21MLP\u538b\u7f29\uff08\u7c97\u7c92\u5ea6\u6df1\u5ea6\u51cf\u5c11+\u7ec6\u7c92\u5ea6\u5bbd\u5ea6\u51cf\u5c11\uff09\uff1b4) \u7ed3\u5408\u68c0\u7d22\u7279\u5b9a\u5fae\u8c03", "result": "\u5728\u591a\u79cdBEIR\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cEffiR\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u5c3a\u5bf8\u6a21\u578b\u7684\u6027\u80fd", "conclusion": "LLM\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u5c42\u5197\u4f59\u6a21\u5f0f\u4e0e\u751f\u6210\u4efb\u52a1\u4e0d\u540c\uff0cMLP\u5c42\u66f4\u53ef\u526a\u679d\u800c\u6ce8\u610f\u529b\u5c42\u5bf9\u8bed\u4e49\u805a\u5408\u66f4\u5173\u952e\u3002EffiR\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u538b\u7f29MLP\u5c42\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u76f8\u5f53\u7684\u68c0\u7d22\u5668"}}

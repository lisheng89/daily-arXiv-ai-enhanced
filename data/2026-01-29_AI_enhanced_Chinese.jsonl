{"id": "2601.20083", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20083", "abs": "https://arxiv.org/abs/2601.20083", "authors": ["Lee Xiong", "Zhirong Chen", "Rahul Mayuranath", "Shangran Qiu", "Arda Ozdemir", "Lu Li", "Yang Hu", "Dave Li", "Jingtao Ren", "Howard Cheng", "Fabian Souto Herrera", "Ahmed Agiza", "Baruch Epshtein", "Anuj Aggarwal", "Julia Ulziisaikhan", "Chao Wang", "Dinesh Ramasamy", "Parshva Doshi", "Sri Reddy", "Arnold Overwijk"], "title": "LLaTTE: Scaling Laws for Multi-Stage Sequence Modeling in Large-Scale Ads Recommendation", "comment": "Lee Xiong, Zhirong Chen, and Rahul Mayuranath contributed equally to this work", "summary": "We present LLaTTE (LLM-Style Latent Transformers for Temporal Events), a scalable transformer architecture for production ads recommendation. Through systematic experiments, we demonstrate that sequence modeling in recommendation systems follows predictable power-law scaling similar to LLMs. Crucially, we find that semantic features bend the scaling curve: they are a prerequisite for scaling, enabling the model to effectively utilize the capacity of deeper and longer architectures. To realize the benefits of continued scaling under strict latency constraints, we introduce a two-stage architecture that offloads the heavy computation of large, long-context models to an asynchronous upstream user model. We demonstrate that upstream improvements transfer predictably to downstream ranking tasks. Deployed as the largest user model at Meta, this multi-stage framework drives a 4.3\\% conversion uplift on Facebook Feed and Reels with minimal serving overhead, establishing a practical blueprint for harnessing scaling laws in industrial recommender systems.", "AI": {"tldr": "LLaTTE\uff1a\u7528\u4e8e\u5e7f\u544a\u63a8\u8350\u7684\u89c4\u6a21\u5316Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5b9e\u73b0LLM\u5f0f\u7684\u5e42\u5f8b\u6269\u5c55\uff0c\u5728Meta\u90e8\u7f72\u5e26\u67654.3%\u8f6c\u5316\u63d0\u5347", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e8f\u5217\u5efa\u6a21\u9075\u5faa\u7c7b\u4f3cLLM\u7684\u53ef\u9884\u6d4b\u5e42\u5f8b\u6269\u5c55\u89c4\u5f8b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5b9e\u73b0\u89c4\u6a21\u5316\u7684\u95ee\u9898\uff0c\u540c\u65f6\u53d1\u73b0\u8bed\u4e49\u7279\u5f81\u662f\u6269\u5c55\u7684\u524d\u63d0\u6761\u4ef6", "method": "\u63d0\u51faLLaTTE\u67b6\u6784\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a\u5c06\u5927\u578b\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u7684\u91cd\u8ba1\u7b97\u5378\u8f7d\u5230\u5f02\u6b65\u4e0a\u6e38\u7528\u6237\u6a21\u578b\uff0c\u4e0b\u6e38\u6392\u5e8f\u4efb\u52a1\u53ef\u9884\u6d4b\u5730\u53d7\u76ca\u4e8e\u4e0a\u6e38\u6539\u8fdb", "result": "\u90e8\u7f72\u4e3aMeta\u6700\u5927\u7684\u7528\u6237\u6a21\u578b\uff0c\u5728Facebook Feed\u548cReels\u4e0a\u5b9e\u73b04.3%\u7684\u8f6c\u5316\u63d0\u5347\uff0c\u670d\u52a1\u5f00\u9500\u6700\u5c0f\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u6269\u5c55\u5b9a\u5f8b\u7684\u5b9e\u7528\u6027", "conclusion": "\u4e3a\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u5229\u7528\u6269\u5c55\u5b9a\u5f8b\u63d0\u4f9b\u4e86\u5b9e\u7528\u84dd\u56fe\uff0c\u8bc1\u660e\u8bed\u4e49\u7279\u5f81\u662f\u6269\u5c55\u524d\u63d0\uff0c\u4e24\u9636\u6bb5\u67b6\u6784\u53ef\u5728\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u89c4\u6a21\u5316\u6539\u8fdb"}}
{"id": "2601.20084", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20084", "abs": "https://arxiv.org/abs/2601.20084", "authors": ["Yash Saxena", "Ankur Padia", "Kalpa Gunaratna", "Manas Gaur"], "title": "IMRNNs: An Efficient Method for Interpretable Dense Retrieval via Embedding Modulation", "comment": "Accepted in EACL 2026", "summary": "Interpretability in black-box dense retrievers remains a central challenge in Retrieval-Augmented Generation (RAG). Understanding how queries and documents semantically interact is critical for diagnosing retrieval behavior and improving model design. However, existing dense retrievers rely on static embeddings for both queries and documents, which obscures this bidirectional relationship. Post-hoc approaches such as re-rankers are computationally expensive, add inference latency, and still fail to reveal the underlying semantic alignment. To address these limitations, we propose Interpretable Modular Retrieval Neural Networks (IMRNNs), a lightweight framework that augments any dense retriever with dynamic, bidirectional modulation at inference time. IMRNNs employ two independent adapters: one conditions document embeddings on the current query, while the other refines the query embedding using corpus-level feedback from initially retrieved documents. This iterative modulation process enables the model to adapt representations dynamically and expose interpretable semantic dependencies between queries and documents. Empirically, IMRNNs not only enhance interpretability but also improve retrieval effectiveness. Across seven benchmark datasets, applying our method to standard dense retrievers yields average gains of +6.35% nDCG, +7.14% recall, and +7.04% MRR over state-of-the-art baselines. These results demonstrate that incorporating interpretability-driven modulation can both explain and enhance retrieval in RAG systems.", "AI": {"tldr": "IMRNNs\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u53cc\u5411\u8c03\u5236\u589e\u5f3a\u7a20\u5bc6\u68c0\u7d22\u5668\u7684\u53ef\u89e3\u91ca\u6027\u548c\u68c0\u7d22\u6548\u679c\uff0c\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u9002\u914d\u5668\u5206\u522b\u57fa\u4e8e\u67e5\u8be2\u8c03\u6574\u6587\u6863\u5d4c\u5165\u548c\u57fa\u4e8e\u68c0\u7d22\u6587\u6863\u53cd\u9988\u8c03\u6574\u67e5\u8be2\u5d4c\u5165\u3002", "motivation": "\u89e3\u51b3\u9ed1\u76d2\u7a20\u5bc6\u68c0\u7d22\u5668\u5728RAG\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u5d4c\u5165\uff0c\u63a9\u76d6\u4e86\u67e5\u8be2\u548c\u6587\u6863\u4e4b\u95f4\u7684\u53cc\u5411\u8bed\u4e49\u5173\u7cfb\uff0c\u800c\u4e8b\u540e\u91cd\u6392\u5e8f\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u63ed\u793a\u5e95\u5c42\u8bed\u4e49\u5bf9\u9f50\u3002", "method": "\u63d0\u51faIMRNNs\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u72ec\u7acb\u9002\u914d\u5668\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5f53\u524d\u67e5\u8be2\u6761\u4ef6\u5316\u6587\u6863\u5d4c\u5165\uff0c\u53e6\u4e00\u4e2a\u4f7f\u7528\u521d\u59cb\u68c0\u7d22\u6587\u6863\u7684\u8bed\u6599\u7ea7\u53cd\u9988\u7cbe\u5316\u67e5\u8be2\u5d4c\u5165\u3002\u901a\u8fc7\u8fed\u4ee3\u8c03\u5236\u8fc7\u7a0b\u52a8\u6001\u8c03\u6574\u8868\u793a\uff0c\u66b4\u9732\u67e5\u8be2\u548c\u6587\u6863\u4e4b\u95f4\u7684\u53ef\u89e3\u91ca\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5c06IMRNNs\u5e94\u7528\u4e8e\u6807\u51c6\u7a20\u5bc6\u68c0\u7d22\u5668\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5e73\u5747\u63d0\u5347\uff1anDCG +6.35%\u3001\u53ec\u56de\u7387 +7.14%\u3001MRR +7.04%\u3002\u540c\u65f6\u589e\u5f3a\u4e86\u68c0\u7d22\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5f15\u5165\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u7684\u8c03\u5236\u673a\u5236\u65e2\u80fd\u89e3\u91ca\u53c8\u80fd\u589e\u5f3aRAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u68c0\u7d22\u6548\u679c\u53ef\u4ee5\u534f\u540c\u63d0\u5347\u3002"}}
{"id": "2601.20131", "categories": ["cs.IR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20131", "abs": "https://arxiv.org/abs/2601.20131", "authors": ["Deep Shah", "Sanket Badhe", "Nehal Kathrotia"], "title": "Taxonomy of the Retrieval System Framework: Pitfalls and Paradigms", "comment": null, "summary": "Designing an embedding retrieval system requires navigating a complex design space of conflicting trade-offs between efficiency and effectiveness. This work structures these decisions as a vertical traversal of the system design stack. We begin with the Representation Layer by examining how loss functions and architectures, specifically Bi-encoders and Cross-encoders, define semantic relevance and geometric projection. Next, we analyze the Granularity Layer and evaluate how segmentation strategies like Atomic and Hierarchical chunking mitigate information bottlenecks in long-context documents. Moving to the Orchestration Layer, we discuss methods that transcend the single-vector paradigm, including hierarchical retrieval, agentic decomposition, and multi-stage reranking pipelines to resolve capacity limitations. Finally, we address the Robustness Layer by identifying architectural mitigations for domain generalization failures, lexical blind spots, and the silent degradation of retrieval quality due to temporal drift. By categorizing these limitations and design choices, we provide a comprehensive framework for practitioners to optimize the efficiency-effectiveness frontier in modern neural search systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u5c42\u6846\u67b6\u6765\u7cfb\u7edf\u5316\u8bbe\u8ba1\u5d4c\u5165\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u8868\u793a\u5c42\u3001\u7c92\u5ea6\u5c42\u3001\u7f16\u6392\u5c42\u548c\u9c81\u68d2\u6027\u5c42\u6765\u5e73\u8861\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u8bbe\u8ba1\u5d4c\u5165\u68c0\u7d22\u7cfb\u7edf\u9700\u8981\u5728\u6548\u7387\u4e0e\u6548\u679c\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u7684\u6743\u8861\u51b3\u7b56\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u6846\u67b6\u6765\u6307\u5bfc\u8fd9\u4e9b\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u5782\u76f4\u904d\u5386\u7cfb\u7edf\u8bbe\u8ba1\u6808\u7684\u56db\u5c42\u6846\u67b6\uff1a\u8868\u793a\u5c42\uff08\u635f\u5931\u51fd\u6570\u548c\u67b6\u6784\uff09\u3001\u7c92\u5ea6\u5c42\uff08\u5206\u5272\u7b56\u7565\uff09\u3001\u7f16\u6392\u5c42\uff08\u8d85\u8d8a\u5355\u5411\u91cf\u8303\u5f0f\u7684\u65b9\u6cd5\uff09\u3001\u9c81\u68d2\u6027\u5c42\uff08\u67b6\u6784\u7f13\u89e3\u63aa\u65bd\uff09\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u8fd9\u4e9b\u9650\u5236\u548c\u8bbe\u8ba1\u9009\u62e9\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u73b0\u4ee3\u795e\u7ecf\u641c\u7d22\u7cfb\u7edf\u4e2d\u6548\u7387\u4e0e\u6548\u679c\u7684\u524d\u6cbf\u3002", "conclusion": "\u8be5\u56db\u5c42\u6846\u67b6\u4e3a\u5d4c\u5165\u68c0\u7d22\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u8bba\uff0c\u5e2e\u52a9\u5b9e\u8df5\u8005\u5728\u590d\u6742\u7684\u6548\u7387-\u6548\u679c\u6743\u8861\u7a7a\u95f4\u4e2d\u505a\u51fa\u660e\u667a\u7684\u8bbe\u8ba1\u51b3\u7b56\u3002"}}
{"id": "2601.20199", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20199", "abs": "https://arxiv.org/abs/2601.20199", "authors": ["Jing Yan", "Yimeng Bai", "Zongyu Liu", "Yahui Liu", "Junwei Wang", "Jingze Huang", "Haoda Li", "Sihao Ding", "Shaohui Ruan", "Yang Zhang"], "title": "MERGE: Next-Generation Item Indexing Paradigm for Large-Scale Streaming Recommendation", "comment": null, "summary": "Item indexing, which maps a large corpus of items into compact discrete representations, is critical for both discriminative and generative recommender systems, yet existing Vector Quantization (VQ)-based approaches struggle with the highly skewed and non-stationary item distributions common in streaming industry recommenders, leading to poor assignment accuracy, imbalanced cluster occupancy, and insufficient cluster separation. To address these challenges, we propose MERGE, a next-generation item indexing paradigm that adaptively constructs clusters from scratch, dynamically monitors cluster occupancy, and forms hierarchical index structures via fine-to-coarse merging. Extensive experiments demonstrate that MERGE significantly improves assignment accuracy, cluster uniformity, and cluster separation compared with existing indexing methods, while online A/B tests show substantial gains in key business metrics, highlighting its potential as a foundational indexing approach for large-scale recommendation.", "AI": {"tldr": "MERGE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u54c1\u7d22\u5f15\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6784\u5efa\u805a\u7c7b\u3001\u52a8\u6001\u76d1\u63a7\u805a\u7c7b\u5360\u7528\u7387\u4ee5\u53ca\u7ec6\u5230\u7c97\u7684\u5c42\u6b21\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVQ\u65b9\u6cd5\u5728\u6d41\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u5904\u7406\u9ad8\u5ea6\u504f\u659c\u548c\u975e\u5e73\u7a33\u7269\u54c1\u5206\u5e03\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5411\u91cf\u91cf\u5316(VQ)\u7684\u7269\u54c1\u7d22\u5f15\u65b9\u6cd5\u5728\u5904\u7406\u6d41\u5f0f\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u9ad8\u5ea6\u504f\u659c\u548c\u975e\u5e73\u7a33\u7269\u54c1\u5206\u5e03\u65f6\uff0c\u9762\u4e34\u5206\u914d\u7cbe\u5ea6\u5dee\u3001\u805a\u7c7b\u5360\u7528\u4e0d\u5e73\u8861\u548c\u805a\u7c7b\u5206\u79bb\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7d22\u5f15\u89e3\u51b3\u65b9\u6848\u3002", "method": "MERGE\u91c7\u7528\u81ea\u9002\u5e94\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u805a\u7c7b\u3001\u52a8\u6001\u76d1\u63a7\u805a\u7c7b\u5360\u7528\u7387\uff0c\u5e76\u901a\u8fc7\u7ec6\u5230\u7c97\u7684\u5c42\u6b21\u5408\u5e76\u5f62\u6210\u5206\u5c42\u7d22\u5f15\u7ed3\u6784\u7684\u65b0\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMERGE\u5728\u5206\u914d\u7cbe\u5ea6\u3001\u805a\u7c7b\u5747\u5300\u6027\u548c\u805a\u7c7b\u5206\u79bb\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7d22\u5f15\u65b9\u6cd5\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u5173\u952e\u4e1a\u52a1\u6307\u6807\u6709\u5b9e\u8d28\u6027\u63d0\u5347\u3002", "conclusion": "MERGE\u6709\u6f5c\u529b\u6210\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u7684\u57fa\u7840\u7d22\u5f15\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVQ\u65b9\u6cd5\u5728\u6d41\u5f0f\u63a8\u8350\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.20215", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20215", "abs": "https://arxiv.org/abs/2601.20215", "authors": ["Na Li", "Jiaqi Yu", "Minzhi Xie", "Tiantian He", "Xiaoxiao Xu", "Zixiu Wang", "Lantao Hu", "Yongqi Liu", "Han Li", "Kaiqiao Zhan", "Kun Gai"], "title": "Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation", "comment": null, "summary": "Short-video recommender systems typically optimize ranking models using dense user behavioral signals, such as clicks and watch time. However, these signals are only indirect proxies of user satisfaction and often suffer from noise and bias. Recently, explicit satisfaction feedback collected through questionnaires has emerged as a high-quality direct alignment supervision, but is extremely sparse and easily overwhelmed by abundant behavioral data, making it difficult to incorporate into online recommendation models. To address these challenges, we propose a novel framework which is towards End-to-End Alignment of user Satisfaction via Questionaire, named EASQ, to enable real-time alignment of ranking models with true user satisfaction. Specifically, we first construct an independent parameter pathway for sparse questionnaire signals by combining a multi-task architecture and a lightweight LoRA module. The multi-task design separates sparse satisfaction supervision from dense behavioral signals, preventing the former from being overwhelmed. The LoRA module pre-inject these preferences in a parameter-isolated manner, ensuring stability in the backbone while optimizing user satisfaction. Furthermore, we employ a DPO-based optimization objective tailored for online learning, which aligns the main model outputs with sparse satisfaction signals in real time. This design enables end-to-end online learning, allowing the model to continuously adapt to new questionnaire feedback while maintaining the stability and effectiveness of the backbone. Extensive offline experiments and large-scale online A/B tests demonstrate that EASQ consistently improves user satisfaction metrics across multiple scenarios. EASQ has been successfully deployed in a production short-video recommendation system, delivering significant and stable business gains.", "AI": {"tldr": "EASQ\u6846\u67b6\u901a\u8fc7\u95ee\u5377\u53cd\u9988\u5b9e\u73b0\u63a8\u8350\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u7528\u6237\u6ee1\u610f\u5ea6\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u6ee1\u610f\u5ea6\u4fe1\u53f7\u88ab\u6d77\u91cf\u884c\u4e3a\u6570\u636e\u6df9\u6ca1\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u70b9\u51fb\u3001\u89c2\u770b\u65f6\u957f\u7b49\u884c\u4e3a\u4fe1\u53f7\u4f5c\u4e3a\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u95f4\u63a5\u4ee3\u7406\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u5b58\u5728\u566a\u58f0\u548c\u504f\u5dee\u3002\u95ee\u5377\u6536\u96c6\u7684\u663e\u5f0f\u6ee1\u610f\u5ea6\u53cd\u9988\u8d28\u91cf\u9ad8\u4f46\u6781\u5176\u7a00\u758f\uff0c\u96be\u4ee5\u878d\u5165\u5b9e\u65f6\u63a8\u8350\u6a21\u578b\u3002", "method": "1) \u6784\u5efa\u72ec\u7acb\u53c2\u6570\u901a\u8def\uff1a\u7ed3\u5408\u591a\u4efb\u52a1\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7LoRA\u6a21\u5757\uff0c\u5206\u79bb\u7a00\u758f\u6ee1\u610f\u5ea6\u76d1\u7763\u4e0e\u5bc6\u96c6\u884c\u4e3a\u4fe1\u53f7\uff1b2) DPO\u4f18\u5316\u76ee\u6807\uff1a\u4e3a\u4e3b\u6a21\u578b\u8f93\u51fa\u4e0e\u7a00\u758f\u6ee1\u610f\u5ea6\u4fe1\u53f7\u63d0\u4f9b\u5b9e\u65f6\u5bf9\u9f50\uff1b3) \u7aef\u5230\u7aef\u5728\u7ebf\u5b66\u4e60\uff1a\u6a21\u578b\u6301\u7eed\u9002\u5e94\u65b0\u95ee\u5377\u53cd\u9988\uff0c\u540c\u65f6\u4fdd\u6301\u4e3b\u5e72\u7a33\u5b9a\u6027\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0cEASQ\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u6301\u7eed\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u6307\u6807\u3002\u5df2\u5728\u751f\u4ea7\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u5e26\u6765\u663e\u8457\u4e14\u7a33\u5b9a\u7684\u4e1a\u52a1\u6536\u76ca\u3002", "conclusion": "EASQ\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u6ee1\u610f\u5ea6\u4fe1\u53f7\u878d\u5165\u5b9e\u65f6\u63a8\u8350\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u901a\u8fc7\u53c2\u6570\u9694\u79bb\u548c\u5b9e\u65f6\u5bf9\u9f50\u5b9e\u73b0\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u76f4\u63a5\u76d1\u7763\u4fe1\u53f7\u3002"}}
{"id": "2601.20234", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20234", "abs": "https://arxiv.org/abs/2601.20234", "authors": ["Qihang Yu", "Kairui Fu", "Zhaocheng Du", "Yuxuan Si", "Kaiyuan Li", "Weihao Zhao", "Zhicheng Zhang", "Jieming Zhu", "Quanyu Dai", "Zhenhua Dong", "Shengyu Zhang", "Kun Kuang", "Fei Wu"], "title": "MALLOC: Benchmarking the Memory-aware Long Sequence Compression for Large Sequential Recommendation", "comment": null, "summary": "The scaling law, which indicates that model performance improves with increasing dataset and model capacity, has fueled a growing trend in expanding recommendation models in both industry and academia. However, the advent of large-scale recommenders also brings significantly higher computational costs, particularly under the long-sequence dependencies inherent in the user intent of recommendation systems. Current approaches often rely on pre-storing the intermediate states of the past behavior for each user, thereby reducing the quadratic re-computation cost for the following requests. Despite their effectiveness, these methods often treat memory merely as a medium for acceleration, without adequately considering the space overhead it introduces. This presents a critical challenge in real-world recommendation systems with billions of users, each of whom might initiate thousands of interactions and require massive memory for state storage. Fortunately, there have been several memory management strategies examined for compression in LLM, while most have not been evaluated on the recommendation task. To mitigate this gap, we introduce MALLOC, a comprehensive benchmark for memory-aware long sequence compression. MALLOC presents a comprehensive investigation and systematic classification of memory management techniques applicable to large sequential recommendations. These techniques are integrated into state-of-the-art recommenders, enabling a reproducible and accessible evaluation platform. Through extensive experiments across accuracy, efficiency, and complexity, we demonstrate the holistic reliability of MALLOC in advancing large-scale recommendation. Code is available at https://anonymous.4open.science/r/MALLOC.", "AI": {"tldr": "MALLOC\u662f\u4e00\u4e2a\u7528\u4e8e\u5185\u5b58\u611f\u77e5\u957f\u5e8f\u5217\u538b\u7f29\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u56e0\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u957f\u4f9d\u8d56\u5173\u7cfb\u5e26\u6765\u7684\u5de8\u5927\u5185\u5b58\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u63a8\u8350\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\uff0c\u8ba1\u7b97\u6210\u672c\u6025\u5267\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7528\u6237\u884c\u4e3a\u7684\u957f\u5e8f\u5217\u4f9d\u8d56\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u9884\u5b58\u50a8\u7528\u6237\u5386\u53f2\u884c\u4e3a\u4e2d\u95f4\u72b6\u6001\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u5ffd\u89c6\u4e86\u7531\u6b64\u5e26\u6765\u7684\u5de8\u5927\u5185\u5b58\u7a7a\u95f4\u6210\u672c\uff0c\u8fd9\u5728\u62e5\u6709\u6570\u5341\u4ebf\u7528\u6237\u3001\u6bcf\u4e2a\u7528\u6237\u53ef\u80fd\u6709\u6570\u5343\u6b21\u4ea4\u4e92\u7684\u771f\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86MALLOC\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u7c7b\u548c\u6574\u5408\u4e86\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e8f\u5217\u63a8\u8350\u7684\u5185\u5b58\u7ba1\u7406\u6280\u672f\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6280\u672f\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u63a8\u8350\u6a21\u578b\u4e2d\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u6613\u8bbf\u95ee\u7684\u8bc4\u4f30\u5e73\u53f0\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u590d\u6742\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u9a8c\u8bc1\u4e86MALLOC\u5728\u63a8\u8fdb\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u53d1\u5c55\u65b9\u9762\u7684\u5168\u9762\u53ef\u9760\u6027\u3002", "conclusion": "MALLOC\u586b\u8865\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u5185\u5b58\u7ba1\u7406\u6280\u672f\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u5927\u89c4\u6a21\u5e8f\u5217\u63a8\u8350\u7684\u5185\u5b58\u538b\u7f29\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2601.20283", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20283", "abs": "https://arxiv.org/abs/2601.20283", "authors": ["Tanmay Karmakar", "Sourav Saha", "Debapriyo Majumdar", "Surjyanee Halder"], "title": "One Word is Enough: Minimal Adversarial Perturbations for Neural Text Ranking", "comment": "To appear at ECIR 2026", "summary": "Neural ranking models (NRMs) achieve strong retrieval effectiveness, yet prior work has shown they are vulnerable to adversarial perturbations. We revisit this robustness question with a minimal, query-aware attack that promotes a target document by inserting or substituting a single, semantically aligned word - the query center. We study heuristic and gradient-guided variants, including a white-box method that identifies influential insertion points. On TREC-DL 2019/2020 with BERT and monoT5 re-rankers, our single-word attacks achieve up to 91% success while modifying fewer than two tokens per document on average, achieving competitive rank and score boosts with far fewer edits under a comparable white-box setup to ensure fair evaluation against PRADA. We also introduce new diagnostic metrics to analyze attack sensitivity beyond aggregate success rates. Our analysis reveals a Goldilocks zone in which mid-ranked documents are most vulnerable. These findings demonstrate practical risks and motivate future defenses for robust neural ranking.", "AI": {"tldr": "\u9488\u5bf9\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u7684\u5355\u8bcd\u8bed\u4e49\u653b\u51fb\uff1a\u901a\u8fc7\u63d2\u5165\u6216\u66ff\u6362\u4e00\u4e2a\u4e0e\u67e5\u8be2\u8bed\u4e49\u5bf9\u9f50\u7684\"\u67e5\u8be2\u4e2d\u5fc3\u8bcd\"\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u5347\u76ee\u6807\u6587\u6863\u6392\u540d\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe91%\uff0c\u5e73\u5747\u6bcf\u6587\u6863\u4fee\u6539\u4e0d\u52302\u4e2atoken\u3002", "motivation": "\u867d\u7136\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u5728\u68c0\u7d22\u6548\u679c\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5148\u524d\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6270\u52a8\u653b\u51fb\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u6700\u5c0f\u5316\u7684\u67e5\u8be2\u611f\u77e5\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5c0f\u5316\u7684\u67e5\u8be2\u611f\u77e5\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165\u6216\u66ff\u6362\u5355\u4e2a\u8bed\u4e49\u5bf9\u9f50\u7684\"\u67e5\u8be2\u4e2d\u5fc3\u8bcd\"\u6765\u63d0\u5347\u76ee\u6807\u6587\u6863\u6392\u540d\u3002\u7814\u7a76\u4e86\u542f\u53d1\u5f0f\u548c\u68af\u5ea6\u5f15\u5bfc\u7684\u53d8\u4f53\uff0c\u5305\u62ec\u8bc6\u522b\u6709\u5f71\u54cd\u529b\u63d2\u5165\u70b9\u7684\u767d\u76d2\u65b9\u6cd5\u3002\u5728TREC-DL 2019/2020\u6570\u636e\u96c6\u4e0a\u4f7f\u7528BERT\u548cmonoT5\u91cd\u65b0\u6392\u5e8f\u5668\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5355\u8bcd\u8bed\u4e49\u653b\u51fb\u5728BERT\u548cmonoT5\u91cd\u65b0\u6392\u5e8f\u5668\u4e0a\u8fbe\u523091%\u7684\u6210\u529f\u7387\uff0c\u5e73\u5747\u6bcf\u6587\u6863\u4fee\u6539\u4e0d\u52302\u4e2atoken\u3002\u5728\u53ef\u6bd4\u7684\u767d\u76d2\u8bbe\u7f6e\u4e0b\uff0c\u4ee5\u66f4\u5c11\u7684\u7f16\u8f91\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u6392\u540d\u548c\u5206\u6570\u63d0\u5347\u3002\u5206\u6790\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\"Goldilocks\u533a\u57df\"\uff0c\u4e2d\u7b49\u6392\u540d\u7684\u6587\u6863\u6700\u4e3a\u8106\u5f31\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5c55\u793a\u4e86\u795e\u7ecf\u6392\u5e8f\u6a21\u578b\u7684\u5b9e\u9645\u98ce\u9669\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u65b0\u7684\u8bca\u65ad\u6307\u6807\u6765\u5206\u6790\u653b\u51fb\u654f\u611f\u6027\uff0c\u8d85\u8d8a\u4e86\u805a\u5408\u6210\u529f\u7387\u3002"}}
{"id": "2601.20316", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20316", "abs": "https://arxiv.org/abs/2601.20316", "authors": ["Kargi Chauhan", "Mahalakshmi Venkateswarlu"], "title": "Less is More: Benchmarking LLM Based Recommendation Agents", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed for personalized product recommendations, with practitioners commonly assuming that longer user purchase histories lead to better predictions. We challenge this assumption through a systematic benchmark of four state of the art LLMs GPT-4o-mini, DeepSeek-V3, Qwen2.5-72B, and Gemini 2.5 Flash across context lengths ranging from 5 to 50 items using the REGEN dataset.\n  Surprisingly, our experiments with 50 users in a within subject design reveal no significant quality improvement with increased context length. Quality scores remain flat across all conditions (0.17--0.23). Our findings have significant practical implications: practitioners can reduce inference costs by approximately 88\\% by using context (5--10 items) instead of longer histories (50 items), without sacrificing recommendation quality. We also analyze latency patterns across providers and find model specific behaviors that inform deployment decisions. This work challenges the existing ``more context is better'' paradigm and provides actionable guidelines for cost effective LLM based recommendation systems.", "AI": {"tldr": "LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u589e\u52a0\u7528\u6237\u5386\u53f2\u957f\u5ea6\uff085-50\u9879\uff09\u5e76\u4e0d\u4f1a\u663e\u8457\u63d0\u5347\u63a8\u8350\u8d28\u91cf\uff0c\u4f7f\u75285-10\u9879\u5386\u53f2\u5373\u53ef\u8282\u770188%\u63a8\u7406\u6210\u672c\u800c\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u6311\u6218\u5f53\u524dLLM\u63a8\u8350\u7cfb\u7edf\u4e2d\"\u66f4\u591a\u4e0a\u4e0b\u6587\u66f4\u597d\"\u7684\u666e\u904d\u5047\u8bbe\uff0c\u9a8c\u8bc1\u589e\u52a0\u7528\u6237\u8d2d\u4e70\u5386\u53f2\u957f\u5ea6\u662f\u5426\u771f\u7684\u80fd\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u3002", "method": "\u5728REGEN\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9GPT-4o-mini\u3001DeepSeek-V3\u3001Qwen2.5-72B\u548cGemini 2.5 Flash\u56db\u79cdSOTA LLM\u8fdb\u884c\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u752850\u540d\u7528\u6237\u7684\u53d7\u8bd5\u8005\u5185\u8bbe\u8ba1\uff0c\u6bd4\u8f835-50\u9879\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\uff085-50\u9879\uff09\u4e0b\u7684\u8d28\u91cf\u8bc4\u5206\u4fdd\u6301\u7a33\u5b9a\uff080.17-0.23\uff09\uff0c\u6ca1\u6709\u663e\u8457\u6539\u5584\u3002\u4f7f\u75285-10\u9879\u5386\u53f2\u76f8\u6bd450\u9879\u53ef\u8282\u7701\u7ea688%\u63a8\u7406\u6210\u672c\u800c\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "conclusion": "LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\"\u66f4\u591a\u4e0a\u4e0b\u6587\u66f4\u597d\"\u7684\u8303\u5f0f\u88ab\u6311\u6218\uff0c\u4f7f\u7528\u8f83\u77ed\u4e0a\u4e0b\u6587\uff085-10\u9879\uff09\u5373\u53ef\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u9ad8\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2601.20391", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20391", "abs": "https://arxiv.org/abs/2601.20391", "authors": ["Zhuocheng Zhang", "Kangheng Liang", "Guanxuan Li", "Paul Henderson", "Richard Mccreadie", "Zijun Long"], "title": "Eliminating Hallucination in Diffusion-Augmented Interactive Text-to-Image Retrieval", "comment": null, "summary": "Diffusion-Augmented Interactive Text-to-Image Retrieval (DAI-TIR) is a promising paradigm that improves retrieval performance by generating query images via diffusion models and using them as additional ``views'' of the user's intent. However, these generative views can be incorrect because diffusion generation may introduce hallucinated visual cues that conflict with the original query text. Indeed, we empirically demonstrate that these hallucinated cues can substantially degrade DAI-TIR performance. To address this, we propose Diffusion-aware Multi-view Contrastive Learning (DMCL), a hallucination-robust training framework that casts DAI-TIR as joint optimization over representations of query intent and the target image. DMCL introduces semantic-consistency and diffusion-aware contrastive objectives to align textual and diffusion-generated query views while suppressing hallucinated query signals. This yields an encoder that acts as a semantic filter, effectively mapping hallucinated cues into a null space, improving robustness to spurious cues and better representing the user's intent. Attention visualization and geometric embedding-space analyses corroborate this filtering behavior. Across five standard benchmarks, DMCL delivers consistent improvements in multi-round Hits@10, reaching as high as 7.37\\% over prior fine-tuned and zero-shot baselines, which indicates it is a general and robust training framework for DAI-TIR.", "AI": {"tldr": "\u63d0\u51fa\u4e86DMCL\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u611f\u77e5\u7684\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u89e3\u51b3\u6269\u6563\u589e\u5f3a\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd", "motivation": "\u6269\u6563\u589e\u5f3a\u7684\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22(DAI-TIR)\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u67e5\u8be2\u56fe\u50cf\u4f5c\u4e3a\u7528\u6237\u610f\u56fe\u7684\u989d\u5916\"\u89c6\u56fe\"\uff0c\u4f46\u6269\u6563\u751f\u6210\u53ef\u80fd\u5f15\u5165\u4e0e\u539f\u59cb\u67e5\u8be2\u6587\u672c\u51b2\u7a81\u7684\u5e7b\u89c9\u89c6\u89c9\u7ebf\u7d22\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u68c0\u7d22\u6027\u80fd", "method": "\u63d0\u51fa\u6269\u6563\u611f\u77e5\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60(DMCL)\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6269\u6563\u611f\u77e5\u5bf9\u6bd4\u76ee\u6807\uff0c\u5bf9\u9f50\u6587\u672c\u548c\u6269\u6563\u751f\u6210\u7684\u67e5\u8be2\u89c6\u56fe\uff0c\u540c\u65f6\u6291\u5236\u5e7b\u89c9\u67e5\u8be2\u4fe1\u53f7\uff0c\u4f7f\u7f16\u7801\u5668\u5145\u5f53\u8bed\u4e49\u8fc7\u6ee4\u5668", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMCL\u5728\u591a\u8f6eHits@10\u6307\u6807\u4e0a\u6301\u7eed\u6539\u8fdb\uff0c\u6700\u9ad8\u63d0\u53477.37%\uff0c\u8d85\u8fc7\u5148\u524d\u5fae\u8c03\u548c\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u548c\u51e0\u4f55\u5d4c\u5165\u7a7a\u95f4\u5206\u6790\u8bc1\u5b9e\u4e86\u8fc7\u6ee4\u884c\u4e3a", "conclusion": "DMCL\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u9c81\u68d2\u7684DAI-TIR\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6291\u5236\u6269\u6563\u6a21\u578b\u5f15\u5165\u7684\u5e7b\u89c9\u7ebf\u7d22\uff0c\u66f4\u597d\u5730\u8868\u793a\u7528\u6237\u610f\u56fe\uff0c\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd"}}
{"id": "2601.20623", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20623", "abs": "https://arxiv.org/abs/2601.20623", "authors": ["Hongyi Cai"], "title": "When Vision Meets Texts in Listwise Reranking", "comment": null, "summary": "Recent advancements in information retrieval have highlighted the potential of integrating visual and textual information, yet effective reranking for image-text documents remains challenging due to the modality gap and scarcity of aligned datasets. Meanwhile, existing approaches often rely on large models (7B to 32B parameters) with reasoning-based distillation, incurring unnecessary computational overhead while primarily focusing on textual modalities. In this paper, we propose Rank-Nexus, a multimodal image-text document reranker that performs listwise qualitative reranking on retrieved lists incorporating both images and texts. To bridge the modality gap, we introduce a progressive cross-modal training strategy. We first train modalities separately: leveraging abundant text reranking data, we distill knowledge into the text branch. For images, where data is scarce, we construct distilled pairs from multimodal large language model (MLLM) captions on image retrieval benchmarks. Subsequently, we distill a joint image-text reranking dataset. Rank-Nexus achieves outstanding performance on text reranking benchmarks (TREC, BEIR) and the challenging image reranking benchmark (INQUIRE, MMDocIR), using only a lightweight 2B pretrained visual-language model. This efficient design ensures strong generalization across diverse multimodal scenarios without excessive parameters or reasoning overhead.", "AI": {"tldr": "Rank-Nexus\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u6587\u6863\u91cd\u6392\u5668\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8de8\u6a21\u6001\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u5728\u4ec5\u4f7f\u75282B\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5728\u6587\u672c\u548c\u56fe\u50cf\u91cd\u6392\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4fe1\u606f\u68c0\u7d22\u4e2d\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u6574\u5408\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u56fe\u50cf-\u6587\u672c\u6587\u6863\u91cd\u6392\u9762\u4e34\u6a21\u6001\u9e3f\u6c9f\u548c\u5bf9\u9f50\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u6a21\u578b\uff087B-32B\u53c2\u6570\uff09\u548c\u63a8\u7406\u84b8\u998f\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6a21\u6001\u3002", "method": "\u63d0\u51faRank-Nexus\u591a\u6a21\u6001\u56fe\u50cf-\u6587\u672c\u6587\u6863\u91cd\u6392\u5668\uff0c\u91c7\u7528\u5217\u8868\u5f0f\u5b9a\u6027\u91cd\u6392\u3002\u5f15\u5165\u6e10\u8fdb\u5f0f\u8de8\u6a21\u6001\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u5206\u522b\u8bad\u7ec3\u6a21\u6001\uff1a\u5229\u7528\u4e30\u5bcc\u6587\u672c\u91cd\u6392\u6570\u636e\u84b8\u998f\u5230\u6587\u672c\u5206\u652f\uff1b2\uff09\u5bf9\u7a00\u7f3a\u56fe\u50cf\u6570\u636e\uff0c\u4eceMLLM\u5728\u56fe\u50cf\u68c0\u7d22\u57fa\u51c6\u4e0a\u751f\u6210\u6807\u9898\u6784\u5efa\u84b8\u998f\u5bf9\uff1b3\uff09\u84b8\u998f\u8054\u5408\u56fe\u50cf-\u6587\u672c\u91cd\u6392\u6570\u636e\u96c6\u3002", "result": "\u5728\u6587\u672c\u91cd\u6392\u57fa\u51c6\uff08TREC, BEIR\uff09\u548c\u6311\u6218\u6027\u56fe\u50cf\u91cd\u6392\u57fa\u51c6\uff08INQUIRE, MMDocIR\uff09\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u4ec5\u4f7f\u7528\u8f7b\u91cf\u7ea72B\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u9ad8\u6548\u8bbe\u8ba1\u786e\u4fdd\u5728\u591a\u6837\u5316\u591a\u6a21\u6001\u573a\u666f\u4e2d\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u8fc7\u591a\u53c2\u6570\u6216\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "Rank-Nexus\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8de8\u6a21\u6001\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u4ee5\u8f7b\u91cf\u7ea7\u67b6\u6784\u5b9e\u73b0\u5f3a\u5927\u7684\u591a\u6a21\u6001\u91cd\u6392\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.20671", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20671", "abs": "https://arxiv.org/abs/2601.20671", "authors": ["Jaime Arguello", "Fernando Diaz", "Maik Fr\u00f6ebe", "To Eun Kim", "Bhaskar Mitra"], "title": "Overview of the TREC 2025 Tip-of-the-Tongue track", "comment": null, "summary": "Tip-of-the-tongue (ToT) known-item retrieval involves re-finding an item for which the searcher does not reliably recall an identifier. ToT information requests (or queries) are verbose and tend to include several complex phenomena, making them especially difficult for existing information retrieval systems. The TREC 2025 ToT track focused on a single ad-hoc retrieval task. This year, we extended the track to general domain and incorporated different sets of test queries from diverse sources, namely from the MS-ToT dataset, manual topic development, and LLM-based synthetic query generation. This year, 9 groups (including the track coordinators) submitted 32 runs.", "AI": {"tldr": "TREC 2025 ToT \u8ddf\u8e2a\u6269\u5c55\u4e86\u9886\u57df\u8303\u56f4\uff0c\u6574\u5408\u4e86\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u6d4b\u8bd5\u67e5\u8be2\uff0c\u5171\u67099\u4e2a\u56e2\u961f\u63d0\u4ea4\u4e8632\u4e2a\u8fd0\u884c\u7ed3\u679c\u3002", "motivation": "\u820c\u5c16\u73b0\u8c61\uff08ToT\uff09\u5df2\u77e5\u9879\u68c0\u7d22\u6d89\u53ca\u91cd\u65b0\u67e5\u627e\u7528\u6237\u65e0\u6cd5\u53ef\u9760\u56de\u5fc6\u6807\u8bc6\u7b26\u7684\u9879\u76ee\u3002ToT\u4fe1\u606f\u8bf7\u6c42\u901a\u5e38\u5197\u957f\u4e14\u5305\u542b\u591a\u79cd\u590d\u6742\u73b0\u8c61\uff0c\u8fd9\u5bf9\u73b0\u6709\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u63d0\u51fa\u4e86\u7279\u522b\u6311\u6218\u3002", "method": "\u6269\u5c55TREC ToT\u8ddf\u8e2a\u5230\u901a\u7528\u9886\u57df\uff0c\u6574\u5408\u4e86\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u6765\u6e90\u7684\u6d4b\u8bd5\u67e5\u8be2\uff1aMS-ToT\u6570\u636e\u96c6\u3001\u624b\u52a8\u4e3b\u9898\u5f00\u53d1\u548c\u57fa\u4e8eLLM\u7684\u5408\u6210\u67e5\u8be2\u751f\u6210\u3002", "result": "\u5171\u67099\u4e2a\u56e2\u961f\uff08\u5305\u62ec\u8ddf\u8e2a\u534f\u8c03\u8005\uff09\u63d0\u4ea4\u4e8632\u4e2a\u8fd0\u884c\u7ed3\u679c\u3002", "conclusion": "TREC 2025 ToT\u8ddf\u8e2a\u901a\u8fc7\u6269\u5c55\u9886\u57df\u8303\u56f4\u548c\u6574\u5408\u591a\u6837\u5316\u67e5\u8be2\u6765\u6e90\uff0c\u4e3a\u7814\u7a76\u820c\u5c16\u73b0\u8c61\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.20709", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.20709", "abs": "https://arxiv.org/abs/2601.20709", "authors": ["Huan He", "Xueqing Peng", "Yutong Xie", "Qijia Liu", "Chia-Hsuan Chang", "Lingfei Qian", "Brian Ondov", "Qiaozhu Mei", "Hua Xu"], "title": "MedViz: An Agent-based, Visual-guided Research Assistant for Navigating Biomedical Literature", "comment": null, "summary": "Biomedical researchers face increasing challenges in navigating millions of publications in diverse domains. Traditional search engines typically return articles as ranked text lists, offering little support for global exploration or in-depth analysis. Although recent advances in generative AI and large language models have shown promise in tasks such as summarization, extraction, and question answering, their dialog-based implementations are poorly integrated with literature search workflows. To address this gap, we introduce MedViz, a visual analytics system that integrates multiple AI agents with interactive visualization to support the exploration of the large-scale biomedical literature. MedViz combines a semantic map of millions of articles with agent-driven functions for querying, summarizing, and hypothesis generation, allowing researchers to iteratively refine questions, identify trends, and uncover hidden connections. By bridging intelligent agents with interactive visualization, MedViz transforms biomedical literature search into a dynamic, exploratory process that accelerates knowledge discovery.", "AI": {"tldr": "MedViz\u662f\u4e00\u4e2a\u7ed3\u5408\u591aAI\u4ee3\u7406\u4e0e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7684\u751f\u7269\u533b\u5b66\u6587\u732e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u548c\u667a\u80fd\u4ee3\u7406\u529f\u80fd\u652f\u6301\u5927\u89c4\u6a21\u6587\u732e\u63a2\u7d22\u4e0e\u77e5\u8bc6\u53d1\u73b0\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u7814\u7a76\u8005\u9762\u4e34\u6d77\u91cf\u591a\u6837\u5316\u6587\u732e\u7684\u5bfc\u822a\u6311\u6218\uff0c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u4ec5\u63d0\u4f9b\u6392\u540d\u6587\u672c\u5217\u8868\uff0c\u7f3a\u4e4f\u5168\u5c40\u63a2\u7d22\u548c\u6df1\u5ea6\u5206\u6790\u652f\u6301\u3002\u867d\u7136\u751f\u6210\u5f0fAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6458\u8981\u3001\u63d0\u53d6\u548c\u95ee\u7b54\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u8bdd\u5f0f\u5b9e\u73b0\u4e0e\u6587\u732e\u641c\u7d22\u5de5\u4f5c\u6d41\u6574\u5408\u4e0d\u8db3\u3002", "method": "MedViz\u6574\u5408\u591a\u4e2aAI\u4ee3\u7406\u4e0e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u7ed3\u5408\u6570\u767e\u4e07\u7bc7\u6587\u7ae0\u7684\u8bed\u4e49\u5730\u56fe\uff0c\u63d0\u4f9b\u67e5\u8be2\u3001\u6458\u8981\u548c\u5047\u8bbe\u751f\u6210\u7b49\u4ee3\u7406\u9a71\u52a8\u529f\u80fd\uff0c\u652f\u6301\u7814\u7a76\u8005\u8fed\u4ee3\u5f0f\u95ee\u9898\u7cbe\u70bc\u3001\u8d8b\u52bf\u8bc6\u522b\u548c\u9690\u85cf\u8fde\u63a5\u53d1\u73b0\u3002", "result": "MedViz\u5c06\u751f\u7269\u533b\u5b66\u6587\u732e\u641c\u7d22\u8f6c\u53d8\u4e3a\u52a8\u6001\u7684\u63a2\u7d22\u8fc7\u7a0b\uff0c\u901a\u8fc7\u667a\u80fd\u4ee3\u7406\u4e0e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7684\u7ed3\u5408\uff0c\u52a0\u901f\u77e5\u8bc6\u53d1\u73b0\u3002", "conclusion": "MedViz\u901a\u8fc7\u6574\u5408AI\u4ee3\u7406\u4e0e\u53ef\u89c6\u5316\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u751f\u7269\u533b\u5b66\u6587\u732e\u63a2\u7d22\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u53d1\u73b0\u5de5\u5177\u3002"}}

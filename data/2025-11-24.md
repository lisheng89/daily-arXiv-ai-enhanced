<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 4]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.16921)
*Liming Xiang,Jing Feng,Ziqi Yin,Zijian Li,Daihao Xue,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出了一种误差有界的近似最近邻搜索方法，通过δ-EMG图结构确保返回结果为真实值的(1/δ)近似，在保持高召回率的同时提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 当前近似最近邻搜索方法主要基于ε-召回率原则，只关注正确结果的召回率，无法保证错误结果的偏差范围。需要一种能提供理论误差保证的方法。

Method: 采用基于图的框架，提出δ-EMG（误差有界单调图），通过在构建过程中施加δ-单调几何约束，确保贪心搜索能收敛到(1/δ)近似邻居。进一步开发了δ-EMQG（误差有界单调量化图）来提升可扩展性。

Result: 在ANN-Benchmarks数据集上的实验显示，在召回率要求0.99时，在SIFT1M数据集上达到19,000 QPS，比其他方法性能提升超过40%。

Conclusion: 提出的误差有界近似最近邻搜索方法在保持高召回率的同时提供了理论误差保证，在大规模数据集上表现出优越的性能。

Abstract: Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\%.

</details>


### [2] [RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers](https://arxiv.org/abs/2511.16943)
*Tianyu Zhan,Kairui Fu,Zheqi Lv,Shengyu Zhang*

Main category: cs.IR

TL;DR: RASTP方法通过结合语义显著性和注意力中心性来动态剪枝输入序列中的低信息语义标记，在保持推荐性能的同时显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统使用语义标识符(SIDs)表示物品，但多个SIDs会显著增加输入序列长度，导致计算复杂度和内存消耗增加。现有方法主要优化注意力计算和KV缓存，但直接剪枝输入序列中的低信息标记可能更有效。

Method: 提出RASTP方法，通过结合语义显著性（通过表示幅度衡量）和注意力中心性（来自累积注意力权重）来评估标记重要性，动态剪枝低信息或无关的语义标记。

Result: 在三个真实世界Amazon数据集上的实验表明，RASTP减少了26.7%的训练时间，同时保持或略微提高了推荐性能。

Conclusion: RASTP通过直接剪枝输入序列中的低信息语义标记，有效解决了生成式推荐系统中序列长度过长的问题，在减少计算成本的同时维持了模型性能。

Abstract: Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.

</details>


### [3] [CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation](https://arxiv.org/abs/2511.17041)
*Xiangrui Xiong,Yichuan Lu,Zifei Pan,Chang Sun*

Main category: cs.IR

TL;DR: CLLMRec是一个基于大语言模型的MOOC概念推荐框架，通过语义对齐和先验知识蒸馏技术，无需依赖结构化知识图谱即可实现认知感知的个性化概念推荐。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高质量结构化知识图谱，但在真实教育场景中这类图谱往往稀缺，限制了概念推荐的实用性。

Method: 采用语义对齐构建统一表示空间，通过教师-学生架构进行先验知识蒸馏，结合深度知识追踪建模学习者实时认知状态。

Result: 在两个真实MOOC数据集上的实验表明，CLLMRec在多个评估指标上显著优于现有基线方法。

Conclusion: 该框架能够在不依赖显式结构先验的情况下，生成真正认知感知和个性化的概念推荐。

Abstract: The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.

</details>


### [4] [Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters](https://arxiv.org/abs/2511.17044)
*Zhan Su,Fengran Mo,Jian-yun Nie*

Main category: cs.IR

TL;DR: Poly-PRAG提出了一种新的参数化检索增强生成范式，通过潜在路由编码过程使用少量潜在LoRA适配器编码整个文档空间，解决了传统PRAG方法中一对一文档编码方案的数据稀缺和推理开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统PRAG方法采用一对一的文档编码方案，每个文档使用专用的LoRA适配器，这导致两个主要问题：训练数据稀缺（单个LoRA适配器的训练数据集有限）和推理时的高开销（需要为每个候选段落合并LLM权重和新的LoRA适配器）。

Method: 在离线编码阶段，将文档集合编码视为多任务学习过程，每个段落分配唯一任务标识符。通过路由函数使用少量潜在LoRA适配器编码整个段落空间。在线推理时，路由函数根据输入查询选择性地激活潜在专家子集。

Result: 在多个知识密集型NLP任务上的综合评估表明，Poly-PRAG在四个不同数据集上取得了最先进的结果，证明了该方法的有效性。

Conclusion: Poly-PRAG通过潜在路由编码过程有效解决了传统PRAG方法的数据稀缺和计算效率问题，为参数化检索增强生成提供了更高效的解决方案。

Abstract: Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.

</details>

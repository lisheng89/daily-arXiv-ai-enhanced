<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction](https://arxiv.org/abs/2512.07846)
*Guoyao Li,Ran He,Shusen Jing,Kayhan Behdin,Yubo Wang,Sundara Raman Ramachandran,Chanh Nguyen,Jian Sheng,Xiaojing Ma,Chuanrui Zhu,Sriram Vasudevan,Muchen Wu,Sayan Ghosh,Lin Su,Qingquan Song,Xiaoqing Wang,Zhipeng Wang,Qing Lan,Yanning Chen,Jingwei Wu,Luke Simon,Wenjing Zhang,Qi Guo,Fedor Borisyuk*

Main category: cs.IR

TL;DR: MixLM是一个新颖的LLM排序框架，通过将物品描述编码为少量嵌入token来减少输入上下文长度，在保持相关性的同时将吞吐量提升10倍，成功部署于LinkedIn搜索应用并带来显著用户增长。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在语义理解方面表现出色，但在工业级延迟和吞吐量要求下计算开销过高。特别是交叉编码器排序系统需要处理长上下文预填充工作负载，因为模型需要同时处理用户、查询和物品信息。

Method: 提出MixLM框架，使用混合交互（文本和嵌入token的混合）表示输入。将目录中所有物品编码为少量嵌入token并存储在近线缓存中，在线推理时使用编码后的物品描述，将物品长度从数千文本token减少到几个嵌入token。

Result: 在相同延迟预算下，MixLM将吞吐量提高了10.0倍，同时保持了相关性指标。效率提升使得LLM驱动的搜索能够全流量部署，在线A/B测试中每日活跃用户显著增加0.47%。

Conclusion: MixLM通过创新的混合交互方法有效解决了LLM排序系统的计算效率问题，在保持语义强度的同时大幅提升系统吞吐量，在实际工业应用中证明了其价值。

Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.

</details>


### [2] [Detecting Privileged Documents by Ranking Connected Network Entities](https://arxiv.org/abs/2512.08073)
*Jianping Zhang,Han Qin,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 提出基于邮件元数据构建人际关系网络的方法，通过分析实体与律师的互动频率来识别特权文档


<details>
  <summary>Details</summary>
Motivation: 传统特权文档识别方法可能不够准确，需要更有效的方法来识别涉及法律专业人员的特权通信

Method: 从邮件头元数据构建人际关系网络，将实体分类为律师和非律师，基于与律师的互动频率计算实体得分，结合连接强度识别特权文档

Result: 实验结果表明该算法能有效对法律实体进行排序，提高特权文档检测的准确性

Conclusion: 基于人际关系网络分析的方法能够有效识别特权文档，为法律文档审查提供了新的技术途径

Abstract: This paper presents a link analysis approach for identifying privileged documents by constructing a network of human entities derived from email header metadata. Entities are classified as either counsel or non-counsel based on a predefined list of known legal professionals. The core assumption is that individuals with frequent interactions with lawyers are more likely to participate in privileged communications. To quantify this likelihood, an algorithm assigns a score to each entity within the network. By utilizing both entity scores and the strength of their connections, the method enhances the identification of privileged documents. Experimental results demonstrate the algorithm's effectiveness in ranking legal entities for privileged document detection.

</details>


### [3] [A Comparative Study of Retrieval Methods in Azure AI Search](https://arxiv.org/abs/2512.08078)
*Qiang Mao,Han Qin,Robert Neary,Charles Wang,Fusheng Wei,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 评估Azure AI Search中不同检索方法（关键词、语义、向量、混合、混合语义）在电子取证早期案件评估中的表现，为法律从业者选择RAG配置提供指导


<details>
  <summary>Details</summary>
Motivation: 律师希望超越传统的关键词和语义搜索，利用大语言模型通过自然语言提问来更高效地获取文档审查中的关键信息，特别是在电子取证的早期案件评估阶段

Method: 在微软Azure的RAG框架中，比较Azure AI Search的五种检索方法：关键词检索、语义检索、向量检索、混合检索和混合语义检索，评估它们在早期案件评估任务中的表现

Result: 研究展示了每种检索方法生成的AI回答的准确性、相关性和一致性，为法律从业者提供了不同RAG配置的性能数据

Conclusion: 法律从业者可以利用本研究的结果来优化未来RAG配置的选择，提高电子取证早期案件评估的效率和效果

Abstract: Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.

</details>


### [4] [Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery](https://arxiv.org/abs/2512.08079)
*Qiang Mao,Fusheng Wei,Robert Neary,Charles Wang,Han Qin,Jianping Zhang,Nathaniel Huber-Fliflet*

Main category: cs.IR

TL;DR: 论文提出自动化图像聚类描述系统，结合图像聚类、图像描述和LLM，评估采样策略、提示技术和生成方法，为法律发现等大规模图像管理提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 数字图像快速增长给法律发现、数字档案和内容管理带来挑战，需要自动化方法来高效组织和描述大规模图像数据集，替代昂贵且不切实际的人工审核。

Method: 使用K-means聚类将图像分为20个视觉连贯的簇，用Azure AI Vision API生成基础描述。评估三个维度：1) 图像采样策略（随机、基于质心、分层、混合、密度采样 vs 全量）；2) 提示技术（标准提示 vs 思维链提示）；3) 描述生成方法（LLM生成 vs TF-IDF和模板方法）。使用语义相似度和覆盖率指标评估质量。

Result: 每个簇使用20张图像的战略采样与全量包含表现相当，但显著降低计算成本（仅分层采样略有下降）。LLM方法始终优于TF-IDF基线，标准提示在此任务中优于思维链提示。

Conclusion: 研究结果为部署可扩展、准确的聚类描述系统提供实用指导，支持法律发现和其他需要自动化组织大规模图像集合的领域的高容量工作流程。

Abstract: The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.

</details>


### [5] [Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters](https://arxiv.org/abs/2512.08083)
*Keith Huffman,Jianping Zhang,Nathaniel Huber-Fliflet,Fusheng Wei,Peter Gronvall*

Main category: cs.IR

TL;DR: 本文通过实证研究探讨了随机性在基于LLM的律师-客户特权文件检测分类中的作用，发现LLM能有效识别特权文件，随机性控制参数对分类性能影响很小，而利用随机性的方法能显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 在法律事务中，文本分类模型常用于筛选大型数据集以查找符合特定标准（如法律特权通信和律师指导文件）的文档。LLM在此领域表现出色，但随机性对分类输出的影响尚未充分研究，特别是在法律合规流程中。

Method: 本文进行了实证研究，聚焦四个关键维度：1) LLM识别法律特权文件的有效性；2) 随机性控制参数对分类输出的影响；3) 随机性对整体分类性能的影响；4) 利用随机性提高准确性的方法学。

Result: 实验结果表明：LLM能有效识别特权文件；随机性控制参数对分类性能影响很小；更重要的是，开发的利用随机性的方法能显著提高准确性。该方法还能增强企业对LLM输出的信心，特别是在制裁合规流程中。

Conclusion: 随着组织越来越多地依赖LLM增强合规工作流程，减少输出变异性有助于建立内部和监管机构对LLM衍生的制裁筛查决策的信心。利用随机性的方法不仅能提高准确性，还能增强企业在合规流程中对LLM输出的信任。

Abstract: In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.

</details>


### [6] [Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring](https://arxiv.org/abs/2512.08398)
*Jiin Park,Hyuna Jeon,Yoonseo Lee,Jisu Hong,Misuk Kim*

Main category: cs.IR

TL;DR: 本文提出一种基于LLM的三元组抽取方法，将工业标准文档转化为本体知识图谱，有效捕捉文档的层次和逻辑结构，显著提升KG-RAG在多种QA任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 工业标准文档包含大量技术信息和复杂规则，采用表格、适用范围、约束、例外和数值计算等高度结构化格式，使得知识图谱构建特别具有挑战性。传统方法难以有效表示这些领域特定的语义结构。

Method: 1) 将文档组织成层次化语义结构；2) 将句子和表格分解为基于条件和数值规则的原子命题；3) 通过LLM-based三元组抽取将其整合到本体知识图谱中；4) 构建规则、表格、多跳QA和有毒条款检测数据集；5) 实现本体感知的KG-RAG框架进行对比评估。

Result: 实验结果表明，该方法在所有QA类型上都比现有KG-RAG方法取得了显著的性能提升，证明了即使在条件、约束和适用范围交织的工业文档中，也能实现可靠且可扩展的知识表示。

Conclusion: 本研究证明了即使对于条件、约束和适用范围交织的工业文档，可靠且可扩展的知识表示也是可行的，为未来领域特定的RAG开发和智能文档管理做出了贡献。

Abstract: Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.

</details>


### [7] [VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation](https://arxiv.org/abs/2512.08702)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Zitong Wan,Hewei Wang,Weijie Liu,Yijie Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: VI-MMRec是一个模型无关、无需训练成本的框架，通过基于模态特征相似性的虚拟用户-物品交互来增强稀疏的多模态推荐数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐模型受限于普遍存在的数据稀疏问题，用户通常只与少量物品交互，导致模型将未观察到的物品任意视为负样本，限制了推荐效果。

Method: 提出VI-MMRec框架，通过两种策略构建虚拟用户-物品交互：1) Overlay策略独立聚合模态特定相似性以保留模态特定用户偏好；2) Synergistic策略整体融合跨模态相似性以捕捉互补用户偏好。采用统计信息权重分配机制自适应分配权重，作为即插即用框架与现有模型无缝集成。

Result: 在六个真实世界数据集上使用七个最先进的多模态推荐模型进行综合实验，验证了VI-MMRec的有效性。

Conclusion: VI-MMRec是一个灵活、无需训练成本的即插即用框架，能够显著增强现有多模态推荐模型的性能，具有实际部署优势。

Abstract: Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.

</details>

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation](https://arxiv.org/abs/2510.12815)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: 提出DAC4Rec框架，将扩散过程与强化学习结合，通过扩散模型的去噪能力增强离线推荐系统的鲁棒性，并采用Q值引导的策略优化来处理次优轨迹。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习推荐系统面临数据效率低、依赖预收集轨迹的问题，现有方法难以处理噪声数据且无法有效捕捉长期用户偏好，导致推荐策略不理想。

Method: 集成扩散过程与强化学习，利用扩散模型的去噪能力增强鲁棒性；采用Q值引导的策略优化策略；引入基于能量的采样策略减少推荐生成的随机性。

Result: 在六个真实世界离线数据集和在线模拟环境中验证了有效性，能够优化长期用户偏好，且扩散策略可无缝集成到其他RL算法中。

Conclusion: DAC4Rec框架通过扩散增强的actor-critic方法有效解决了离线RL推荐系统的挑战，具有广泛的适用性和良好的性能表现。

Abstract: Reinforcement learning-based recommender systems (RL4RS) have gained
attention for their ability to adapt to dynamic user preferences. However,
these systems face challenges, particularly in offline settings, where data
inefficiency and reliance on pre-collected trajectories limit their broader
applicability. While offline reinforcement learning methods leverage extensive
datasets to address these issues, they often struggle with noisy data and fail
to capture long-term user preferences, resulting in suboptimal recommendation
policies. To overcome these limitations, we propose Diffusion-enhanced
Actor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates
diffusion processes with reinforcement learning to model complex user
preferences more effectively. DAC4Rec leverages the denoising capabilities of
diffusion models to enhance the robustness of offline RL algorithms and
incorporates a Q-value-guided policy optimization strategy to better handle
suboptimal trajectories. Additionally, we introduce an energy-based sampling
strategy to reduce randomness during recommendation generation, ensuring more
targeted and reliable outcomes. We validate the effectiveness of DAC4Rec
through extensive experiments on six real-world offline datasets and in an
online simulation environment, demonstrating its ability to optimize long-term
user preferences. Furthermore, we show that the proposed diffusion policy can
be seamlessly integrated into other commonly used RL algorithms in RL4RS,
highlighting its versatility and wide applicability.

</details>


### [2] [Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior](https://arxiv.org/abs/2510.12816)
*Xiaocong Chen,Siyu Wang,Lina Yao*

Main category: cs.IR

TL;DR: MDT4Rec是一个基于决策变换器的离线强化学习推荐系统框架，通过将轨迹拼接从训练阶段转移到动作推断，并使用预训练大语言模型初始化来改善从次优历史数据中学习的能力。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习推荐系统在现实场景中面临的两个主要挑战：从次优用户反馈数据中学习和表示复杂的用户-物品交互。

Method: 1) 将轨迹拼接从训练阶段转移到动作推断阶段，允许系统在必要时缩短历史上下文；2) 使用预训练LLM初始化决策变换器进行知识迁移；3) 用多层感知机替换线性嵌入层；4) 采用低秩适应(LoRA)高效微调少量参数。

Result: 在五个公共数据集和在线模拟环境中的评估表明，MDT4Rec优于现有方法。

Conclusion: MDT4Rec通过改进的轨迹拼接策略和基于LLM的知识迁移，有效提升了离线强化学习推荐系统在次优数据环境下的性能。

Abstract: Reinforcement Learning-based recommender systems (RLRS) offer an effective
way to handle sequential recommendation tasks but often face difficulties in
real-world settings, where user feedback data can be sub-optimal or sparse. In
this paper, we introduce MDT4Rec, an offline RLRS framework that builds on the
Decision Transformer (DT) to address two major challenges: learning from
sub-optimal histories and representing complex user-item interactions. First,
MDT4Rec shifts the trajectory stitching procedure from the training phase to
action inference, allowing the system to shorten its historical context when
necessary and thereby ignore negative or unsuccessful past experiences. Second,
MDT4Rec initializes DT with a pre-trained large language model (LLM) for
knowledge transfer, replaces linear embedding layers with Multi-Layer
Perceptrons (MLPs) for more flexible representations, and employs Low-Rank
Adaptation (LoRA) to efficiently fine-tune only a small subset of parameters.
We evaluate MDT4Rec on five public datasets and in an online simulation
environment, demonstrating that it outperforms existing methods.

</details>


### [3] [Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering](https://arxiv.org/abs/2510.12959)
*Md Aminul Islam,Elena Zheleva,Ren Wang*

Main category: cs.IR

TL;DR: 提出了一种后处理流行度去偏方法PPD，用于纠正基于GNN的协同过滤中的流行度偏差，无需重新训练模型


<details>
  <summary>Details</summary>
Motivation: GNN模型在协同过滤中会通过消息传递机制传播和放大流行度偏差，现有方法通常修改训练目标但未能直接对抗GNN聚合过程中的偏差传播

Method: 通过估计交互级别的流行度，并使用流行度方向向量从节点表示中移除流行度成分，从而减少偏差同时保留用户偏好

Result: 实验结果表明该方法在GNN协同过滤的流行度偏差校正方面优于现有最先进方法

Conclusion: PPD方法能有效纠正GNN中的流行度偏差，提高推荐质量，且无需重新训练模型

Abstract: User historical interaction data is the primary signal for learning user
preferences in collaborative filtering (CF). However, the training data often
exhibits a long-tailed distribution, where only a few items have the majority
of interactions. CF models trained directly on such imbalanced data are prone
to learning popularity bias, which reduces personalization and leads to
suboptimal recommendation quality. Graph Neural Networks (GNNs), while
effective for CF due to their message passing mechanism, can further propagate
and amplify popularity bias through their aggregation process. Existing
approaches typically address popularity bias by modifying training objectives
but fail to directly counteract the bias propagated during GNN's neighborhood
aggregation. Applying weights to interactions during aggregation can help
alleviate this problem, yet it risks distorting model learning due to unstable
node representations in the early stages of training. In this paper, we propose
a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias
in GNN-based CF and operates directly on pre-trained embeddings without
requiring retraining. By estimating interaction-level popularity and removing
popularity components from node representations via a popularity direction
vector, PPD reduces bias while preserving user preferences. Experimental
results show that our method outperforms state-of-the-art approaches for
popularity bias correction in GNN-based CF.

</details>


### [4] [Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval](https://arxiv.org/abs/2510.13095)
*Yingchen zhang,Ruqing zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv*

Main category: cs.IR

TL;DR: 提出R4R框架，将自由形式的思维链推理转化为结构化格式，并在检索过程中迭代优化推理，以增强生成式检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式检索方法主要利用LLMs的生成能力，而忽略了其推理能力同样可以提升检索效果。初步研究发现思维链推理能改善检索，但存在冗长和与文档标识空间对齐不佳的问题。

Method: R4R框架：1) 生成初始结构化推理；2) 交替进行约束解码生成候选文档标识和基于检索结果更新推理；使用单个LLM同时作为推理生成器和检索器，无需额外模型或训练。

Result: 在Natural Questions、MS MARCO和真实世界商品搜索基准上的广泛实验验证了R4R的有效性。

Conclusion: R4R通过结构化推理和迭代优化机制，成功将LLMs的推理能力融入生成式检索，显著提升了检索性能。

Abstract: Generative retrieval (GR) is an emerging paradigm that leverages large
language models (LLMs) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of LLMs to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an LLM is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid decoding. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
LLM that has been instruction-tuned for GR. At inference time, R4R first uses
the LLM to generate an initial structured reasoning; then the same LLM
alternates between (i) constrained decoding with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single LLM serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.

</details>


### [5] [ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG](https://arxiv.org/abs/2510.13193)
*Yikuan Hu,Jifeng Zhu,Lanrui Tang,Chen Huang*

Main category: cs.IR

TL;DR: REMINDRAG是一种基于知识图谱的检索增强生成系统，通过LLM引导的图遍历和记忆重放机制，在保持成本效益的同时提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG-RAG系统难以在系统有效性和成本效率之间实现有效协同，导致性能不理想或计算成本过高。

Method: 采用LLM引导的图遍历策略，包括节点探索、节点利用和记忆重放，通过KG边嵌入记忆遍历经验。

Result: 理论和实验验证了REMINDRAG的有效性，在多个基准数据集和LLM骨干网络上均优于现有基线方法。

Conclusion: REMINDRAG通过记忆重放机制成功平衡了KG-RAG系统的有效性和成本效率，为知识图谱增强的检索生成系统提供了新的解决方案。

Abstract: Knowledge graphs (KGs), with their structured representation capabilities,
offer promising avenue for enhancing Retrieval Augmented Generation (RAG)
systems, leading to the development of KG-RAG systems. Nevertheless, existing
methods often struggle to achieve effective synergy between system
effectiveness and cost efficiency, leading to neither unsatisfying performance
nor excessive LLM prompt tokens and inference time. To this end, this paper
proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node
exploration, node exploitation, and, most notably, memory replay, to improve
both system effectiveness and cost efficiency. Specifically, REMINDRAG
memorizes traversal experience within KG edge embeddings, mirroring the way
LLMs "memorize" world knowledge within their parameters, but in a train-free
manner. We theoretically and experimentally confirm the effectiveness of
REMINDRAG, demonstrating its superiority over existing baselines across various
benchmark datasets and LLM backbones. Our code is available at
https://github.com/kilgrims/ReMindRAG.

</details>


### [6] [LLM-guided Hierarchical Retrieval](https://arxiv.org/abs/2510.13217)
*Nilesh Gupta,Wei-Cheng Chang,Ngot Bui,Cho-Jui Hsieh,Inderjit S. Dhillon*

Main category: cs.IR

TL;DR: LATTICE是一个分层检索框架，通过语义树结构组织语料库，使LLM能够以对数搜索复杂度在大规模语料库中进行推理和导航。


<details>
  <summary>Details</summary>
Motivation: 解决现有IR系统在处理复杂多面查询时的局限性：检索-重排序范式继承嵌入检索的缺陷，参数化生成方法难以更新新信息，长上下文方法计算不可行。

Method: 两阶段方法：离线阶段通过自底向上聚合或自顶向下分割策略构建语义层次结构；在线阶段使用搜索LLM导航树结构，通过校准潜在相关性评分算法处理LLM判断的噪声问题。

Result: 在BRIGHT基准测试中实现最先进的零样本性能，Recall@100提升9%，nDCG@10提升5%；与微调SOTA方法DIVER-v2在静态语料评估子集上表现相当。

Conclusion: LATTICE框架有效解决了LLM在大规模语料库中的推理导航问题，通过层次化结构实现了高效检索，且无需训练即可达到优异性能。

Abstract: Modern IR systems are increasingly tasked with answering complex,
multi-faceted queries that require deep reasoning rather than simple keyword or
semantic matching. While LLM-based IR has shown great promise, the prevailing
retrieve-then-rerank paradigm inherits the limitations of embedding-based
retrieval; parametric generative approaches are difficult to update with new
information; and long-context methods that place the entire corpus in context
are computationally infeasible for large document collections. To address these
challenges, we introduce LATTICE, a hierarchical retrieval framework that
enables an LLM to reason over and navigate large corpora with logarithmic
search complexity by imposing a semantic tree structure on the corpus. Our
approach consists of two stages: (1) an offline phase that organizes the corpus
into a semantic hierarchy via either a bottom-up agglomerative strategy or a
top-down divisive strategy using multi-level summaries and (2) an online
traversal phase where a search LLM navigates this tree. A central challenge in
such LLM-guided search is that the model's relevance judgments are noisy,
context-dependent, and unaware of the hierarchy, making cross-branch and
cross-level comparisons difficult. To overcome this, we propose a traversal
algorithm that estimates calibrated latent relevance scores from local LLM
outputs and aggregates them into a global path relevance metric. Our
training-free framework achieves state-of-the-art zero-shot performance on the
reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in
Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline.
Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains
comparable results on BRIGHT subsets that use a static corpus for evaluation.

</details>


### [7] [Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation](https://arxiv.org/abs/2510.13229)
*Yi Zhang,Lili Xie,Ruihong Qiu,Jiajun Liu,Sen Wang*

Main category: cs.IR

TL;DR: 提出一种基于离线强化学习的新框架，通过模仿学习从LLM生成的轨迹中提取奖励模型，避免LLM微调，降低计算开销，并在推荐系统中实现优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中具有潜力，但直接部署存在延迟、幻觉和偏见等问题，需要更高效的解决方案。

Method: 使用逆强化学习从LLM演示中提取奖励模型，通过离线强化学习框架训练推荐策略，无需LLM微调。

Result: 在两个基准数据集上的实验验证了方法的有效性，性能优于最先进的基于强化学习和上下文学习的基线方法。

Conclusion: 该框架成功将LLM的语义理解能力转移到推荐策略中，解决了LLM直接部署的挑战，同时保持了高性能。

Abstract: Recommender systems (RecSys) have become critical tools for enhancing user
engagement by delivering personalized content across diverse digital platforms.
Recent advancements in large language models (LLMs) demonstrate significant
potential for improving RecSys, primarily due to their exceptional
generalization capabilities and sophisticated contextual understanding, which
facilitate the generation of flexible and interpretable recommendations.
However, the direct deployment of LLMs as primary recommendation policies
presents notable challenges, including persistent latency issues stemming from
frequent API calls and inherent model limitations such as hallucinations and
biases. To address these issues, this paper proposes a novel offline
reinforcement learning (RL) framework that leverages imitation learning from
LLM-generated trajectories. Specifically, inverse reinforcement learning is
employed to extract robust reward models from LLM demonstrations. This approach
negates the need for LLM fine-tuning, thereby substantially reducing
computational overhead. Simultaneously, the RL policy is guided by the
cumulative rewards derived from these demonstrations, effectively transferring
the semantic insights captured by the LLM. Comprehensive experiments conducted
on two benchmark datasets validate the effectiveness of the proposed method,
demonstrating superior performance when compared against state-of-the-art
RL-based and in-context learning baselines. The code can be found at
https://github.com/ArronDZhang/IL-Rec.

</details>


### [8] [Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models](https://arxiv.org/abs/2510.13359)
*Yuki Yada,Sho Akiyama,Ryo Watanabe,Yuta Ueno,Yusuke Shido,Andre Rusli*

Main category: cs.IR

TL;DR: 在Mercari电商平台上应用视觉语言模型进行商品推荐，通过微调SigLIP模型并使用商品图像-标题对生成嵌入，在离线和在线测试中均取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 在拥有数千万月活跃用户的大规模电商平台上，推荐视觉相似商品对于帮助用户高效发现符合偏好的商品至关重要。

Method: 微调基于sigmoid对比损失的SigLIP视觉语言模型，使用100万个Mercari商品图像-标题对开发用于推荐系统的图像编码器。

Result: 离线分析中nDCG@5提升9.1%；在线A/B测试中点击率提升50%，转化率提升14%。

Conclusion: 基于视觉语言模型的编码器在电商商品推荐中具有显著效果，为开发基于视觉相似性的推荐系统提供了实用见解。

Abstract: On large-scale e-commerce platforms with tens of millions of active monthly
users, recommending visually similar products is essential for enabling users
to efficiently discover items that align with their preferences. This study
presents the application of a vision-language model (VLM) -- which has
demonstrated strong performance in image recognition and image-text retrieval
tasks -- to product recommendations on Mercari, a major consumer-to-consumer
marketplace used by more than 20 million monthly users in Japan. Specifically,
we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using
one million product image-title pairs from Mercari collected over a three-month
period, and developed an image encoder for generating item embeddings used in
the recommendation system. Our evaluation comprised an offline analysis of
historical interaction logs and an online A/B test in a production environment.
In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared
with the baseline. In the online A/B test, the click-through rate improved by
50% whereas the conversion rate improved by 14% compared with the existing
model. These results demonstrate the effectiveness of VLM-based encoders for
e-commerce product recommendations and provide practical insights into the
development of visual similarity-based recommendation systems.

</details>


### [9] [MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation](https://arxiv.org/abs/2510.13371)
*Jiin Park,Misuk Kim*

Main category: cs.IR

TL;DR: MADRec是一个基于LLM的多方面驱动推荐代理，通过无监督提取评论中的多方面信息构建用户和物品画像，实现直接推荐、序列推荐和解释生成，在精度和可解释性方面优于传统和LLM基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM集成到推荐系统的方法大多局限于简单文本生成或静态提示推理，无法捕捉用户偏好和真实世界交互的复杂性。

Method: 通过基于方面类别的总结生成结构化画像，应用重排序构建高密度输入，当输出中缺少真实物品时使用自反馈机制动态调整推理标准。

Result: 跨多个领域的实验表明，MADRec在精度和可解释性方面优于传统和LLM基线方法，人工评估进一步证实了生成解释的说服力。

Conclusion: MADRec展示了LLM代理在推荐系统中的潜力，能够有效处理复杂用户偏好并生成有说服力的解释。

Abstract: Recent attempts to integrate large language models (LLMs) into recommender
systems have gained momentum, but most remain limited to simple text generation
or static prompt-based inference, failing to capture the complexity of user
preferences and real-world interactions. This study proposes the Multi-Aspect
Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs
user and item profiles by unsupervised extraction of multi-aspect information
from reviews and performs direct recommendation, sequential recommendation, and
explanation generation. MADRec generates structured profiles via
aspect-category-based summarization and applies Re-Ranking to construct
high-density inputs. When the ground-truth item is missing from the output, the
Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments
across multiple domains show that MADRec outperforms traditional and LLM-based
baselines in both precision and explainability, with human evaluation further
confirming the persuasiveness of the generated explanations.

</details>


### [10] [RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge](https://arxiv.org/abs/2510.13590)
*Jiale Han,Austin Cheung,Yubai Wei,Zheng Yu,Xusheng Wang,Bing Zhu,Yi Yang*

Main category: cs.IR

TL;DR: 提出了Temporal GraphRAG (TG-RAG)方法，通过构建双层时序图来增强RAG系统的时间感知能力，解决现有方法在时序知识表示和增量更新方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统忽略了知识的时序特性，难以区分不同时间点的相同事实，且缺乏有效的增量更新机制和时序评估方法。

Method: 构建包含时序知识图和分层时间图的双层时序图，生成多粒度时序摘要，支持增量更新和动态子图检索。

Result: TG-RAG在时序问答任务上显著优于现有基线方法，证明了其在处理时序知识和增量更新方面的有效性。

Conclusion: TG-RAG通过时序图表示和分层时间结构，成功解决了RAG系统的时间感知问题，为时序知识管理提供了有效解决方案。

Abstract: Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

</details>


### [11] [HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation](https://arxiv.org/abs/2510.13738)
*Jingyi Zhou,Cheng Chen,Kai Zuo,Manjie Xu,Zhendong Fu,Yibo Chen,Xu Tang,Yao Hu*

Main category: cs.IR

TL;DR: 提出HyMiRec混合多兴趣序列推荐框架，解决LLM推荐中长序列截断和兴趣多样性不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐方法因推理延迟和特征获取限制，只能截断用户行为序列，丢失长期偏好信号；且依赖单预测嵌入，忽视用户兴趣的多样性

Method: 使用轻量级推荐器提取长序列的粗粒度兴趣嵌入，LLM推荐器捕获细粒度兴趣嵌入；引入基于余弦相似度的残差码本压缩用户历史嵌入；设计解耦多兴趣学习模块自适应学习多个兴趣信号

Result: 在基准数据集和工业数据集上的实验表明优于现有方法，在线A/B测试显示在真实推荐系统中带来持续改进

Conclusion: HyMiRec通过混合架构有效解决了LLM推荐中的长序列建模和兴趣多样性问题，在真实场景中表现出色

Abstract: Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

</details>

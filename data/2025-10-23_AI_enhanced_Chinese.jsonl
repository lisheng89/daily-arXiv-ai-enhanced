{"id": "2510.18936", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18936", "abs": "https://arxiv.org/abs/2510.18936", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Samita Bai", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "SBAN: A Framework \\& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining", "comment": null, "summary": "This paper introduces SBAN (Source code, Binary, Assembly, and Natural\nLanguage Description), a large-scale, multi-dimensional dataset designed to\nadvance the pre-training and evaluation of large language models (LLMs) for\nsoftware code analysis. SBAN comprises more than 3 million samples, including\n2.9 million benign and 672,000 malware respectively, each represented across\nfour complementary layers: binary code, assembly instructions, natural language\ndescriptions, and source code. This unique multimodal structure enables\nresearch on cross-representation learning, semantic understanding of software,\nand automated malware detection. Beyond security applications, SBAN supports\nbroader tasks such as code translation, code explanation, and other software\nmining tasks involving heterogeneous data. It is particularly suited for\nscalable training of deep models, including transformers and other LLM\narchitectures. By bridging low-level machine representations and high-level\nhuman semantics, SBAN provides a robust foundation for building intelligent\nsystems that reason about code. We believe that this dataset opens new\nopportunities for mining software behavior, improving security analytics, and\nenhancing LLM capabilities in pre-training and fine-tuning tasks for software\ncode mining.", "AI": {"tldr": "SBAN\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u7ef4\u5ea6\u6570\u636e\u96c6\uff0c\u5305\u542b300\u591a\u4e07\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u4e8c\u8fdb\u5236\u4ee3\u7801\u3001\u6c47\u7f16\u6307\u4ee4\u3001\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u6e90\u4ee3\u7801\u56db\u4e2a\u5c42\u6b21\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u4ee3\u7801\u5206\u6790\u4e2d\u7684\u9884\u8bad\u7ec3\u548c\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u5206\u6790\u6570\u636e\u96c6\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u8868\u793a\u5f62\u5f0f\uff0c\u7f3a\u4e4f\u8de8\u8868\u793a\u5f62\u5f0f\u7684\u5b66\u4e60\u80fd\u529b\u3002SBAN\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u7ed3\u6784\u8fde\u63a5\u4f4e\u7ea7\u673a\u5668\u8868\u793a\u548c\u9ad8\u7ea7\u4eba\u7c7b\u8bed\u4e49\uff0c\u4e3a\u6784\u5efa\u667a\u80fd\u4ee3\u7801\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u6784\u5efa\u5305\u542b300\u591a\u4e07\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d290\u4e07\u4e2a\u826f\u6027\u6837\u672c\u548c67.2\u4e07\u4e2a\u6076\u610f\u8f6f\u4ef6\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u63d0\u4f9b\u4e8c\u8fdb\u5236\u4ee3\u7801\u3001\u6c47\u7f16\u6307\u4ee4\u3001\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u548c\u6e90\u4ee3\u7801\u56db\u79cd\u8868\u793a\u5f62\u5f0f\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6a21\u6001\u7684\u8f6f\u4ef6\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u652f\u6301\u8de8\u8868\u793a\u5b66\u4e60\u3001\u8f6f\u4ef6\u8bed\u4e49\u7406\u89e3\u548c\u81ea\u52a8\u5316\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7b49\u7814\u7a76\u4efb\u52a1\u3002", "conclusion": "SBAN\u6570\u636e\u96c6\u4e3a\u8f6f\u4ef6\u884c\u4e3a\u6316\u6398\u3001\u5b89\u5168\u5206\u6790\u6539\u8fdb\u4ee5\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u4ee3\u7801\u6316\u6398\u4e2d\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u662f\u6784\u5efa\u667a\u80fd\u4ee3\u7801\u63a8\u7406\u7cfb\u7edf\u7684\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2510.19006", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19006", "abs": "https://arxiv.org/abs/2510.19006", "authors": ["Hamed Jelodar", "Mohammad Meymani", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security", "comment": null, "summary": "Generative AI and large language models (LLMs) have shown strong capabilities\nin code understanding, but their use in cybersecurity, particularly for malware\ndetection and analysis, remains limited. Existing detection systems often fail\nto generalize to obfuscated or previously unseen threats, underscoring the need\nfor more adaptable and explainable models. To address this challenge, we\nintroduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and\npretrained on a large-scale corpus of over one million malware samples,\nspanning both source and assembly code. XGen-Q uses a multi-stage prompt\nstrategy combined with retrieval-augmented generation (RAG) to deliver reliable\nmalware identification and detailed forensic reporting, even in the presence of\ncomplex code obfuscation. To further enhance generalization, we design a\ntraining pipeline that systematically exposes the model to diverse obfuscation\npatterns. Experimental results show that XGen-Q achieves significantly lower\nperplexity than competitive baselines and exhibits strong performance on novel\nmalware samples, demonstrating the promise of LLM-based approaches for\ninterpretable and robust malware analysis.", "AI": {"tldr": "XGen-Q\u662f\u57fa\u4e8eQwen-Coder\u67b6\u6784\u7684\u9886\u57df\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u548c\u5206\u6790\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u7b56\u7565\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u5728\u590d\u6742\u4ee3\u7801\u6df7\u6dc6\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u6076\u610f\u8f6f\u4ef6\u8bc6\u522b\u548c\u8be6\u7ec6\u53d6\u8bc1\u62a5\u544a\u3002", "motivation": "\u73b0\u6709\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u5bf9\u6df7\u6dc6\u6216\u672a\u77e5\u5a01\u80c1\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002\u751f\u6210\u5f0fAI\u548cLLM\u5728\u4ee3\u7801\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7f51\u7edc\u5b89\u5168\u7279\u522b\u662f\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002", "method": "\u57fa\u4e8eQwen-Coder\u67b6\u6784\u6784\u5efa\u9886\u57df\u9002\u5e94LLM\uff0c\u5728\u8d85\u8fc7100\u4e07\u4e2a\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u6db5\u76d6\u6e90\u4ee3\u7801\u548c\u6c47\u7f16\u4ee3\u7801\u3002\u91c7\u7528\u591a\u9636\u6bb5\u63d0\u793a\u7b56\u7565\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u8bbe\u8ba1\u7cfb\u7edf\u66b4\u9732\u6a21\u578b\u4e8e\u591a\u6837\u5316\u6df7\u6dc6\u6a21\u5f0f\u7684\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aXGen-Q\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\uff0c\u5e76\u5728\u65b0\u578b\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u65b9\u9762\u5177\u6709\u524d\u666f\uff0cXGen-Q\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.19221", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19221", "abs": "https://arxiv.org/abs/2510.19221", "authors": ["Yingchen Zhang", "Ruqing Zhang", "Jiafeng Guo", "Wenjun Peng", "Sen Li", "Fuyu Lv", "Xueqi Cheng"], "title": "C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search", "comment": null, "summary": "Designing document identifiers (docids) that carry rich semantic information\nwhile maintaining tractable search spaces is a important challenge in\ngenerative retrieval (GR). Popular codebook methods address this by building a\nhierarchical semantic tree and constraining generation to its child nodes, yet\ntheir numeric identifiers cannot leverage the large language model's pretrained\nnatural language understanding. Conversely, using text as docid provides more\nsemantic expressivity but inflates the decoding space, making the system\nbrittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)\nfirst construct semantic numerical docid via hierarchical clustering; (ii) then\nextract high-frequency metadata keywords and iteratively replace each numeric\nlabel with its cluster's top-K keywords; and (iii) an optional two-level\nsemantic smoothing step further enhances the fluency of C2T-ID. Experiments on\nNatural Questions and Taobao's product search demonstrate that C2T-ID\nsignificantly outperforms atomic, semantic codebook, and pure-text docid\nbaselines, demonstrating its effectiveness in balancing semantic expressiveness\nwith search space constraints.", "AI": {"tldr": "C2T-ID\u662f\u4e00\u79cd\u7ed3\u5408\u6570\u503c\u6807\u8bc6\u7b26\u548c\u6587\u672c\u6807\u8bc6\u7b26\u4f18\u52bf\u7684\u6587\u6863\u6807\u8bc6\u7b26\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u6784\u5efa\u8bed\u4e49\u6570\u503cdocid\uff0c\u518d\u7528\u9ad8\u9891\u5173\u952e\u8bcd\u66ff\u6362\u6570\u503c\u6807\u7b7e\uff0c\u5e73\u8861\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u548c\u641c\u7d22\u7a7a\u95f4\u7ea6\u675f\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u6587\u6863\u6807\u8bc6\u7b26\u8bbe\u8ba1\u7684\u6311\u6218\uff1a\u6570\u503c\u6807\u8bc6\u7b26\u65e0\u6cd5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u800c\u7eaf\u6587\u672c\u6807\u8bc6\u7b26\u4f1a\u6269\u5927\u89e3\u7801\u7a7a\u95f4\u4e14\u5bf9\u65e9\u671f\u9519\u8bef\u654f\u611f\u3002", "method": "1) \u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u6784\u5efa\u8bed\u4e49\u6570\u503cdocid\uff1b2) \u63d0\u53d6\u9ad8\u9891\u5143\u6570\u636e\u5173\u952e\u8bcd\u5e76\u8fed\u4ee3\u66ff\u6362\u6570\u503c\u6807\u7b7e\uff1b3) \u53ef\u9009\u7684\u4e24\u7ea7\u8bed\u4e49\u5e73\u6ed1\u6b65\u9aa4\u589e\u5f3aC2T-ID\u7684\u6d41\u7545\u6027\u3002", "result": "\u5728Natural Questions\u548c\u6dd8\u5b9d\u4ea7\u54c1\u641c\u7d22\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cC2T-ID\u663e\u8457\u4f18\u4e8e\u539f\u5b50\u6807\u8bc6\u7b26\u3001\u8bed\u4e49\u7801\u672c\u548c\u7eaf\u6587\u672cdocid\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "C2T-ID\u5728\u5e73\u8861\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u548c\u641c\u7d22\u7a7a\u95f4\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u7684\u6587\u6863\u6807\u8bc6\u7b26\u8bbe\u8ba1\u95ee\u9898\u3002"}}
{"id": "2510.19340", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19340", "abs": "https://arxiv.org/abs/2510.19340", "authors": ["L. Caspari", "M. Dinzinger", "K. Gosh Dastidar", "C. Fellicious", "J. Mitrovi\u0107", "M. Granitzer"], "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale", "comment": null, "summary": "Dense retrieval systems have proven to be effective across various\nbenchmarks, but require substantial memory to store large search indices.\nRecent advances in embedding compression show that index sizes can be greatly\nreduced with minimal loss in ranking quality. However, existing studies often\noverlook the role of corpus complexity -- a critical factor, as recent work\nshows that both corpus size and document length strongly affect dense retrieval\nperformance. In this paper, we introduce CoRECT (Controlled Retrieval\nEvaluation of Compression Techniques), a framework for large-scale evaluation\nof embedding compression methods, supported by a newly curated dataset\ncollection. To demonstrate its utility, we benchmark eight representative types\nof compression methods. Notably, we show that non-learned compression achieves\nsubstantial index size reduction, even on up to 100M passages, with\nstatistically insignificant performance loss. However, selecting the optimal\ncompression method remains challenging, as performance varies across models.\nSuch variability highlights the necessity of CoRECT to enable consistent\ncomparison and informed selection of compression methods. All code, data, and\nresults are available on GitHub and HuggingFace.", "AI": {"tldr": "CoRECT\u6846\u67b6\u7528\u4e8e\u5927\u89c4\u6a21\u8bc4\u4f30\u5d4c\u5165\u538b\u7f29\u65b9\u6cd5\uff0c\u5728100M\u6bb5\u843d\u4e0a\u975e\u5b66\u4e60\u538b\u7f29\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c0f\u7d22\u5f15\u5927\u5c0f\u4e14\u6027\u80fd\u635f\u5931\u7edf\u8ba1\u4e0d\u663e\u8457\uff0c\u4f46\u6700\u4f73\u538b\u7f29\u65b9\u6cd5\u9009\u62e9\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565\u8bed\u6599\u590d\u6742\u6027\u5bf9\u7a20\u5bc6\u68c0\u7d22\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u800c\u8bed\u6599\u5927\u5c0f\u548c\u6587\u6863\u957f\u5ea6\u662f\u91cd\u8981\u56e0\u7d20\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u538b\u7f29\u65b9\u6cd5\u5728\u4e0d\u540c\u8bed\u6599\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faCoRECT\u6846\u67b6\uff0c\u5305\u542b\u65b0\u7b56\u5212\u7684\u6570\u636e\u96c6\u96c6\u5408\uff0c\u5bf98\u79cd\u4ee3\u8868\u6027\u538b\u7f29\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5d4c\u5165\u538b\u7f29\u8bc4\u4f30\u3002", "result": "\u975e\u5b66\u4e60\u538b\u7f29\u65b9\u6cd5\u5728100M\u6bb5\u843d\u4e0a\u80fd\u5b9e\u73b0\u663e\u8457\u7684\u7d22\u5f15\u5927\u5c0f\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931\u7edf\u8ba1\u4e0d\u663e\u8457\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u95f4\u6027\u80fd\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u538b\u7f29\u65b9\u6cd5\u9009\u62e9\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981CoRECT\u6846\u67b6\u6765\u786e\u4fdd\u4e00\u81f4\u6bd4\u8f83\u548c\u660e\u667a\u9009\u62e9\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u548c\u7ed3\u679c\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.19758", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.19758", "abs": "https://arxiv.org/abs/2510.19758", "authors": ["Joseph Casale", "Andrew Silverschotz", "Joseph DeSimone"], "title": "Top-P Masking for Cross Language Information Retrieval", "comment": "Unsubmitted", "summary": "Top-K masking schemes have been proposed as a method to promote sparse\nrepresentations in Information Retrieval (IR) tasks, as a simple alternative to\nFloating Point Operations per Second (FLOPS) regularization. Algorithms such as\nBilingual Lexical and Document Expansion Model (BLADE), adopt this approach as\na post-processing stage. We propose using Top-P Dynamic Masking similar to\nNucleus Sampling in Large Language Models, and demonstrate better performance\nthan Top-K masking. Specifically, we evaluate our methods in the domain of\nCross Language Information Retrieval (CLIR)", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528Top-P\u52a8\u6001\u63a9\u7801\uff08\u7c7b\u4f3c\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6838\u91c7\u6837\uff09\u66ff\u4ee3Top-K\u63a9\u7801\u65b9\u6848\uff0c\u5728\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "Top-K\u63a9\u7801\u65b9\u6848\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u4fc3\u8fdb\u7a00\u758f\u8868\u793a\u7684\u7b80\u5355\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u5982BLADE\u4ec5\u5c06\u5176\u4f5c\u4e3a\u540e\u5904\u7406\u9636\u6bb5\u4f7f\u7528\u3002", "method": "\u91c7\u7528Top-P\u52a8\u6001\u63a9\u7801\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6838\u91c7\u6837\u6280\u672f\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684Top-K\u63a9\u7801\u65b9\u6848\u3002", "result": "\u5728\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cTop-P\u52a8\u6001\u63a9\u7801\u65b9\u6cd5\u6bd4Top-K\u63a9\u7801\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Top-P\u52a8\u6001\u63a9\u7801\u662f\u6bd4Top-K\u63a9\u7801\u66f4\u6709\u6548\u7684\u7a00\u758f\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002"}}

{"id": "2511.10962", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.10962", "abs": "https://arxiv.org/abs/2511.10962", "authors": ["Xintian Han", "Honggang Chen", "Quan Lin", "Jingyue Gao", "Xiangyuan Ren", "Lifei Zhu", "Zhisheng Ye", "Shikang Wu", "XiongHang Xie", "Xiaochu Gan", "Bingzheng Wei", "Peng Xu", "Zhe Wang", "Yuchao Zheng", "Jingjian Lin", "Di Wu", "Junfeng Ge"], "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation", "comment": null, "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.", "AI": {"tldr": "LEMUR\u662f\u9996\u4e2a\u4ece\u539f\u59cb\u6570\u636e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u591a\u6a21\u6001\u548c\u63a8\u8350\u7ec4\u4ef6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u4e2d\u7684\u76ee\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u5728\u6296\u97f3\u641c\u7d22\u548c\u5e7f\u544a\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eID\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u548c\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u73b0\u6709\u7684\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u5b66\u4e60\u4e0e\u63a8\u8350\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u4e14\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u65b0\u6570\u636e\u3002", "method": "\u63d0\u51faLEMUR\u7cfb\u7edf\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u5f0f\u8054\u5408\u4f18\u5316\u591a\u6a21\u6001\u548c\u63a8\u8350\u7ec4\u4ef6\uff1b\u5f15\u5165\u65b0\u9896\u7684\u8bb0\u5fc6\u5e93\u673a\u5236\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u589e\u91cf\u7d2f\u79ef\u5386\u53f2\u591a\u6a21\u6001\u8868\u793a\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u6296\u97f3\u641c\u7d22\u90e8\u7f72\u4e00\u4e2a\u6708\u540e\uff0cLEMUR\u4f7f\u67e5\u8be2\u53d8\u66f4\u7387\u8870\u51cf\u51cf\u5c110.843%\uff0cQAUC\u63d0\u53470.81%\uff1b\u5728\u6296\u97f3\u5e7f\u544a\u7684\u591a\u4e2a\u79bb\u7ebf\u6307\u6807\u4e0a\u4e5f\u663e\u793a\u51fa\u663e\u8457\u589e\u76ca\u3002", "conclusion": "\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7aef\u5230\u7aef\u591a\u6a21\u6001\u63a8\u8350\u5728\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.11010", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.11010", "abs": "https://arxiv.org/abs/2511.11010", "authors": ["Kyle Deeds", "Ying-Hsiang Huang", "Claire Gong", "Shreya Shaji", "Alison Yan", "Leslie Harka", "Samuel J Klein", "Shannon Zejiang Shen", "Mark Phillips", "Trevor Owens", "Benjamin Charles Germain Lee"], "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs", "comment": "10 pages, 5 figures, 2 tables", "summary": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.", "AI": {"tldr": "GovScape\u662f\u4e00\u4e2a\u652f\u6301\u591a\u6a21\u6001\u641c\u7d22\u7684\u516c\u5171\u7cfb\u7edf\uff0c\u53ef\u5bf91000\u4e07\u4efd\u653f\u5e9cPDF\u6587\u4ef6\u8fdb\u884c\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u7cbe\u786e\u6587\u672c\u641c\u7d22\u3001\u8bed\u4e49\u6587\u672c\u641c\u7d22\u548c\u89c6\u89c9\u641c\u7d22\u3002", "motivation": "\u73b0\u6709\u7684\u7f51\u9875\u6863\u6848\u867d\u7136\u4fdd\u5b58\u4e86\u5927\u91cf\u653f\u5e9cPDF\u6587\u4ef6\uff0c\u4f46\u5728\u8bbf\u95ee\u548c\u53d1\u73b0\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u76ee\u524d\u7684\u6d4f\u89c8\u65b9\u5f0f\u4ec5\u9650\u4e8e\u4e0b\u8f7d\u5355\u4e2aPDF\u548c\u57fa\u672c\u5173\u952e\u8bcd\u641c\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u7cbe\u786e\u6587\u672c\u641c\u7d22\u3001\u8bed\u4e49\u6587\u672c\u641c\u7d22\u548c\u89c6\u89c9\u641c\u7d22\u7684\u591a\u6a21\u6001\u641c\u7d22\u7cfb\u7edf\uff0c\u652f\u6301\u5bf91000\u4e07\u4efdPDF\u6587\u4ef6\u8fdb\u884c\u8de8\u9875\u641c\u7d22\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86GovScape\u7cfb\u7edf\uff0c\u5904\u74061000\u4e07\u4efdPDF\u7684\u603b\u8ba1\u7b97\u6210\u672c\u7ea6\u4e3a1500\u7f8e\u5143\uff0c\u76f8\u5f53\u4e8e\u6bcf\u7f8e\u5143\u5904\u740647000\u9875PDF\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "GovScape\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u641c\u7d22\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5df2\u5f00\u59cb\u54111\u4ebf+PDF\u89c4\u6a21\u6269\u5c55\uff0c\u4e3a\u653f\u5e9c\u6587\u6863\u7684\u8bbf\u95ee\u548c\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11172", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11172", "abs": "https://arxiv.org/abs/2511.11172", "authors": ["Mubaraka Sani Ibrahim", "Isah Charles Saidu", "Lehel Csato"], "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition", "comment": "((1) African University of Science and Technology (Abuja, Nigeria), (2) Baze University (Abuja, Nigeria), (3) Babes-Bolyai University (Cluj-Napoca, Romania))", "summary": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u63d2\u8865\u5947\u5f02\u503c\u5206\u89e3\u7684\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edfGroup Soft-Impute SVD\uff0c\u7528\u4e8e\u89e3\u51b3\u7fa4\u7ec4\u63a8\u8350\u4e2d\u7684\u6570\u636e\u7a00\u758f\u6027\u548c\u9ad8\u7ef4\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u7fa4\u7ec4\u6d3b\u52a8\u7684\u666e\u53ca\uff0c\u9700\u8981\u57fa\u4e8e\u7fa4\u7ec4\u6210\u5458\u96c6\u4f53\u504f\u597d\u4e3a\u7fa4\u7ec4\u63d0\u4f9b\u63a8\u8350\u3002\u73b0\u6709\u7fa4\u7ec4\u63a8\u8350\u7cfb\u7edf\u5728\u5904\u7406\u7a00\u758f\u9ad8\u7ef4\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u8f6f\u63d2\u8865\u5947\u5f02\u503c\u5206\u89e3(SVD)\u8fdb\u884c\u4f4e\u79e9\u77e9\u9635\u8865\u5168\uff0c\u901a\u8fc7Group Soft-Impute SVD\u65b9\u6cd5\u589e\u5f3a\u7fa4\u7ec4\u63a8\u8350\u6027\u80fd\u3002", "result": "\u5728Goodbooks\u3001Movielens\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u7528\u6237\u7fa4\u7ec4\u7684\u53ec\u56de\u7387\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7fa4\u7ec4\u77e9\u9635\u5206\u89e3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6240\u6709\u7fa4\u7ec4\u89c4\u6a21\u4e0a\u90fd\u80fd\u8fbe\u5230\u53ef\u6bd4\u7ed3\u679c\uff0c\u4e14\u80fd\u6062\u590d\u66f4\u4f4e\u7684\u77e9\u9635\u79e9\u3002", "conclusion": "Group Soft-Impute SVD\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u7a00\u758f\u6570\u636e\uff0c\u5728\u7fa4\u7ec4\u63a8\u8350\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.11255", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11255", "abs": "https://arxiv.org/abs/2511.11255", "authors": ["Wencai Ye", "Mingjie Sun", "Shuhang Chen", "Wenjin Wu", "Peng Jiang"], "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation", "comment": "Accepted by AAAI 2026 (Oral)", "summary": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.", "AI": {"tldr": "Align\u00b3GR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00token\u7ea7\u3001\u884c\u4e3a\u5efa\u6a21\u7ea7\u548c\u504f\u597d\u7ea7\u5bf9\u9f50\uff0c\u89e3\u51b3LLMs\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u548c\u884c\u4e3a\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8aSOTA\u57fa\u7ebf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u7ed3\u6784\u5316\u4e16\u754c\u77e5\u8bc6\u548c\u591a\u6b65\u63a8\u7406\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5c06\u5176\u8f6c\u5316\u4e3a\u73b0\u5b9e\u63a8\u8350\u7cfb\u7edf\u65f6\u9762\u4e34\u8bed\u4e49\u548c\u884c\u4e3a\u4e0d\u5bf9\u9f50\u7684\u6839\u672c\u6311\u6218\u3002", "method": "\u63d0\u51faAlign\u00b3GR\u6846\u67b6\uff1a\u53cctokenization\u878d\u5408\u7528\u6237-\u9879\u76ee\u8bed\u4e49\u548c\u534f\u540c\u4fe1\u53f7\uff1b\u589e\u5f3a\u7684\u884c\u4e3a\u5efa\u6a21\u4e0e\u53cc\u5411\u8bed\u4e49\u5bf9\u9f50\uff1b\u7ed3\u5408\u81ea\u535a\u5f08\u548c\u771f\u5b9e\u4e16\u754c\u53cd\u9988\u7684\u6e10\u8fdb\u5f0fDPO\u7b56\u7565\u8fdb\u884c\u52a8\u6001\u504f\u597d\u9002\u5e94\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0aRecall@10\u63d0\u534717.8%\uff0cNDCG@10\u63d0\u534720.2%\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u548c\u5de5\u4e1a\u7ea7\u5927\u89c4\u6a21\u63a8\u8350\u5e73\u53f0\u90e8\u7f72\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6536\u76ca\u3002", "conclusion": "Align\u00b3GR\u901a\u8fc7\u591a\u5c42\u6b21\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebf\u90e8\u7f72\u4e2d\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.11305", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11305", "abs": "https://arxiv.org/abs/2511.11305", "authors": ["Chenghan Fu", "Daoze Zhang", "Yukang Lin", "Zhanheng Nie", "Xiang Zhang", "Jianyu Liu", "Yueran Liu", "Wanxian Guan", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising", "comment": "31 pages, 12 figures", "summary": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.", "AI": {"tldr": "MOON\u662f\u4e00\u4e2a\u7528\u4e8e\u7535\u5546\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u53ef\u6301\u7eed\u8fed\u4ee3\u5b9e\u8df5\u6846\u67b6\uff0c\u5df2\u5728\u6dd8\u5b9d\u641c\u7d22\u5e7f\u544a\u7cfb\u7edf\u4e2d\u5168\u9762\u90e8\u7f72\uff0c\u5728CTR\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b020%\u7684\u5728\u7ebf\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e0e\u4e0b\u6e38\u4efb\u52a1\u76ee\u6807\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u4e49\u4ea4\u6362\u7387\u6765\u91cf\u5316\u4e2d\u95f4\u6307\u6807\u6539\u8fdb\u5bf9\u4e0b\u6e38\u6536\u76ca\u7684\u8f6c\u5316\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u9884\u8bad\u7ec3\u3001\u540e\u8bad\u7ec3\u548c\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u5904\u7406\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u6a21\u578b\u67b6\u6784\u548c\u4e0b\u6e38\u5e94\u7528\u56db\u4e2a\u7ef4\u5ea6\u7684\u4f18\u5316\u3002", "result": "\u5728CTR\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b020%\u7684\u5728\u7ebf\u63d0\u5347\uff0c\u8fd9\u662f\u4e09\u5e74\u6765\u8be5\u4efb\u52a1\u4e0a\u6700\u5927\u7684\u6539\u8fdb\uff0c\u5e76\u5b8c\u6210\u4e86\u4e94\u6b21\u5168\u89c4\u6a21\u8fed\u4ee3\u3002", "conclusion": "MOON\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3a\u7535\u5546\u9886\u57df\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u8df5\u7ecf\u9a8c\u548c\u53ef\u6269\u5c55\u6027\u7814\u7a76\u3002"}}
{"id": "2511.11370", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11370", "abs": "https://arxiv.org/abs/2511.11370", "authors": ["Jiahao Wang", "Bokang Fu", "Yu Zhu", "Yuli Liu"], "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation", "comment": null, "summary": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.\n  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u96c6\u5408\u7684\u53cd\u601d\u5b66\u4e60\u6846\u67b6SRLF\uff0c\u901a\u8fc7\"\u8bc4\u4f30-\u9a8c\u8bc1-\u53cd\u601d\"\u95ed\u73af\u673a\u5236\uff0c\u5229\u7528LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5bf9\u6574\u7ec4\u7269\u54c1\u8fdb\u884c\u6574\u4f53\u5224\u65ad\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u70b9\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u7269\u54c1\u8bc4\u5206\u5efa\u6a21\uff0c\u5bfc\u81f4\u7528\u6237\u504f\u597d\u7406\u89e3\u4e0d\u51c6\u786e\u548c\u7269\u54c1\u8bed\u4e49\u8868\u793a\u50f5\u5316\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SRLF\u6846\u67b6\u91c7\u7528\u96c6\u5408\u5c42\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7efc\u5408\u5206\u6790\u7269\u54c1\u95f4\u7684\u590d\u6742\u76f8\u4e92\u5173\u7cfb\u53ca\u5176\u4e0e\u7528\u6237\u504f\u597d\u7684\u6574\u4f53\u5339\u914d\u5ea6\uff0c\u901a\u8fc7\u95ed\u73af\u5b66\u4e60\u673a\u5236\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u5e8f\u5217\u63a8\u8350\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u96c6\u5408\u89c6\u89d2\u5bf9\u4e8e\u5b9e\u73b0\u5e8f\u5217\u63a8\u8350\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cSRLF\u6846\u67b6\u4e3a\u89e3\u51b3\u4f20\u7edf\u70b9\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}

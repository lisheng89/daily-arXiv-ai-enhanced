<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SMES: Towards Scalable Multi-Task Recommendation via Expert Sparsity](https://arxiv.org/abs/2602.09386)
*Yukun Zhang,Si Dong,Xu Wang,Bo Chen,Qinglin Jia,Shengzhe Wang,Jinlong Jiao,Runhan Li,Jiaqing Liu,Chaoyi Ma,Ruiming Tang,Guorui Zhou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: SMES是一个可扩展的稀疏MoE框架，通过渐进式专家路由解决多任务推荐中专家激活爆炸和负载倾斜问题，在快手大规模短视频服务中部署并取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统通常依赖多任务学习来估计多样化的用户反馈信号。然而，简单地增加模型容量会带来过高的在线推理成本，并且对于标签分布稀疏的任务往往收益递减。均匀参数扩展与异构任务容量需求之间的不匹配是多任务推荐可扩展性的根本挑战。

Method: 提出SMES框架，采用渐进式专家路由：将专家激活分解为跨任务联合选择的任务共享专家子集和任务自适应私有专家，明确限制每个实例的专家执行同时保留任务特定容量。此外，引入全局多门负载均衡正则化器，通过调节所有任务的聚合专家利用率来稳定训练。

Result: SMES已在快手大规模短视频服务中部署，支持超过4亿日活用户。在线实验显示稳定改进：GAUC增益0.29%，用户观看时间提升0.31%。

Conclusion: SMES通过参数稀疏化作为原则性扩展范式，解决了多任务推荐中稀疏MoE应用的关键障碍，实现了可扩展的高效推荐系统。

Abstract: Industrial recommender systems typically rely on multi-task learning to estimate diverse user feedback signals and aggregate them for ranking. Recent advances in model scaling have shown promising gains in recommendation. However, naively increasing model capacity imposes prohibitive online inference costs and often yields diminishing returns for sparse tasks with skewed label distributions. This mismatch between uniform parameter scaling and heterogeneous task capacity demands poses a fundamental challenge for scalable multi-task recommendation. In this work, we investigate parameter sparsification as a principled scaling paradigm and identify two critical obstacles when applying sparse Mixture-of-Experts (MoE) to multi-task recommendation: exploded expert activation that undermines instance-level sparsity and expert load skew caused by independent task-wise routing. To address these challenges, we propose SMES, a scalable sparse MoE framework with progressive expert routing. SMES decomposes expert activation into a task-shared expert subset jointly selected across tasks and task-adaptive private experts, explicitly bounding per-instance expert execution while preserving task-specific capacity. In addition, SMES introduces a global multi-gate load-balancing regularizer that stabilizes training by regulating aggregated expert utilization across all tasks. SMES has been deployed in Kuaishou large-scale short-video services, supporting over 400 million daily active users. Extensive online experiments demonstrate stable improvements, with GAUC gain of 0.29% and a 0.31% uplift in user watch time.

</details>


### [2] [Query-Mixed Interest Extraction and Heterogeneous Interaction: A Scalable CTR Model for Industrial Recommender Systems](https://arxiv.org/abs/2602.09387)
*Fangye Wang,Guowei Yang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: HeMix是一个可扩展的推荐排序模型，通过自适应序列标记化和异构交互结构统一建模，在工业场景中显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统中，稀疏多字段输入和超长用户行为序列使得特征交互学习面临挑战。现有方法难以同时建模上下文相关和上下文无关的用户意图，且交互机制效率低下、同质化，导致预测性能不佳。

Method: 提出HeMix模型，包含Query-Mixed兴趣提取模块（通过动态和固定查询联合建模上下文相关和上下文无关的用户兴趣）和HeteroMixer块（替代自注意力，实现高效多粒度跨特征交互，采用多头令牌融合、异构交互和组对齐重建流程）。

Result: 在工业规模数据集上，HeMix展现出良好的扩展性，模型规模扩大能稳定提升推荐准确性。部署在AMAP平台后获得显著在线收益：GMV提升0.61%，PV_CTR提升2.32%，UV_CVR提升0.81%。

Conclusion: HeMix通过统一的自适应序列标记化和异构交互结构，有效解决了工业推荐系统中的特征交互学习挑战，实现了可扩展的高性能推荐。

Abstract: Learning effective feature interactions is central to modern recommender systems, yet remains challenging in industrial settings due to sparse multi-field inputs and ultra-long user behavior sequences. While recent scaling efforts have improved model capacity, they often fail to construct both context-aware and context-independent user intent from the long-term and real-time behavior sequence. Meanwhile, recent work also suffers from inefficient and homogeneous interaction mechanisms, leading to suboptimal prediction performance. To address these limitations, we propose HeMix, a scalable ranking model that unifies adaptive sequence tokenization and heterogeneous interaction structure. Specifically, HeMix introduces a Query-Mixed Interest Extraction module that jointly models context-aware and context-independent user interests via dynamic and fixed queries over global and real-time behavior sequences. For interaction, we replace self-attention with the HeteroMixer block, enabling efficient, multi-granularity cross-feature interactions that adopt the multi-head token fusion, heterogeneous interaction and group-aligned reconstruction pipelines. HeMix demonstrates favorable scaling behavior, driven by the HeteroMixer block, where increasing model scale via parameter expansion leads to steady improvements in recommendation accuracy. Experiments on industrial-scale datasets show that HeMix scales effectively and consistently outperforms strong baselines. Most importantly, HeMix has been deployed on the AMAP platform, delivering significant online gains: +0.61% GMV, +2.32% PV_CTR, and +0.81% UV_CVR.

</details>


### [3] [SARM: LLM-Augmented Semantic Anchor for End-to-End Live-Streaming Ranking](https://arxiv.org/abs/2602.09401)
*Ruochen Yang,Yueyang Liu,Zijie Zhuang,Changxin Lao,Yuhui Zhang,Jiangxia Cao,Jia Xu,Xiang Chen,Haoke Xiao,Xiangyu Wu,Xiaoyou Zhou,Xiao Lv,Shuang Yang,Tingwen Liu,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: SARM是一个端到端的直播推荐排序架构，通过将自然语言语义锚点直接集成到排序优化中，解决了大规模直播推荐中非平稳内容语义建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 工业部署中两种常见方法存在根本限制：离散语义抽象通过聚类牺牲了描述精度，而密集多模态嵌入独立提取且与排序优化弱对齐，限制了细粒度内容感知排序。

Method: 提出SARM架构，将可学习的文本token作为语义锚点与排序特征联合优化；采用轻量级双token门控设计捕获特定领域直播语义；使用非对称部署策略保持低延迟在线训练和服务。

Result: 离线评估和大规模A/B测试显示相对于生产基线有持续改进；SARM已完全部署，每日服务超过4亿用户。

Conclusion: SARM通过将自然语言语义锚点直接集成到排序优化中，实现了细粒度的作者表示，解决了大规模直播推荐中内容语义建模的挑战，并在工业部署中取得了显著效果。

Abstract: Large-scale live-streaming recommendation requires precise modeling of non-stationary content semantics under strict real-time serving constraints. In industrial deployment, two common approaches exhibit fundamental limitations: discrete semantic abstractions sacrifice descriptive precision through clustering, while dense multimodal embeddings are extracted independently and remain weakly aligned with ranking optimization, limiting fine-grained content-aware ranking. To address these limitations, we propose \textbf{SARM}, an end-to-end ranking architecture that integrates natural-language semantic anchors directly into ranking optimization, enabling fine-grained author representations conditioned on multimodal content. Each semantic anchor is represented as learnable text tokens jointly optimized with ranking features, allowing the model to adapt content descriptions to ranking objectives. A lightweight dual-token gated design captures domain-specific live-streaming semantics, while an asymmetric deployment strategy preserves low-latency online training and serving. Extensive offline evaluation and large-scale A/B tests show consistent improvements over production baselines. SARM is fully deployed and serves over 400 million users daily.

</details>


### [4] [Personalized Parameter-Efficient Fine-Tuning of Foundation Models for Multimodal Recommendation](https://arxiv.org/abs/2602.09445)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.IR

TL;DR: PerPEFT：一种个性化参数高效微调策略，通过用户兴趣分组为不同用户群体分配独立的PEFT模块，使多模态基础模型能根据用户兴趣生成个性化物品嵌入，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态基础模型的推荐系统虽然使用参数高效微调（PEFT）来适应推荐任务，但物品嵌入仍然是用户无关的，没有考虑不同用户兴趣关注不同物品方面的问题。

Method: 提出PerPEFT个性化PEFT策略：1）按用户兴趣对用户进行分组；2）为每个用户组分配独立的PEFT模块，使每个模块能捕捉对该组购买决策最相关的细粒度物品方面；3）引入专门的训练技术来增强这种用户组条件化。

Result: PerPEFT在实验中表现优异：1）比最强基线提升高达15.3%（NDCG@20）；2）在不同PEFT变体上都能带来一致的性能增益；3）即使实现个性化，PEFT仍然轻量，仅增加基础模型参数量的1.3%。

Conclusion: PerPEFT通过用户兴趣分组的个性化PEFT策略，有效解决了多模态基础模型中物品嵌入用户无关的问题，显著提升了推荐性能，同时保持了参数高效性，是一种通用且有效的多模态推荐方法。

Abstract: In recent years, substantial research has integrated multimodal item metadata into recommender systems, often by using pre-trained multimodal foundation models to encode such data. Since these models are not originally trained for recommendation tasks, recent works efficiently adapt them via parameter-efficient fine-tuning (PEFT). However, even with PEFT, item embeddings from multimodal foundation models remain user-blind: item embeddings are not conditioned on user interests, despite the fact that users with diverse interests attend to different item aspects. To address this limitation, we propose PerPEFT, a personalized PEFT strategy for multimodal recommendation. Specifically, PerPEFT groups users by interest and assigns a distinct PEFT module to each group, enabling each module to capture the fine-grained item aspects most predictive of that group`s purchase decisions. We further introduce a specialized training technique that strengthens this user-group conditioning. Notably, PerPEFT is PEFT-agnostic and can be paired with any PEFT method applicable to multimodal foundation models. Through extensive experiments, we show that (1) PerPEFT outperforms the strongest baseline by up to 15.3% (NDCG@20) and (2) delivers consistent gains across diverse PEFT variants. It is noteworthy that, even with personalization, PEFT remains lightweight, adding only 1.3% of the parameter count of the foundation model. We provide our code and datasets at https://github.com/kswoo97/PerPEFT.

</details>


### [5] [The Wisdom of Many Queries: Complexity-Diversity Principle for Dense Retriever Training](https://arxiv.org/abs/2602.09448)
*Xincan Feng,Noriki Nishida,Yusuke Sakai,Yuji Matsumoto*

Main category: cs.IR

TL;DR: 研究发现查询多样性对密集检索的影响取决于查询复杂度，提出复杂度-多样性原则（CDP），为多跳检索任务提供零样本多查询合成方法，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 先前研究在合成数据生成中查询多样性对密集检索的影响存在矛盾结果，需要量化多样性影响并使其可测量。

Method: 设计Q-D指标量化多样性影响，在4种基准类型（31个数据集）上进行实验，深入分析多跳数据，发现多样性收益与查询复杂度强相关，提出复杂度-多样性原则（CDP）。

Result: 查询多样性特别有利于多跳检索，多样性收益与查询复杂度强相关（r≥0.95，p<0.05，12/14条件下），CDP提供可操作阈值（CW>10：使用多样性；CW<7：避免）。

Conclusion: 查询复杂度决定最佳多样性水平，基于CDP提出的零样本多查询合成方法在多跳任务中实现最先进性能。

Abstract: Prior work reports conflicting results on query diversity in synthetic data generation for dense retrieval. We identify this conflict and design Q-D metrics to quantify diversity's impact, making the problem measurable. Through experiments on 4 benchmark types (31 datasets), we find query diversity especially benefits multi-hop retrieval. Deep analysis on multi-hop data reveals that diversity benefit correlates strongly with query complexity ($r$$\geq$0.95, $p$$<$0.05 in 12/14 conditions), measured by content words (CW). We formalize this as the Complexity-Diversity Principle (CDP): query complexity determines optimal diversity. CDP provides actionable thresholds (CW$>$10: use diversity; CW$<$7: avoid it). Guided by CDP, we propose zero-shot multi-query synthesis for multi-hop tasks, achieving state-of-the-art performance.

</details>


### [6] [With Argus Eyes: Assessing Retrieval Gaps via Uncertainty Scoring to Detect and Remedy Retrieval Blind Spots](https://arxiv.org/abs/2602.09616)
*Zeinab Sadat Taghavi,Ali Modarressi,Hinrich Schutze,Andreas Marfurt*

Main category: cs.IR

TL;DR: 论文发现神经检索器存在"盲点"问题，即无法检索到与查询相关但嵌入相似度低的实体，并提出ARGUS方法通过针对性文档增强来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 可靠的检索增强生成(RAG)系统依赖于检索器查找相关信息的能力。研究发现神经检索器存在"盲点"问题，即无法检索到与查询相关但嵌入相似度低的实体，这会影响RAG系统的鲁棒性和可信度。

Method: 1) 提出检索概率分数(RPS)来预测盲点风险；2) 引入ARGUS管道，通过从知识库(如维基百科首段)进行针对性文档增强，提高高风险(低RPS)实体的可检索性。

Result: 在BRIGHT、IMPLIRET和RAR-B数据集上的实验表明，ARGUS在所有评估的检索器上都取得了显著改进(平均+3.4 nDCG@5和+4.5 nDCG@10)，在具有挑战性的子集上提升更大。

Conclusion: 预先修复检索盲点对于构建鲁棒可信的RAG系统至关重要。ARGUS方法通过针对性文档增强有效提高了高风险实体的可检索性。

Abstract: Reliable retrieval-augmented generation (RAG) systems depend fundamentally on the retriever's ability to find relevant information. We show that neural retrievers used in RAG systems have blind spots, which we define as the failure to retrieve entities that are relevant to the query, but have low similarity to the query embedding. We investigate the training-induced biases that cause such blind spot entities to be mapped to inaccessible parts of the embedding space, resulting in low retrievability. Using a large-scale dataset constructed from Wikidata relations and first paragraphs of Wikipedia, and our proposed Retrieval Probability Score (RPS), we show that blind spot risk in standard retrievers (e.g., CONTRIEVER, REASONIR) can be predicted pre-index from entity embedding geometry, avoiding expensive retrieval evaluations. To address these blind spots, we introduce ARGUS, a pipeline that enables the retrievability of high-risk (low-RPS) entities through targeted document augmentation from a knowledge base (KB), first paragraphs of Wikipedia, in our case. Extensive experiments on BRIGHT, IMPLIRET, and RAR-B show that ARGUS achieves consistent improvements across all evaluated retrievers (averaging +3.4 nDCG@5 and +4.5 nDCG@10 absolute points), with substantially larger gains in challenging subsets. These results establish that preemptively remedying blind spots is critical for building robust and trustworthy RAG systems (Code and Data).

</details>


### [7] [DiffuReason: Bridging Latent Reasoning and Generative Refinement for Sequential Recommendation](https://arxiv.org/abs/2602.09744)
*Jie Jiang,Yang Wu,Qian Li,Yuling Xiong,Yihang Su,Junbang Huo,Longfei Lu,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: DiffuReason是一个用于序列推荐的"Think-then-Diffuse"框架，通过多步推理令牌进行潜在推理，扩散过程去噪，以及端到端强化学习优化，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法存在两个主要问题：1) 使用确定性潜在链会累积噪声且忽略了用户意图的不确定性；2) 采用分阶段训练流程阻碍了联合优化和探索。需要一种能处理推理噪声并支持端到端优化的统一框架。

Method: 提出DiffuReason框架，包含三个阶段：1) Think阶段：生成Thinking Tokens对用户历史进行多步推理，形成初始意图假设；2) Diffuse阶段：通过扩散过程将意图建模为概率分布，对推理结果进行迭代去噪；3) GRPO强化学习：使用Group Relative Policy Optimization实现端到端对齐，优化排序性能。

Result: 在四个基准数据集上的实验表明，DiffuReason能持续提升多种骨干架构的性能。在大型工业平台上的在线A/B测试进一步验证了其实际有效性。

Conclusion: DiffuReason通过整合多步推理、扩散去噪和端到端强化学习，解决了现有序列推荐方法中推理噪声累积和分阶段优化的局限性，为处理用户意图不确定性提供了有效的统一框架。

Abstract: Latent reasoning has emerged as a promising paradigm for sequential recommendation, enabling models to capture complex user intent through multi-step deliberation. Yet existing approaches often rely on deterministic latent chains that accumulate noise and overlook the uncertainty inherent in user intent, and they are typically trained in staged pipelines that hinder joint optimization and exploration. To address these challenges, we propose DiffuReason, a unified "Think-then-Diffuse" framework for sequential recommendation. It integrates multi-step Thinking Tokens for latent reasoning, diffusion-based refinement for denoising intermediate representations, and end-to-end Group Relative Policy Optimization (GRPO) alignment to optimize for ranking performance. In the Think stage, the model generates Thinking Tokens that reason over user history to form an initial intent hypothesis. In the Diffuse stage, rather than treating this hypothesis as the final output, we refine it through a diffusion process that models user intent as a probabilistic distribution, providing iterative denoising against reasoning noise. Finally, GRPO-based reinforcement learning enables the reasoning and refinement modules to co-evolve throughout training, without the constraints of staged optimization. Extensive experiments on four benchmarks demonstrate that DiffuReason consistently improves diverse backbone architectures. Online A/B tests on a large-scale industrial platform further validate its practical effectiveness.

</details>


### [8] [Internalizing Multi-Agent Reasoning for Accurate and Efficient LLM-based Recommendation](https://arxiv.org/abs/2602.09829)
*Yang Wu,Haoze Wang,Qian Li,Jun Zhang,Huan Yu,Jie Jiang*

Main category: cs.IR

TL;DR: STAR框架通过轨迹驱动内化方法，将多智能体教师系统的复杂推理能力蒸馏到单一高效推荐模型中，显著提升性能同时消除迭代延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能为推荐系统提供世界知识和语义推理能力，但如何有效整合协同信号并避免过高推理延迟成为关键瓶颈。

Method: 提出轨迹驱动内化框架：1) 设计多智能体教师系统，使用协同信号翻译机制将行为模式转换为自然语言证据；2) 通过轨迹驱动蒸馏管道将智能体逻辑（规划、工具使用、自我反思）转移到紧凑的STAR模型中。

Result: STAR模型性能超越其教师系统8.7%至39.5%，同时消除了迭代延迟，为实时推理增强推荐铺平道路。

Conclusion: 该研究证明了通过轨迹驱动内化方法，可以将复杂的多智能体推理能力有效蒸馏到单一高效模型中，实现高性能的实时推理增强推荐。

Abstract: Large Language Models (LLMs) are reshaping recommender systems by leveraging extensive world knowledge and semantic reasoning to interpret user intent. However, effectively integrating these capabilities with collaborative signals while avoiding prohibitive inference latency remains a critical bottleneck. To address this, we propose a trajectory-driven internalization framework to develop a Single-agent Trajectory-Aligned Recommender (STAR). Specifically, to internalize complex reasoning capabilities into a single efficient model, we first design a multi-agent teacher system capable of multi-turn tool usage and reflection. This teacher utilizes a Collaborative Signal Translation mechanism to explicitly convert latent behavioral patterns into descriptive natural language evidence to enhance reasoning accuracy. Subsequently, a trajectory-driven distillation pipeline transfers this agentic logic, including planning, tool usage, and self-reflection, into the compact STAR model. Extensive experiments demonstrate that STAR surpasses its teacher by 8.7% to 39.5% while eliminating iterative latency, paving the way for real-time, reasoning-enhanced recommendation.

</details>


### [9] [QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search](https://arxiv.org/abs/2602.09901)
*Jianzhao Huang,Xiaorui Huang,Fei Zhao,Yunpeng Liu,Hui Zhang,Fangcheng Shi,Congfeng Li,Zechen Sun,Yi Wu,Yao Hu,Yunhan Bai,Shaosheng Cao*

Main category: cs.IR

TL;DR: QP-OneModel是一个统一的生成式LLM，用于社交网络搜索中的多任务查询理解，通过三阶段对齐策略和强化学习，显著提升查询处理性能。


<details>
  <summary>Details</summary>
Motivation: 传统查询处理系统使用孤立的判别模型，存在语义理解有限、维护成本高的问题。现有LLM方法往往孤立优化子任务，缺乏语义协同，且难以适应社交网络场景的非正式语言模式和业务定义。

Method: 将异构子任务重新表述为统一的序列生成范式，采用渐进式三阶段对齐策略，最终通过多奖励强化学习进行优化。生成意图描述作为新的高保真语义信号，增强下游任务。

Result: 离线评估显示整体性能提升7.35%，NER任务F1提升9.01%，Term Weighting提升9.31%。在未见任务上准确率比32B模型高7.60%。在线A/B测试显示检索相关性提升0.21%，用户留存提升0.044%。

Conclusion: QP-OneModel通过统一的生成式LLM框架有效解决了社交网络查询处理中的语义理解问题，展示了在实际工业部署中的价值，显著提升了查询理解性能。

Abstract: Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%.

</details>


### [10] [Efficient Learning of Sparse Representations from Interactions](https://arxiv.org/abs/2602.09935)
*Vojtěch Vančura,Martin Spišák,Rodrigo Alves,Ladislav Peška*

Main category: cs.IR

TL;DR: 提出一种训练高维稀疏嵌入层的方法，替代传统密集嵌入，在检索阶段实现嵌入压缩与表达能力的平衡，在推荐准确度几乎不变的情况下实现10-100倍的嵌入尺寸缩减。


<details>
  <summary>Details</summary>
Motivation: 在生产推荐系统的初始检索阶段，存在嵌入表达能力与系统可扩展性、延迟之间的固有权衡，需要既紧凑又具有表达能力的表示。

Method: 提出一种训练策略，学习高维稀疏嵌入层替代传统密集嵌入，修改生产级协同过滤自编码器ELSA，实现嵌入压缩。

Result: 实现嵌入尺寸10倍缩减时推荐准确度无损失，100倍缩减时仅损失2.5%；活跃嵌入维度揭示了可解释的倒排索引结构，可直接与模型潜在空间对齐。

Conclusion: 该方法在保持推荐准确度的同时显著压缩嵌入尺寸，稀疏嵌入结构具有可解释性，可直接集成分段级推荐功能到候选检索模型中。

Abstract: Behavioral patterns captured in embeddings learned from interaction data are pivotal across various stages of production recommender systems. However, in the initial retrieval stage, practitioners face an inherent tradeoff between embedding expressiveness and the scalability and latency of serving components, resulting in the need for representations that are both compact and expressive. To address this challenge, we propose a training strategy for learning high-dimensional sparse embedding layers in place of conventional dense ones, balancing efficiency, representational expressiveness, and interpretability. To demonstrate our approach, we modified the production-grade collaborative filtering autoencoder ELSA, achieving up to 10x reduction in embedding size with no loss of recommendation accuracy, and up to 100x reduction with only a 2.5% loss. Moreover, the active embedding dimensions reveal an interpretable inverted-index structure that segments items in a way directly aligned with the model's latent space, thereby enabling integration of segment-level recommendation functionality (e.g., 2D homepage layouts) within the candidate retrieval model itself. Source codes, additional results, as well as a live demo are available at https://github.com/zombak79/compressed_elsa

</details>


### [11] [Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design](https://arxiv.org/abs/2602.10016)
*Bojian Hou,Xiaolong Liu,Xiaoyi Liu,Jiaqi Xu,Yasmine Badr,Mengyue Hang,Sudhanshu Chanpuriya,Junqing Zhou,Yuhang Yang,Han Xu,Qiuling Suo,Laming Chen,Yuxi Hu,Jiasheng Zhang,Huaqing Xiong,Yuzhen Huang,Chao Chen,Yue Dong,Yi Yang,Shuo Chang,Xiaorui Gan,Wenlin Chen,Santanu Kolay,Darren Liu,Jade Nie,Chunzhi Yang,Jiyan Yang,Huayu Li*

Main category: cs.IR

TL;DR: Kunlun是一个可扩展的推荐系统架构，通过提高模型效率和资源分配来改善缩放效率，实现了从17%到37%的MFU提升，并在Meta Ads中部署应用。


<details>
  <summary>Details</summary>
Motivation: 推荐系统缺乏可预测的缩放定律，尤其是在处理用户历史和上下文特征时。缩放效率低下是主要障碍，源于低效模块和次优资源分配。

Method: 提出Kunlun架构，包含低层优化（广义点积注意力、分层种子池化、滑动窗口注意力）和高层创新（计算跳过、事件级个性化）。

Result: 在NVIDIA B200 GPU上将MFU从17%提升到37%，缩放效率比现有方法提高一倍，已在Meta Ads主要模型中部署并产生显著生产影响。

Conclusion: Kunlun通过系统性的效率和资源分配改进，解决了推荐系统缩放效率低下的问题，实现了可预测的缩放定律，并在实际生产中验证了其有效性。

Abstract: Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.

</details>


### [12] [Overview of the TREC 2025 RAGTIME Track](https://arxiv.org/abs/2602.10024)
*Dawn Lawrie,Sean MacAvaney,James Mayfield,Luca Soldaini,Eugene Yang,Andrew Yates*

Main category: cs.IR

TL;DR: RAGTIME是TREC的多语言评估赛道，专注于从多语言新闻文档生成报告，包含三个任务类型，共有13个团队提交了125个运行结果。


<details>
  <summary>Details</summary>
Motivation: 研究从多语言源文档生成报告的能力，特别是针对阿拉伯语、中文、英语和俄语新闻文档的报告生成。

Method: 创建包含四种语言新闻文档的文档集合，设立三个任务类型：多语言报告生成、英语报告生成和多语言信息检索，邀请研究团队参与评估。

Result: 共有13个参与团队（加上赛道协调员提供的基线）为三个任务提交了125个运行结果。

Conclusion: 该概述描述了RAGTIME赛道的三个任务并呈现了可用结果，为多语言报告生成研究提供了评估框架和数据。

Abstract: The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrieval (MLIR). A total of 125 runs were submitted by 13 participating teams (and as baselines by the track coordinators) for three tasks. This overview describes these three tasks and presents the available results.

</details>

{"id": "2511.15996", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15996", "abs": "https://arxiv.org/abs/2511.15996", "authors": ["Amin Bigdeli", "Radin Hamidi Rad", "Mert Incesu", "Negar Arabzadeh", "Charles L. A. Clarke", "Ebrahim Bagheri"], "title": "QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation", "comment": "4 pages", "summary": "We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.", "AI": {"tldr": "QueryGym\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684Python\u5de5\u5177\u5305\uff0c\u4e3a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u67e5\u8be2\u91cd\u6784\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u5206\u6563\u3001\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u67e5\u8be2\u91cd\u6784\u65b9\u6cd5\u867d\u7136\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u679c\uff0c\u4f46\u5404\u65b9\u6cd5\u7684\u5b9e\u73b0\u5206\u6563\u5728\u4e0d\u540c\u4f5c\u8005\u624b\u4e2d\uff0c\u7f3a\u4e4f\u7edf\u4e00\u5de5\u5177\u5305\uff0c\u963b\u788d\u4e86\u516c\u5e73\u6bd4\u8f83\u3001\u5feb\u901f\u5b9e\u9a8c\u3001\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\u548c\u53ef\u9760\u90e8\u7f72\u3002", "method": "\u63d0\u4f9bPython API\u652f\u6301\u591a\u79cdLLM\u65b9\u6cd5\u5e94\u7528\u3001\u68c0\u7d22\u65e0\u5173\u7684\u63a5\u53e3\u652f\u6301Pyserini\u548cPyTerrier\u7b49\u540e\u7aef\u96c6\u6210\u3001\u96c6\u4e2d\u5f0f\u63d0\u793a\u7ba1\u7406\u7cfb\u7edf\u3001\u5185\u7f6eBEIR\u548cMS MARCO\u57fa\u51c6\u652f\u6301\uff0c\u4ee5\u53ca\u5b8c\u5168\u5f00\u6e90\u7684\u6269\u5c55\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u4e86QueryGym\u5de5\u5177\u5305\uff0c\u4e3aLLM\u67e5\u8be2\u91cd\u6784\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5b9e\u73b0\u3001\u6267\u884c\u548c\u6bd4\u8f83\u6846\u67b6\u3002", "conclusion": "QueryGym\u586b\u8865\u4e86LLM\u67e5\u8be2\u91cd\u6784\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u5de5\u5177\u5305\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7814\u7a76\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u5feb\u901f\u53d1\u5c55\u3002"}}
{"id": "2511.16106", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16106", "abs": "https://arxiv.org/abs/2511.16106", "authors": ["Archish S", "Ankit Garg", "Kirankumar Shiragur", "Neeraj Kayal"], "title": "Incorporating Token Importance in Multi-Vector Retrieval", "comment": null, "summary": "ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.\n  In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\\% through few-shot fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5bf9ColBERT\u4e2dChamfer\u8ddd\u79bb\u51fd\u6570\u7684\u6539\u8fdb\uff0c\u901a\u8fc7\u8ba1\u7b97\u67e5\u8be2\u4ee4\u724c\u8d21\u732e\u7684\u52a0\u6743\u548c\u6765\u589e\u5f3a\u8868\u8fbe\u6027\uff0c\u5176\u4e2d\u6743\u91cd\u53cd\u6620\u4ee4\u724c\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u589e\u5f3aChamfer\u8ddd\u79bb\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u4ee4\u724c\u91cd\u8981\u6027\u6765\u63d0\u5347\u591a\u5411\u91cf\u8868\u793a\u673a\u5236\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5728\u4fdd\u6301\u591a\u5411\u91cf\u8868\u793a\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u8bad\u7ec3\u4ee4\u724c\u6743\u91cd\uff0c\u8ba1\u7b97\u67e5\u8be2\u4ee4\u724c\u8d21\u732e\u7684\u52a0\u6743\u548c\u3002", "result": "\u5728BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f7f\u7528\u57fa\u4e8eIDF\u7684\u6743\u91cd\u5b9e\u73b0\u4e86Recall@10\u5e73\u5747\u63d0\u53471.28%\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u5b9e\u73b0\u4e863.66%\u7684\u63d0\u5347\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5355\u7684\u6269\u5c55\u65b9\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u540e\u671f\u4ea4\u4e92\u591a\u5411\u91cf\u673a\u5236\u7684\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2511.16326", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16326", "abs": "https://arxiv.org/abs/2511.16326", "authors": ["Jiawei Zhou", "Hang Ding", "Haiyun Jiang"], "title": "ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning", "comment": "Under Review in ARR", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u7b54\u6848\u5bf9\u9f50\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u9ad8\u8d28\u91cf\u6b63\u6837\u672c\u5757\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f18\u5316\u68c0\u7d22\u5668\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6807\u51c6\u68c0\u7d22\u5668\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u96be\u4ee5\u533a\u5206\u7a00\u758f\u4f46\u5173\u952e\u7684\u8bc1\u636e\uff0c\u4e14\u5176\u67e5\u8be2-\u6587\u6863\u76f8\u4f3c\u6027\u4f18\u5316\u4e0e\u751f\u6210\u7cbe\u786e\u7b54\u6848\u7684\u4e0b\u6e38\u76ee\u6807\u4e0d\u4e00\u81f4\u3002", "method": "\u9996\u5148\u8bc4\u4f30\u6587\u672c\u5757\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u5145\u5206\u6027\u6765\u8bc6\u522b\u9ad8\u8d28\u91cf\u6b63\u6837\u672c\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8eLLM\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\uff0c\u751f\u6210\u589e\u5f3a\u67e5\u8be2\u5e76\u6316\u6398\u6e10\u8fdb\u5f0f\u56f0\u96be\u8d1f\u6837\u672c\u3002", "result": "\u5728Ultradomain\u548cLongBench\u57fa\u51c6\u768410\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534714.5%\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u4e0a\u4e0b\u6587RAG\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u771f\u6b63\u4ee5\u7b54\u6848\u4e3a\u4e2d\u5fc3\u7684\u68c0\u7d22\u5668\u63d0\u4f9b\u4e86\u7a33\u5065\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.16414", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16414", "abs": "https://arxiv.org/abs/2511.16414", "authors": ["Hao Liu", "Le Wu", "Min Hou", "Han Wu", "Kun Zhang", "Xin Li", "Si Wei"], "title": "An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm", "comment": null, "summary": "Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution.", "AI": {"tldr": "EvoRec\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5b9a\u4f4d-\u9057\u5fd8-\u66f4\u65b0\u6846\u67b6\uff0c\u4e13\u95e8\u4e3a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u7528\u4e8e\u5efa\u6a21\u7528\u6237\u504f\u597d\u7684\u6f14\u5316\uff0c\u901a\u8fc7\u7cbe\u786e\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u6765\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u5e76\u4fdd\u6301\u63a8\u8350\u6027\u80fd\u3002", "motivation": "LLM-based\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u9002\u5e94\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7528\u6237\u504f\u597d\uff0c\u56e0\u4e3aLLM\u53c2\u6570\u91cf\u5927\uff0c\u4f20\u7edf\u91cd\u8bad\u7ec3\u6216\u5fae\u8c03\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u8981\u4e48\u4f1a\u5bfc\u81f4\u975e\u6d3b\u8dc3\u7528\u6237\u504f\u597d\u9057\u5fd8\u3002", "method": "\u63d0\u51faEvoRec\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u4e0e\u504f\u597d\u53d8\u5316\u76f8\u5173\u7684\u5c0f\u53c2\u6570\u96c6\u5e76\u8fdb\u884c\u7cbe\u786e\u66f4\u65b0\uff0c\u4ec5\u4fee\u653930%\u7684LoRA\u9002\u914d\u5668\u53c2\u6570\uff0c\u4e0d\u5f15\u5165\u989d\u5916\u53c2\u6570\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvoRec\u80fd\u6709\u6548\u6f14\u5316LLMRec\u4ee5\u9002\u5e94\u6d3b\u8dc3\u7528\u6237\u504f\u597d\uff0c\u540c\u65f6\u4fdd\u62a4\u975e\u6d3b\u8dc3\u7528\u6237\u7684\u5174\u8da3\u5728\u6f14\u5316\u8fc7\u7a0b\u4e2d\u4e0d\u53d7\u5e72\u6270\u3002", "conclusion": "EvoRec\u4e3aLLM-based\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6f14\u5316\u65b9\u6cd5\uff0c\u5728\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2511.16478", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.16478", "abs": "https://arxiv.org/abs/2511.16478", "authors": ["Elena V. Epure", "Yashar Deldjoo", "Bruno Sguerra", "Markus Schedl", "Manuel Moussallam"], "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation", "comment": "Under review with the ACM Transactions on Recommender Systems (TORS)", "summary": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.\n  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20LLM\u9a71\u52a8\u7684\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u4e86LLM\u5982\u4f55\u91cd\u5851\u7528\u6237\u5efa\u6a21\u3001\u7269\u54c1\u5efa\u6a21\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u8350\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u7684\u6210\u529f\u548c\u98ce\u9669\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\u57fa\u4e8e\u4fe1\u606f\u68c0\u7d22\u6846\u67b6\uff0c\u4e3b\u8981\u5173\u6ce8\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u4f46\u96be\u4ee5\u56de\u7b54\"\u4ec0\u4e48\u662f\u597d\u7684\u63a8\u8350\"\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002LLM\u7684\u51fa\u73b0\u6253\u7834\u4e86\u8fd9\u4e00\u6846\u67b6\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u56de\u987eLLM\u5982\u4f55\u91cd\u5851\u97f3\u4e50\u63a8\u8350\u7684\u7528\u6237\u5efa\u6a21\u3001\u7269\u54c1\u5efa\u6a21\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u8350\uff1b\u7136\u540e\u501f\u9274NLP\u9886\u57df\u7684\u8bc4\u4f30\u5b9e\u8df5\uff1b\u6700\u540e\u901a\u8fc7LLM\u63d0\u793a\u6280\u672f\u63d0\u51fa\u7ed3\u6784\u5316\u7684\u6210\u529f\u548c\u98ce\u9669\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u65b0\u7684\u3001\u6559\u5b66\u6027\u7684\u3001\u8de8\u5b66\u79d1\u7684\u8bc4\u4f30\u89c6\u89d2\uff0c\u4e3aMRS\u793e\u533a\u63d0\u4f9b\u4e86\u5e94\u5bf9LLM\u65f6\u4ee3\u6311\u6218\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u97f3\u4e50\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u6839\u672c\u6027\u5730\u91cd\u65b0\u601d\u8003\u8bc4\u4f30\u8303\u5f0f\uff0c\u4f20\u7edf\u57fa\u4e8e\u51c6\u786e\u6027\u7684\u6307\u6807\u4e0d\u518d\u9002\u7528\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5e94\u5bf9LLM\u5e26\u6765\u7684\u673a\u9047\u548c\u6311\u6218\u3002"}}
{"id": "2511.16543", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16543", "abs": "https://arxiv.org/abs/2511.16543", "authors": ["Jiaheng Zhang", "Daqiang Zhang"], "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation", "comment": "11 pages,3 figures", "summary": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.\n  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.\n  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.", "AI": {"tldr": "Prism\u662f\u4e00\u4e2a\u89e3\u8026\u7684\u6846\u67b6\uff0c\u5c06\u63a8\u8350\u7cfb\u7edf\u5206\u4e3a\u4e13\u95e8\u7684\u6392\u5e8f\u9636\u6bb5\u548c\u89e3\u91ca\u751f\u6210\u9636\u6bb5\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u4f7f\u7528\u6559\u5e08LLM\u751f\u6210\u9ad8\u8d28\u91cf\u89e3\u91ca\u77e5\u8bc6\uff0c\u5b66\u751f\u6a21\u578b\u4e13\u95e8\u5408\u6210\u4e2a\u6027\u5316\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u53ef\u89e3\u91ca\u63a8\u8350\u7cfb\u7edf\u4e2d\u7aef\u5230\u7aef\u67b6\u6784\u5b58\u5728\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u907f\u514d\u6392\u5e8f\u548c\u89e3\u91ca\u8054\u5408\u4f18\u5316\u5bfc\u81f4\u7684\u6b21\u4f18\u59a5\u534f\u3002", "method": "\u91c7\u7528\u89e3\u8026\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5927\u7684\u6559\u5e08LLM\uff08\u5982FLAN-T5-XXL\uff09\u4f5c\u4e3aOracle\u751f\u6210\u9ad8\u4fdd\u771f\u89e3\u91ca\u77e5\u8bc6\uff0c\u7136\u540e\u7531\u7cbe\u8c03\u7684\u7d27\u51d1\u5b66\u751f\u6a21\u578b\uff08\u5982BART-Base\uff09\u4e13\u95e8\u5408\u6210\u4e2a\u6027\u5316\u89e3\u91ca\u3002", "result": "140M\u53c2\u6570\u7684Prism\u6a21\u578b\u5728\u5fe0\u5b9e\u5ea6\u548c\u4e2a\u6027\u5316\u7684\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e11B\u53c2\u6570\u7684\u6559\u5e08\u6a21\u578b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534724\u500d\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1110\u500d\u3002", "conclusion": "\u89e3\u8026\u7ed3\u5408\u6709\u9488\u5bf9\u6027\u7684\u84b8\u998f\u4e3a\u9ad8\u8d28\u91cf\u53ef\u89e3\u91ca\u63a8\u8350\u63d0\u4f9b\u4e86\u9ad8\u6548\u6709\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2511.16576", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16576", "abs": "https://arxiv.org/abs/2511.16576", "authors": ["Alima Subedi", "Sankalpa Pokharel", "Satish Puri"], "title": "PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search", "comment": null, "summary": "Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm.", "AI": {"tldr": "PolyMinHash\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u8fb9\u5f62\u8fd1\u4f3c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06MinHashing\u9002\u914d\u4e3a2D\u591a\u8fb9\u5f62\u54c8\u5e0c\u65b9\u6848\uff0c\u751f\u6210\u4fdd\u6301\u76f8\u4f3c\u6027\u7684\u77ed\u7b7e\u540d\uff0c\u663e\u8457\u51cf\u5c11\u5019\u9009\u5904\u7406\u6570\u91cf\u3002", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u589e\u5927\uff0c\u7cbe\u786e\u6700\u8fd1\u90bb\u641c\u7d22\u53d8\u5f97\u4e0d\u53ef\u884c\uff0c\u800c\u73b0\u6709ANN\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u3001\u56fe\u50cf\u548c\u8f68\u8ff9\u6570\u636e\uff0c\u7f3a\u4e4f\u9488\u5bf9\u591a\u8fb9\u5f62\u6570\u636e\u7684ANN\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u968f\u673a\u91c7\u6837\u70b9\u843d\u5165\u591a\u8fb9\u5f62\u5185\u90e8\u533a\u57df\u6240\u9700\u7684\u70b9\u6570\u6765\u751f\u6210MinHash\u503c\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u57fa\u4e8e\u9762\u79ef\u7684Jaccard\u76f8\u4f3c\u6027\u3002", "result": "\u4e0e\u66b4\u529b\u7b97\u6cd5\u76f8\u6bd4\uff0c\u54c8\u5e0c\u673a\u5236\u5728\u67e5\u8be2\u7ec6\u5316\u9636\u6bb5\u5904\u7406\u7684\u5019\u9009\u6570\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe98%\u3002", "conclusion": "PolyMinHash\u7cfb\u7edf\u5728\u591a\u8fb9\u5f62\u76f8\u4f3c\u6027\u641c\u7d22\u4e2d\u5b9e\u73b0\u4e86\u641c\u7d22\u7cbe\u5ea6\u4e0e\u8fd0\u884c\u65f6\u95f4\u4e4b\u95f4\u7684\u826f\u597d\u6743\u8861\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u3002"}}

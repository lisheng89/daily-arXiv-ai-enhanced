<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model](https://arxiv.org/abs/2602.06393)
*Geonmo Gu,Byeongho Heo,Jaemyung Yu,Jaehui Hwang,Taekyung Kim,Sangmin Lee,HeeJae Jun,Yoohoon Kang,Sangdoo Yun,Dongyoon Han*

Main category: cs.IR

TL;DR: 提出多轮对比学习框架MuCo，通过单次前向处理多个相关查询-目标对，提高训练效率和表示一致性


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入模型使用对比学习，但采用"单轮"范式，每个查询-目标对独立处理，导致计算效率低下且忽略了同一上下文中多个查询之间的潜在关系

Method: 引入多轮对比学习框架MuCo，利用MLLM的对话特性，在单次前向传播中处理与单个图像相关的多个查询-目标对，同时提取多个查询和目标嵌入，共享上下文表示

Result: 在MMEB和M-BEIR基准测试中达到最先进的检索性能，同时显著提高训练效率和跨模态表示一致性

Conclusion: MuCo框架通过多轮对比学习有效解决了传统单轮对比学习的计算效率问题，同时提升了多模态表示的质量和一致性

Abstract: Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a "single-turn" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco

</details>


### [2] [TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2602.06563)
*Yuchen Jiang,Jie Zhu,Xintian Han,Hui Lu,Kunmin Bai,Mingyu Yang,Shikang Wu,Ruihao Zhang,Wenlin Zhao,Shipeng Bai,Sijin Zhou,Huizhi Yang,Tianyi Liu,Wenda Liu,Ziyan Gong,Haoran Ding,Zheng Chai,Deping Xie,Zhe Chen,Yuchao Zheng,Peng Xu*

Main category: cs.IR

TL;DR: 论文提出了TokenMixer-Large，解决了原始TokenMixer架构的设计限制，通过混合-还原操作、层间残差、辅助损失和稀疏MoE架构，将参数扩展到70亿和150亿规模，在字节跳动多个场景中部署并取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型缩放研究（如Wukong、HiFormer、DHEN）的实验规模有限，而作者先前提出的TokenMixer架构虽然有效，但存在残差设计不佳、深度模型梯度更新不足、MoE稀疏化不完整、可扩展性探索有限等核心问题。

Method: 提出TokenMixer-Large架构，采用混合-还原操作、层间残差设计、辅助损失机制，以及新颖的稀疏Pertoken MoE架构，系统地解决了原始TokenMixer的设计限制。

Result: 成功将参数扩展到70亿（在线流量）和150亿（离线实验）规模，在字节跳动多个场景中部署，取得了显著的离线和在线性能提升。

Conclusion: TokenMixer-Large通过系统性地解决原始架构的核心问题，实现了大规模推荐模型的有效扩展，在实际工业场景中验证了其优越性能。

Abstract: In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.

</details>


### [3] [R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction](https://arxiv.org/abs/2602.06622)
*Qidong Liu,Gengnan Wang,Zhichen Liu,Moranxin Wang,Zijian Zhang,Xiao Han,Ni Zhang,Tao Qin,Chen Li*

Main category: cs.IR

TL;DR: R2LED：一种基于语义ID的终身用户建模新范式，通过多路由混合检索和双层融合精炼，解决传统方法中的噪声检索和语义理解不足问题，在性能和效率上均有优势。


<details>
  <summary>Details</summary>
Motivation: 现有终身用户建模方法采用"检索-精炼"两阶段策略，但仍面临两个问题：(1) 数据分布倾斜导致的噪声检索；(2) 精炼阶段缺乏语义理解。虽然语义增强（如LLMs建模或语义嵌入）提供了潜在解决方案，但这些方法面临推理成本过高或表示粒度不足的挑战。

Method: 提出R2LED框架，利用语义ID的多粒度性和轻量性优势。包含两个核心组件：1) 多路由混合检索：通过多个并行召回路由从不同粒度捕捉用户兴趣，并提出混合检索机制从协同和语义视角高效检索候选项；2) 双层融合精炼：包括用于路由级融合的目标感知交叉注意力和用于SID级融合的门控机制，弥合语义空间与协同空间的差距。

Result: 在两个公共数据集上的综合实验结果表明，该方法在性能和效率方面均表现出优越性。代码已开源供复现。

Conclusion: R2LED通过语义ID增强终身用户建模的检索和精炼阶段，有效解决了噪声检索和语义理解不足的问题，在保持效率的同时提升了CTR预测性能，为终身用户建模提供了新的解决方案。

Abstract: Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage "retrieval-refinement" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.

</details>


### [4] [Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan](https://arxiv.org/abs/2602.06654)
*Boyu Chen,Tai Guo,Weiyu Cui,Yuqing Li,Xingxing Wang,Chuan Shi,Cheng Yang*

Main category: cs.IR

TL;DR: 提出分阶段预训练策略解决多模态检索中模态不平衡问题，通过生成式和判别式任务利用语义ID提升性能，在美团数据上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 多模态检索模型在美团外卖等场景中很重要，但现有联合优化方法存在模态不平衡问题，某些模态主导训练而其他模态被忽视，且不同模态训练速度不一致容易导致"单周期问题"。

Method: 1) 提出分阶段预训练策略，每个阶段专注于特定任务，使模型能有效关注和利用多模态特征，灵活控制训练过程；2) 设计生成式和判别式任务来利用压缩高维多模态嵌入的语义ID，帮助模型理解语义ID、查询和物品特征之间的关联。

Result: 在美团大规模真实数据上，相比主流基线方法，R@5、R@10、R@20分别提升3.80%、2.64%、2.17%，N@5、N@10、N@20分别提升5.10%、4.22%、2.09%。在线A/B测试显示收入提升1.12%，点击率提升1.02%。

Conclusion: 提出的分阶段预训练策略能有效解决多模态检索中的模态不平衡问题，通过语义ID的生成式和判别式任务提升模型性能，在实际应用中验证了方法的有效性和优越性。

Abstract: Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.

</details>


### [5] [On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec](https://arxiv.org/abs/2602.06935)
*Shankar Veludandi,Gulrukh Kurdistan,Uzma Mushtaque*

Main category: cs.IR

TL;DR: Cotten4Rec是一种使用余弦相似度线性注意力机制的序列推荐模型，通过优化的CUDA内核实现，显著降低了BERT4Rec等Transformer模型的计算开销，在保持推荐准确性的同时大幅减少内存和运行时消耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐模型（如BERT4Rec）虽然能有效捕捉用户行为模式，但由于使用Softmax注意力机制，产生了大量的中间计算，导致显著的计算开销。在计算资源受限的实际大规模推荐场景中，这种开销成为瓶颈。

Method: 提出Cotten4Rec模型，采用基于余弦相似度的线性时间注意力机制，通过单个优化的CUDA内核实现，最小化中间缓冲区和内核启动开销。相比BERT4Rec和线性注意力基准模型LinRec，显著减少了资源使用。

Result: 在三个基准数据集上的评估表明，Cotten4Rec在适度序列长度和词汇量的数据集上，相比BERT4Rec和LinRec，实现了显著的内存和运行时减少，同时推荐准确性的损失很小。

Conclusion: Cotten4Rec证明了在计算资源关键的实际大规模序列推荐场景中，使用余弦相似度线性注意力机制作为高效替代方案的可行性，在保持良好推荐性能的同时大幅提升计算效率。

Abstract: Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.

</details>

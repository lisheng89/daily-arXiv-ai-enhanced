<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Towards Analysing Invoices and Receipts with Amazon Textract](https://arxiv.org/abs/2512.19958)
*Sneha Oommen,Gabby Sanchez,Cassandra T. Britto,Di Wang,Jordan Chiou,Maria Spichkova*

Main category: cs.IR

TL;DR: 评估AWS Textract在收据数据提取中的表现，分析其在不同格式和条件下的收据处理能力，识别典型问题并提出缓解策略


<details>
  <summary>Details</summary>
Motivation: 评估AWS Textract在现实场景中处理收据数据提取的实际效果，了解该服务在不同格式和质量收据上的表现，为实际应用提供参考

Method: 使用包含多种格式和条件的收据数据集，对AWS Textract功能进行定性分析，评估其在收据数据提取中的表现

Result: Textract在提取收据总额方面表现一致，但存在受图像质量和布局影响的典型问题和异常情况，需要针对性处理

Conclusion: AWS Textract在收据数据提取中具有实用价值，但图像质量和布局会影响提取效果，需要采用适当的缓解策略来应对识别问题

Abstract: This paper presents an evaluation of the AWS Textract in the context of extracting data from receipts. We analyse Textract functionalities using a dataset that includes receipts of varied formats and conditions. Our analysis provided a qualitative view of Textract strengths and limitations. While the receipts totals were consistently detected, we also observed typical issues and irregularities that were often influenced by image quality and layout. Based on the analysis of the observations, we propose mitigation strategies.

</details>


### [2] [IGDMRec: Behavior Conditioned Item Graph Diffusion for Multimodal Recommendation](https://arxiv.org/abs/2512.19983)
*Ziyuan Guo,Jie Guo,Zhenghao Chen,Bin Song,Fei Richard Yu*

Main category: cs.IR

TL;DR: IGDMRec利用扩散模型和分类器自由引导，通过整合用户行为信息来去噪语义物品图，提升多模态推荐系统性能


<details>
  <summary>Details</summary>
Motivation: 现有基于结构的多模态推荐系统通过构建语义物品图取得了SOTA性能，但这些图存在噪声问题：1）多模态信息固有噪声；2）物品语义与用户-物品共现关系不对齐，导致虚假链接和次优推荐

Method: 提出IGDMRec方法：1）行为条件图扩散模块，将交互数据作为条件信息指导语义物品图去噪；2）条件去噪网络实现可管理复杂度的去噪过程；3）对比表示增强方案，利用去噪图和原始图增强物品表示

Result: 在四个真实世界数据集上的实验表明IGDMRec优于竞争基线，鲁棒性分析验证了其去噪能力，消融研究证实了关键组件的有效性

Conclusion: IGDMRec通过扩散模型整合用户行为信息有效去噪语义物品图，提升了多模态推荐系统的性能

Abstract: Multimodal recommender systems (MRSs) are critical for various online platforms, offering users more accurate personalized recommendations by incorporating multimodal information of items. Structure-based MRSs have achieved state-of-the-art performance by constructing semantic item graphs, which explicitly model relationships between items based on modality feature similarity. However, such semantic item graphs are often noisy due to 1) inherent noise in multimodal information and 2) misalignment between item semantics and user-item co-occurrence relationships, which introduces false links and leads to suboptimal recommendations. To address this challenge, we propose Item Graph Diffusion for Multimodal Recommendation (IGDMRec), a novel method that leverages a diffusion model with classifier-free guidance to denoise the semantic item graph by integrating user behavioral information. Specifically, IGDMRec introduces a Behavior-conditioned Graph Diffusion (BGD) module, incorporating interaction data as conditioning information to guide the denoising of the semantic item graph. Additionally, a Conditional Denoising Network (CD-Net) is designed to implement the denoising process with manageable complexity. Finally, we propose a contrastive representation augmentation scheme that leverages both the denoised item graph and the original item graph to enhance item representations. \LL{Extensive experiments on four real-world datasets demonstrate the superiority of IGDMRec over competitive baselines, with robustness analysis validating its denoising capability and ablation studies verifying the effectiveness of its key components.

</details>


### [3] [LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews](https://arxiv.org/abs/2512.20022)
*Kian Godhwani,David Benrimoh*

Main category: cs.IR

TL;DR: OLIVER是一个用于LLM辅助文献筛选的开源管道，评估显示单模型性能受综述特征和提示设计影响大且校准差，而演员-评论家框架能显著提升分类质量和校准可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助文献筛选研究存在局限性：主要评估早期模型、标准化Cochrane综述、单模型设置，且以准确性为主要指标，缺乏对泛化性、配置效果和校准的全面考察。

Method: 开发OLIVER开源管道，评估多个当代LLM在两个非Cochrane系统综述中的表现，使用准确性、AUC和校准指标，并测试结合两个轻量模型的演员-评论家筛选框架及三种聚合规则。

Result: 单模型性能差异大：小综述中模型敏感性高但假阳性多、校准差；大综述中特异性高但召回率低，提示设计影响召回。单模型校准普遍弱。演员-评论家框架显著提升区分度和校准，AUC更高。

Conclusion: LLM有潜力加速文献筛选，但单模型性能受综述特征和提示设计影响大且校准有限。演员-评论家框架能提升分类质量和置信度可靠性，同时保持计算效率，实现低成本大规模筛选。

Abstract: Introduction: Recent work suggests large language models (LLMs) can accelerate screening, but prior evaluations focus on earlier LLMs, standardized Cochrane reviews, single-model setups, and accuracy as the primary metric, leaving generalizability, configuration effects, and calibration largely unexamined.
  Methods: We developed OLIVER (Optimized LLM-based Inclusion and Vetting Engine for Reviews), an open-source pipeline for LLM-assisted abstract screening. We evaluated multiple contemporary LLMs across two non-Cochrane systematic reviews and performance was assessed at both the full-text screening and final inclusion stages using accuracy, AUC, and calibration metrics. We further tested an actor-critic screening framework combining two lightweight models under three aggregation rules.
  Results: Across individual models, performance varied widely. In the smaller Review 1 (821 abstracts, 63 final includes), several models achieved high sensitivity for final includes but at the cost of substantial false positives and poor calibration. In the larger Review 2 (7741 abstracts, 71 final includes), most models were highly specific but struggled to recover true includes, with prompt design influencing recall. Calibration was consistently weak across single-model configurations despite high overall accuracy. Actor-critic screening improved discrimination and markedly reduced calibration error in both reviews, yielding higher AUCs.
  Discussion: LLMs may eventually accelerate abstract screening, but single-model performance is highly sensitive to review characteristics, prompting, and calibration is limited. An actor-critic framework improves classification quality and confidence reliability while remaining computationally efficient, enabling large-scale screening at low cost.

</details>


### [4] [VSA:Visual-Structural Alignment for UI-to-Code](https://arxiv.org/abs/2512.20034)
*Xian Wu,Ming Zhang,Zhiyu Fang,Fei Li,Bin Wang,Yong Jiang,Hao Zhou*

Main category: cs.IR

TL;DR: VSA提出了一种多阶段范式，通过视觉-结构对齐合成有组织的前端资产，解决了现有设计到代码转换方法生成非结构化代码、缺乏组件化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型的设计到代码转换方法主要生成非结构化、扁平化的代码库，缺乏与React或Angular等组件化库的兼容性，导致代码低内聚、高耦合，长期维护困难。

Method: 1) 使用空间感知transformer将视觉输入重构为层次树表示；2) 集成算法模式匹配层识别重复UI模式并封装为模块化模板；3) 通过模式驱动的合成引擎处理模板，确保LLM生成类型安全、支持属性传递的生产级组件。

Result: 实验结果表明，该框架在代码模块化和架构一致性方面相比现有基准有显著提升，有效弥合了原始像素与可扩展软件工程之间的差距。

Conclusion: VSA通过视觉-结构对齐的多阶段范式，能够生成组织良好的前端资产，解决了设计到代码转换中的组件化和可维护性问题。

Abstract: The automation of user interface development has the potential to accelerate software delivery by mitigating intensive manual implementation. Despite the advancements in Large Multimodal Models for design-to-code translation, existing methodologies predominantly yield unstructured, flat codebases that lack compatibility with component-oriented libraries such as React or Angular. Such outputs typically exhibit low cohesion and high coupling, complicating long-term maintenance. In this paper, we propose \textbf{VSA (VSA)}, a multi-stage paradigm designed to synthesize organized frontend assets through visual-structural alignment. Our approach first employs a spatial-aware transformer to reconstruct the visual input into a hierarchical tree representation. Moving beyond basic layout extraction, we integrate an algorithmic pattern-matching layer to identify recurring UI motifs and encapsulate them into modular templates. These templates are then processed via a schema-driven synthesis engine, ensuring the Large Language Model generates type-safe, prop-drilled components suitable for production environments. Experimental results indicate that our framework yields a substantial improvement in code modularity and architectural consistency over state-of-the-art benchmarks, effectively bridging the gap between raw pixels and scalable software engineering.

</details>


### [5] [Collaborative Group-Aware Hashing for Fast Recommender Systems](https://arxiv.org/abs/2512.20172)
*Yan Zhang,Li Deng,Lixin Duan,Ivor W. Tsang,Guowu Yang*

Main category: cs.IR

TL;DR: 提出CGAH方法，通过整合用户和物品的固有群组信息来缓解稀疏性问题，提高哈希推荐系统的准确率


<details>
  <summary>Details</summary>
Motivation: 大规模数据库的快速在线推荐至关重要，但稀疏场景下准确推荐具有挑战性。现有哈希推荐方法在稀疏设置下准确率低，主要因为每位表示能力有限且忽略了用户和物品间的固有关系

Method: 提出协作群组感知哈希(CGAH)方法：1) 通过将潜在向量分类到不同群组来提取用户和物品的固有群组亲和性；2) 将偏好建模为群组亲和性与哈希码相似度的内积；3) 学习包含固有群组信息的哈希码

Result: 在三个公共数据集上的广泛实验表明，CGAH和CGAH-CF在不同稀疏设置下优于最先进的离散协作过滤方法和离散内容感知推荐方法

Conclusion: 通过整合固有群组信息，CGAH方法能够从稀疏交互数据中学习到比其他离散方法更有效的哈希码，显著提高了哈希推荐系统在稀疏场景下的准确率

Abstract: The fast online recommendation is critical for applications with large-scale databases; meanwhile, it is challenging to provide accurate recommendations in sparse scenarios. Hash technique has shown its superiority for speeding up the online recommendation by bit operations on Hamming distance computations. However, existing hashing-based recommendations suffer from low accuracy, especially with sparse settings, due to the limited representation capability of each bit and neglected inherent relations among users and items. To this end, this paper lodges a Collaborative Group-Aware Hashing (CGAH) method for both collaborative filtering (namely CGAH-CF) and content-aware recommendations (namely CGAH) by integrating the inherent group information to alleviate the sparse issue. Firstly, we extract inherent group affinities of users and items by classifying their latent vectors into different groups. Then, the preference is formulated as the inner product of the group affinity and the similarity of hash codes. By learning hash codes with the inherent group information, CGAH obtains more effective hash codes than other discrete methods with sparse interactive data. Extensive experiments on three public datasets show the superior performance of our proposed CGAH and CGAH-CF over the state-of-the-art discrete collaborative filtering methods and discrete content-aware recommendations under different sparse settings.

</details>


### [6] [Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register](https://arxiv.org/abs/2512.20458)
*Shuting Wang,Qiaolin Xia,Hao Wang,Yu Lu,Bobsimons,Zhicheng Dou*

Main category: cs.IR

TL;DR: Laser是一个结构化代理搜索框架，通过符号化动作协议和紧凑上下文寄存器解决现有LLM代理搜索中推理轨迹不稳定、上下文溢出等问题，在复杂多跳查询中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM/LRM的代理搜索系统主要依赖非结构化的自然语言推理，将原始中间轨迹累积在上下文中，导致推理轨迹不稳定、上下文溢出，在复杂多跳查询中性能下降。

Method: Laser定义了一个符号化动作协议，将代理行为组织为三个空间：规划、任务解决和反思。每个动作都有明确的语义和确定性执行格式，配合紧凑的上下文寄存器只存储推理过程的关键状态。

Result: 在Qwen2.5/3系列模型上的实验表明，Laser在具有挑战性的多跳QA数据集上持续优于现有的代理搜索基线方法，无论是在仅提示还是微调设置下。

Conclusion: Laser为稳健、可扩展的代理搜索提供了一个原则性和有效的基础，通过结构化推理过程和可控的上下文管理解决了现有方法的局限性。

Abstract: Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.

</details>


### [7] [Making Large Language Models Efficient Dense Retrievers](https://arxiv.org/abs/2512.20612)
*Yibin Lei,Shwai He,Ang Li,Andrew Yates*

Main category: cs.IR

TL;DR: EffiR框架通过分析LLM在检索任务中的层冗余，发现MLP层比注意力层更可剪枝，采用粗到细策略压缩MLP层，实现高效检索器


<details>
  <summary>Details</summary>
Motivation: 现有直接微调LLM进行稠密检索的方法虽然性能强，但参数量大导致计算效率低。虽然已有研究显示LLM在生成任务中存在层冗余，但尚不清楚在需要将整个序列编码为固定表示的检索任务中是否存在类似冗余

Method: 提出EffiR框架：1) 分析LLM在检索任务中的层冗余特性；2) 发现MLP层比注意力层更可剪枝；3) 采用粗到细策略进行大规模MLP压缩（粗粒度深度减少+细粒度宽度减少）；4) 结合检索特定微调

Result: 在多种BEIR数据集和LLM骨干网络上，EffiR显著减少了模型大小和推理成本，同时保持了全尺寸模型的性能

Conclusion: LLM在检索任务中的层冗余模式与生成任务不同，MLP层更可剪枝而注意力层对语义聚合更关键。EffiR框架通过针对性压缩MLP层，实现了高效且性能相当的检索器

Abstract: Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.

</details>

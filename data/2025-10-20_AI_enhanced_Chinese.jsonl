{"id": "2510.15087", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15087", "abs": "https://arxiv.org/abs/2510.15087", "authors": ["Kai Yin", "Xiangjue Dong", "Chengkai Liu", "Allen Lin", "Lingfeng Shi", "Ali Mostafavi", "James Caverlee"], "title": "DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management", "comment": null, "summary": "Effective and efficient access to relevant information is essential for\ndisaster management. However, no retrieval model is specialized for disaster\nmanagement, and existing general-domain models fail to handle the varied search\nintents inherent to disaster management scenarios, resulting in inconsistent\nand unreliable performance. To this end, we introduce DMRetriever, the first\nseries of dense retrieval models (33M to 7.6B) tailored for this domain. It is\ntrained through a novel three-stage framework of bidirectional attention\nadaptation, unsupervised contrastive pre-training, and difficulty-aware\nprogressive instruction fine-tuning, using high-quality data generated through\nan advanced data refinement pipeline. Comprehensive experiments demonstrate\nthat DMRetriever achieves state-of-the-art (SOTA) performance across all six\nsearch intents at every model scale. Moreover, DMRetriever is highly\nparameter-efficient, with 596M model outperforming baselines over 13.3 X larger\nand 33M model exceeding baselines with only 7.6% of their parameters. All\ncodes, data, and checkpoints are available at\nhttps://github.com/KaiYin97/DMRETRIEVER", "AI": {"tldr": "DMRetriever\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u707e\u5bb3\u7ba1\u7406\u8bbe\u8ba1\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u53c2\u6570\u6548\u7387\u6781\u9ad8\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u68c0\u7d22\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u707e\u5bb3\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u591a\u6837\u5316\u641c\u7d22\u610f\u56fe\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u4e0d\u53ef\u9760\u3002", "method": "\u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\u9002\u5e94\u3001\u65e0\u76d1\u7763\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u96be\u5ea6\u611f\u77e5\u6e10\u8fdb\u6307\u4ee4\u5fae\u8c03\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u8d28\u91cf\u6570\u636e\u7cbe\u70bc\u6d41\u6c34\u7ebf\u3002", "result": "\u5728\u6240\u6709\u516d\u79cd\u641c\u7d22\u610f\u56fe\u548c\u6bcf\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u8fbe\u5230SOTA\u6027\u80fd\uff0c596M\u6a21\u578b\u8d85\u8fc7\u57fa\u7ebf13.3\u500d\u5927\u6a21\u578b\uff0c33M\u6a21\u578b\u4ec5\u7528\u57fa\u7ebf7.6%\u53c2\u6570\u5c31\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "DMRetriever\u4e3a\u707e\u5bb3\u7ba1\u7406\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e13\u4e1a\u5316\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u5353\u8d8a\u3002"}}
{"id": "2510.15286", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15286", "abs": "https://arxiv.org/abs/2510.15286", "authors": ["Xianyang Qi", "Yuan Tian", "Zhaoyu Hu", "Zhirui Kuai", "Chang Liu", "Hongxiang Lin", "Lei Wang"], "title": "MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation", "comment": null, "summary": "Industrial recommender systems critically depend on high-quality ranking\nmodels. However, traditional pipelines still rely on manual feature engineering\nand scenario-specific architectures, which hinder cross-scenario transfer and\nlarge-scale deployment. To address these challenges, we propose\n\\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with\nMulti-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt\nintegrates two key components. The \\textbf{AutoToken} module automatically\nclusters heterogeneous features into semantically coherent tokens, removing the\nneed for human-defined feature groups. The \\textbf{MTmixAttBlock} module\nenables efficient token interaction via a learnable mixing matrix, shared dense\nexperts, and scenario-aware sparse experts, capturing both global patterns and\nscenario-specific behaviors within a single framework. Extensive experiments on\nthe industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently\noutperforms state-of-the-art baselines including Transformer-based models,\nWuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,\nMTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields\nfurther monotonic gains. Large-scale online A/B tests validate the real-world\nimpact: in the \\textit{Homepage} scenario, MTmixAtt increases Payment PV by\n\\textbf{+3.62\\%} and Actual Payment GTV by \\textbf{+2.54\\%}. Overall, MTmixAtt\nprovides a unified and scalable solution for modeling arbitrary heterogeneous\nfeatures across scenarios, significantly improving both user experience and\ncommercial outcomes.", "AI": {"tldr": "MTmixAtt\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u52a8\u7279\u5f81\u805a\u7c7b\u548c\u591a\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u548c\u573a\u666f\u7279\u5b9a\u67b6\u6784\u7684\u9650\u5236\uff0c\u5728\u5de5\u4e1a\u63a8\u8350\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\u548c\u573a\u666f\u7279\u5b9a\u67b6\u6784\uff0c\u963b\u788d\u4e86\u8de8\u573a\u666f\u8fc1\u79fb\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u9700\u8981\u7edf\u4e00\u7684\u67b6\u6784\u6765\u5904\u7406\u5f02\u6784\u7279\u5f81\u5e76\u5b9e\u73b0\u8de8\u573a\u666f\u5efa\u6a21\u3002", "method": "\u63d0\u51faMTmixAtt\u67b6\u6784\uff0c\u5305\u542bAutoToken\u6a21\u5757\u81ea\u52a8\u805a\u7c7b\u5f02\u6784\u7279\u5f81\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684token\uff0c\u4ee5\u53caMTmixAttBlock\u6a21\u5757\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6df7\u5408\u77e9\u9635\u3001\u5171\u4eab\u5bc6\u96c6\u4e13\u5bb6\u548c\u573a\u666f\u611f\u77e5\u7a00\u758f\u4e13\u5bb6\u5b9e\u73b0\u9ad8\u6548token\u4ea4\u4e92\u3002", "result": "\u5728\u7f8e\u56e2TRec\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u591a\u4e2aSOTA\u57fa\u7ebf\u6a21\u578b\u3002\u5728\u53ef\u6bd4\u53c2\u6570\u91cf\u4e0b\u83b7\u5f97\u66f4\u4f18\u7684CTR\u548cCTCVR\u6307\u6807\uff0cMTmixAtt-1B\u7248\u672c\u5b9e\u73b0\u8fdb\u4e00\u6b65\u5355\u8c03\u589e\u76ca\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u9996\u9875\u573a\u666f\u652f\u4ed8PV\u63d0\u53473.62%\uff0c\u5b9e\u9645\u652f\u4ed8GTV\u63d0\u53472.54%\u3002", "conclusion": "MTmixAtt\u4e3a\u8de8\u573a\u666f\u5efa\u6a21\u4efb\u610f\u5f02\u6784\u7279\u5f81\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u5546\u4e1a\u6210\u679c\u3002"}}
{"id": "2510.15299", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15299", "abs": "https://arxiv.org/abs/2510.15299", "authors": ["Yijia Sun", "Shanshan Huang", "Zhiyuan Guan", "Qiang Luo", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "title": "GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework", "comment": null, "summary": "Industrial-scale recommender systems rely on a cascade pipeline in which the\nretrieval stage must return a high-recall candidate set from billions of items\nunder tight latency. Existing solutions ei- ther (i) suffer from limited\nexpressiveness in capturing fine-grained user-item interactions, as seen in\ndecoupled dual-tower architectures that rely on separate encoders, or\ngenerative models that lack precise target-aware matching capabilities, or (ii)\nbuild structured indices (tree, graph, quantization) whose item-centric\ntopologies struggle to incorporate dynamic user preferences and incur\nprohibitive construction and maintenance costs.\n  We present GRank, a novel structured-index-free retrieval paradigm that\nseamlessly unifies target-aware learning with user-centric retrieval. Our key\ninnovations include: (1) A target-aware Generator trained to perform\npersonalized candidate generation via GPU-accelerated MIPS, eliminating\nsemantic drift and maintenance costs of structured indexing; (2) A lightweight\nbut powerful Ranker that performs fine-grained, candidate-specific inference on\nsmall subsets; (3) An end-to-end multi-task learning framework that ensures\nsemantic consistency between generation and ranking objectives.\n  Extensive experiments on two public benchmarks and a billion-item production\ncorpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\\times$\nthe P99 QPS of state-of-the-art tree- and graph-based retrievers.\n  GRank has been fully deployed in production in our recommendation platform\nsince Q2 2025, serving 400 million active users with 99.95% service\navailability. Online A/B tests confirm significant improvements in core\nengagement metrics, with Total App Usage Time increasing by 0.160% in the main\napp and 0.165% in the Lite version.", "AI": {"tldr": "GRank\u662f\u4e00\u4e2a\u65e0\u9700\u7ed3\u6784\u5316\u7d22\u5f15\u7684\u68c0\u7d22\u65b0\u8303\u5f0f\uff0c\u5c06\u76ee\u6807\u611f\u77e5\u5b66\u4e60\u4e0e\u7528\u6237\u4e2d\u5fc3\u68c0\u7d22\u65e0\u7f1d\u7edf\u4e00\uff0c\u5728\u53ec\u56de\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u68c0\u7d22\u9636\u6bb5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u53cc\u5854\u67b6\u6784\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\uff1b(2)\u7ed3\u6784\u5316\u7d22\u5f15\u65b9\u6cd5\u96be\u4ee5\u878d\u5165\u52a8\u6001\u7528\u6237\u504f\u597d\uff0c\u4e14\u6784\u5efa\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\u3002", "method": "GRank\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1)\u76ee\u6807\u611f\u77e5\u751f\u6210\u5668\u901a\u8fc7GPU\u52a0\u901fMIPS\u8fdb\u884c\u4e2a\u6027\u5316\u5019\u9009\u751f\u6210\uff1b(2)\u8f7b\u91cf\u7ea7\u6392\u5e8f\u5668\u5728\u5c0f\u5b50\u96c6\u4e0a\u6267\u884c\u7ec6\u7c92\u5ea6\u63a8\u7406\uff1b(3)\u7aef\u5230\u7aef\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u786e\u4fdd\u751f\u6210\u548c\u6392\u5e8f\u76ee\u6807\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u5341\u4ebf\u7ea7\u751f\u4ea7\u8bed\u6599\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRank\u5c06Recall@500\u63d0\u5347\u8d85\u8fc730%\uff0cP99 QPS\u8fbe\u5230\u6700\u5148\u8fdb\u6811\u548c\u57fa\u4e8e\u56fe\u68c0\u7d22\u5668\u76841.7\u500d\u3002", "conclusion": "GRank\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u670d\u52a14\u4ebf\u6d3b\u8dc3\u7528\u6237\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u786e\u8ba4\u6838\u5fc3\u53c2\u4e0e\u5ea6\u6307\u6807\u663e\u8457\u63d0\u5347\uff0c\u603b\u5e94\u7528\u4f7f\u7528\u65f6\u95f4\u5728\u4e3b\u5e94\u7528\u548c\u8f7b\u91cf\u7248\u5206\u522b\u589e\u52a00.160%\u548c0.165%\u3002"}}
{"id": "2510.15308", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15308", "abs": "https://arxiv.org/abs/2510.15308", "authors": ["Srijan Saket", "Ikuhiro Ihara", "Vaibhav Sharma", "Danish Kalim"], "title": "Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models", "comment": "7 pages, 6 figures, 2 tables", "summary": "In modern recommendation systems and social media platforms like Meta,\nTikTok, and Instagram, large-scale ID-based features often require embedding\ntables that consume significant memory. Managing these embedding sizes can be\nchallenging, leading to bulky models that are harder to deploy and maintain. In\nthis paper, we introduce a method to automatically determine the optimal\nembedding size for ID features, significantly reducing the model size while\nmaintaining performance.\n  Our approach involves defining a custom Keras layer called the dimension mask\nlayer, which sits directly after the embedding lookup. This layer trims the\nembedding vector by allowing only the first N dimensions to pass through. By\ndoing this, we can reduce the input feature dimension by more than half with\nminimal or no loss in model performance metrics. This reduction helps cut down\nthe memory footprint of the model and lowers the risk of overfitting due to\nmulticollinearity.\n  Through offline experiments on public datasets and an online A/B test on a\nreal production dataset, we demonstrate that using a dimension mask layer can\nshrink the effective embedding dimension by 40-50\\%, leading to substantial\nimprovements in memory efficiency. This method provides a scalable solution for\nplatforms dealing with a high volume of ID features, optimizing both resource\nusage and model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u786e\u5b9aID\u7279\u5f81\u6700\u4f73\u5d4c\u5165\u5c3a\u5bf8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ef4\u5ea6\u63a9\u7801\u5c42\u4fee\u526a\u5d4c\u5165\u5411\u91cf\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u548c\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e2d\u7684\u5927\u89c4\u6a21ID\u7279\u5f81\u9700\u8981\u6d88\u8017\u5927\u91cf\u5185\u5b58\u7684\u5d4c\u5165\u8868\uff0c\u5bfc\u81f4\u6a21\u578b\u5e9e\u5927\u96be\u4ee5\u90e8\u7f72\u548c\u7ef4\u62a4\u3002", "method": "\u5b9a\u4e49\u81ea\u5b9a\u4e49Keras\u7ef4\u5ea6\u63a9\u7801\u5c42\uff0c\u653e\u7f6e\u5728\u5d4c\u5165\u67e5\u627e\u4e4b\u540e\uff0c\u901a\u8fc7\u4ec5\u5141\u8bb8\u524dN\u4e2a\u7ef4\u5ea6\u901a\u8fc7\u6765\u4fee\u526a\u5d4c\u5165\u5411\u91cf\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u771f\u5b9e\u751f\u4ea7\u6570\u636e\u7684A/B\u6d4b\u8bd5\u4e2d\uff0c\u6709\u6548\u5d4c\u5165\u7ef4\u5ea6\u51cf\u5c1140-50%\uff0c\u5185\u5b58\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u5927\u91cfID\u7279\u5f81\u7684\u5e73\u53f0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u8d44\u6e90\u4f7f\u7528\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.15428", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15428", "abs": "https://arxiv.org/abs/2510.15428", "authors": ["Sho Okazaki", "Kohei Kaminishi", "Takuma Fujiu", "Yusheng Wang", "Jun Ota"], "title": "Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs", "comment": null, "summary": "Fault cause identification in automated manufacturing lines is challenging\ndue to the system's complexity, frequent reconfigurations, and the limited\nreusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.\nAlthough FMEA worksheets contain valuable expert insights, their reuse across\nheterogeneous lines is hindered by natural language variability, inconsistent\nterminology, and process differences. To address these limitations, this study\nproposes a process-aware framework that enhances FMEA reusability by combining\nmanufacturing-domain conceptualization with graph neural network (GNN)\nreasoning. First, FMEA worksheets from multiple manufacturing lines are\ntransformed into a unified knowledge graph through ontology-guided large\nlanguage model (LLM) extraction, capturing domain concepts such as actions,\nstates, components, and parameters. Second, a Relational Graph Convolutional\nNetwork (RGCN) with the process-aware scoring function learns embeddings that\nrespect both semantic relationships and sequential process flows. Finally, link\nprediction is employed to infer and rank candidate fault causes consistent with\nthe target line's process flow.\n  A case study on automotive pressure sensor assembly lines demonstrates that\nthe proposed method outperforms a state-of-the-art retrieval-augmented\ngeneration (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),\nachieving the best performance (0.523) in fault cause identification. Ablation\nstudies confirm the contributions of both LLM-driven domain conceptualization\nand process-aware learning. These results indicate that the proposed framework\nsignificantly improves the transferability of FMEA knowledge across\nheterogeneous lines, thereby supporting operators in diagnosing failures more\nreliably and paving the way for future domain-adaptive LLM applications in\nsmart manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5236\u9020\u9886\u57df\u6982\u5ff5\u5316\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u8fc7\u7a0b\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u4f53\u5f15\u5bfc\u7684LLM\u63d0\u53d6\u5c06FMEA\u5de5\u4f5c\u8868\u8f6c\u6362\u4e3a\u7edf\u4e00\u77e5\u8bc6\u56fe\uff0c\u4f7f\u7528RGCN\u8fdb\u884c\u94fe\u63a5\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86FMEA\u77e5\u8bc6\u5728\u5f02\u6784\u751f\u4ea7\u7ebf\u95f4\u7684\u53ef\u91cd\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u5316\u5236\u9020\u751f\u4ea7\u7ebf\u4e2d\u6545\u969c\u539f\u56e0\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5305\u62ec\u7cfb\u7edf\u590d\u6742\u6027\u3001\u9891\u7e41\u91cd\u65b0\u914d\u7f6e\u4ee5\u53ca\u73b0\u6709FMEA\u77e5\u8bc6\u6709\u9650\u7684\u53ef\u91cd\u7528\u6027\uff0c\u7279\u522b\u662f\u514b\u670d\u81ea\u7136\u8bed\u8a00\u53d8\u5f02\u6027\u3001\u672f\u8bed\u4e0d\u4e00\u81f4\u548c\u6d41\u7a0b\u5dee\u5f02\u5bf9FMEA\u77e5\u8bc6\u8de8\u7ebf\u91cd\u7528\u7684\u963b\u788d\u3002", "method": "1. \u901a\u8fc7\u672c\u4f53\u5f15\u5bfc\u7684LLM\u63d0\u53d6\u5c06\u591a\u4e2a\u751f\u4ea7\u7ebf\u7684FMEA\u5de5\u4f5c\u8868\u8f6c\u6362\u4e3a\u7edf\u4e00\u77e5\u8bc6\u56fe\uff1b2. \u4f7f\u7528\u5177\u6709\u8fc7\u7a0b\u611f\u77e5\u8bc4\u5206\u51fd\u6570\u7684RGCN\u5b66\u4e60\u65e2\u5c0a\u91cd\u8bed\u4e49\u5173\u7cfb\u53c8\u8003\u8651\u987a\u5e8f\u6d41\u7a0b\u7684\u5d4c\u5165\uff1b3. \u91c7\u7528\u94fe\u63a5\u9884\u6d4b\u6765\u63a8\u65ad\u548c\u6392\u5e8f\u4e0e\u76ee\u6807\u751f\u4ea7\u7ebf\u6d41\u7a0b\u4e00\u81f4\u7684\u5019\u9009\u6545\u969c\u539f\u56e0\u3002", "result": "\u5728\u6c7d\u8f66\u538b\u529b\u4f20\u611f\u5668\u88c5\u914d\u7ebf\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6545\u969c\u539f\u56e0\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff08F1@20 = 0.523\uff09\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u7ebf\uff080.267\uff09\u548cRGCN\u65b9\u6cd5\uff080.400\uff09\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86LLM\u9a71\u52a8\u7684\u9886\u57df\u6982\u5ff5\u5316\u548c\u8fc7\u7a0b\u611f\u77e5\u5b66\u4e60\u7684\u8d21\u732e\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86FMEA\u77e5\u8bc6\u5728\u5f02\u6784\u751f\u4ea7\u7ebf\u95f4\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u6709\u52a9\u4e8e\u64cd\u4f5c\u4eba\u5458\u66f4\u53ef\u9760\u5730\u8bca\u65ad\u6545\u969c\uff0c\u5e76\u4e3a\u667a\u80fd\u5236\u9020\u4e2d\u672a\u6765\u9886\u57df\u81ea\u9002\u5e94LLM\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.15647", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.15647", "abs": "https://arxiv.org/abs/2510.15647", "authors": ["Zhisheng Yang", "Xiaofei Xu", "Ke Deng", "Li Li"], "title": "Enhance Large Language Models as Recommendation Systems with Collaborative Filtering", "comment": null, "summary": "As powerful tools in Natural Language Processing (NLP), Large Language Models\n(LLMs) have been leveraged for crafting recommendations to achieve precise\nalignment with user preferences and elevate the quality of the recommendations.\nThe existing approaches implement both non-tuning and tuning strategies.\nCompared to following the tuning strategy, the approaches following the\nnon-tuning strategy avoid the relatively costly, time-consuming, and\nexpertise-requiring process of further training pre-trained LLMs on\ntask-specific datasets, but they suffer the issue of not having the\ntask-specific business or local enterprise knowledge. To the best of our\nknowledge, none of the existing approaches following the non-tuning strategy\nexplicitly integrates collaborative filtering, one of the most successful\nrecommendation techniques. This study aims to fill the gap by proposing\ncritique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,\nwe train a separate machine-learning model called Critic that implements\ncollaborative filtering for recommendations by learning from the interactions\nbetween many users and items. The Critic provides critiques to LLMs to\nsignificantly refine the recommendations. Extensive experiments have verified\nthe effectiveness of Critic-LLM-RS on real datasets.", "AI": {"tldr": "\u63d0\u51faCritic-LLM-RS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u72ec\u7acb\u7684\u534f\u540c\u8fc7\u6ee4\u6a21\u578bCritic\u4e3aLLMs\u63d0\u4f9b\u53cd\u9988\uff0c\u5728\u4e0d\u5fae\u8c03LLMs\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u63a8\u8350\u8d28\u91cf", "motivation": "\u73b0\u6709\u975e\u5fae\u8c03\u7b56\u7565\u7684LLM\u63a8\u8350\u65b9\u6cd5\u7f3a\u4e4f\u7279\u5b9a\u4efb\u52a1\u7684\u4f01\u4e1a\u77e5\u8bc6\uff0c\u4e14\u672a\u6709\u6548\u6574\u5408\u534f\u540c\u8fc7\u6ee4\u8fd9\u4e00\u6210\u529f\u63a8\u8350\u6280\u672f", "method": "\u8bad\u7ec3\u72ec\u7acb\u7684\u534f\u540c\u8fc7\u6ee4\u6a21\u578bCritic\uff0c\u5b66\u4e60\u7528\u6237\u4e0e\u7269\u54c1\u7684\u4ea4\u4e92\u5173\u7cfb\uff0c\u4e3aLLMs\u63d0\u4f9b\u53cd\u9988\u6765\u4f18\u5316\u63a8\u8350\u7ed3\u679c", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Critic-LLM-RS\u7684\u6709\u6548\u6027", "conclusion": "Critic-LLM-RS\u6210\u529f\u586b\u8865\u4e86\u975e\u5fae\u8c03LLM\u63a8\u8350\u65b9\u6cd5\u4e2d\u534f\u540c\u8fc7\u6ee4\u6574\u5408\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf"}}
{"id": "2510.15682", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15682", "abs": "https://arxiv.org/abs/2510.15682", "authors": ["Ines Besrour", "Jingbo He", "Tobias Schreieder", "Michael F\u00e4rber"], "title": "SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation", "comment": "Accepted at CIKM 2025", "summary": "We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy\nmulti-agent retrieval-augmented generation (RAG) framework for scientific\nquestion answering (QA) with large language models (LLMs). SQuAI addresses key\nlimitations of existing RAG systems in the scholarly domain, where complex,\nopen-domain questions demand accurate answers, explicit claims with citations,\nand retrieval across millions of scientific documents. Built on over 2.3\nmillion full-text papers from arXiv.org, SQuAI employs four collaborative\nagents to decompose complex questions into sub-questions, retrieve targeted\nevidence via hybrid sparse-dense retrieval, and adaptively filter documents to\nimprove contextual relevance. To ensure faithfulness and traceability, SQuAI\nintegrates in-line citations for each generated claim and provides supporting\nsentences from the source documents. Our system improves faithfulness, answer\nrelevance, and contextual relevance by up to +0.088 (12%) over a strong RAG\nbaseline. We further release a benchmark of 1,000 scientific\nquestion-answer-evidence triplets to support reproducibility. With transparent\nreasoning, verifiable citations, and domain-wide scalability, SQuAI\ndemonstrates how multi-agent RAG enables more trustworthy scientific QA with\nLLMs.", "AI": {"tldr": "SQuAI\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u4fe1\u8d56\u7684\u591a\u667a\u80fd\u4f53\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u79d1\u5b66\u95ee\u7b54\uff0c\u57fa\u4e8e230\u4e07\u7bc7arXiv\u8bba\u6587\uff0c\u901a\u8fc7\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\u5206\u89e3\u590d\u6742\u95ee\u9898\u3001\u68c0\u7d22\u8bc1\u636e\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5f15\u7528\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5904\u7406\u590d\u6742\u5f00\u653e\u57df\u95ee\u9898\u9700\u8981\u51c6\u786e\u7b54\u6848\u3001\u660e\u786e\u7684\u5f15\u7528\u58f0\u660e\u4ee5\u53ca\u8de8\u6570\u767e\u4e07\u79d1\u5b66\u6587\u6863\u7684\u68c0\u7d22\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1a\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u7a00\u758f-\u7a20\u5bc6\u68c0\u7d22\u83b7\u53d6\u76ee\u6807\u8bc1\u636e\uff0c\u81ea\u9002\u5e94\u8fc7\u6ee4\u6587\u6863\u4ee5\u63d0\u9ad8\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u96c6\u6210\u5185\u8054\u5f15\u7528\u5e76\u63d0\u4f9b\u6e90\u6587\u6863\u652f\u6301\u53e5\u5b50\u3002", "result": "\u76f8\u6bd4\u5f3aRAG\u57fa\u7ebf\uff0c\u5728\u5fe0\u5b9e\u5ea6\u3001\u7b54\u6848\u76f8\u5173\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u65b9\u9762\u63d0\u5347\u4e86\u9ad8\u8fbe+0.088\uff0812%\uff09\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1000\u4e2a\u79d1\u5b66\u95ee\u7b54\u8bc1\u636e\u4e09\u5143\u7ec4\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "SQuAI\u901a\u8fc7\u900f\u660e\u63a8\u7406\u3001\u53ef\u9a8c\u8bc1\u5f15\u7528\u548c\u9886\u57df\u7ea7\u53ef\u6269\u5c55\u6027\uff0c\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53RAG\u5982\u4f55\u5b9e\u73b0\u66f4\u53ef\u4fe1\u8d56\u7684\u79d1\u5b66\u95ee\u7b54\u3002"}}
{"id": "2510.15683", "categories": ["cs.IR", "cs.AI", "I.2.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.15683", "abs": "https://arxiv.org/abs/2510.15683", "authors": ["Effrosyni Sokli", "Pranav Kasela", "Georgios Peikos", "Gabriella Pasi"], "title": "Mixture of Experts Approaches in Dense Retrieval Tasks", "comment": "8 pages, 4 figures, 3 tables, reproducible code available at\n  https://github.com/FaySokli/SB-MoE , Accepted for publication in Proceedings\n  of the 2025 IEEE/WIC International Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT 2025)", "summary": "Dense Retrieval Models (DRMs) are a prominent development in Information\nRetrieval (IR). A key challenge with these neural Transformer-based models is\nthat they often struggle to generalize beyond the specific tasks and domains\nthey were trained on. To address this challenge, prior research in IR\nincorporated the Mixture-of-Experts (MoE) framework within each Transformer\nlayer of a DRM, which, though effective, substantially increased the number of\nadditional parameters. In this paper, we propose a more efficient design, which\nintroduces a single MoE block (SB-MoE) after the final Transformer layer. To\nassess the retrieval effectiveness of SB-MoE, we perform an empirical\nevaluation across three IR tasks. Our experiments involve two evaluation\nsetups, aiming to assess both in-domain effectiveness and the model's zero-shot\ngeneralizability. In the first setup, we fine-tune SB-MoE with four different\nunderlying DRMs on seven IR benchmarks and evaluate them on their respective\ntest sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform\nzero-shot evaluation on thirteen BEIR datasets. Additionally, we perform\nfurther experiments to analyze the model's dependency on its hyperparameters\n(i.e., the number of employed and activated experts) and investigate how this\nvariation affects SB-MoE's performance. The obtained results show that SB-MoE\nis particularly effective for DRMs with lightweight base models, such as\nTinyBERT and BERT-Small, consistently exceeding standard model fine-tuning\nacross benchmarks. For DRMs with more parameters, such as BERT-Base and\nContriever, our model requires a larger number of training samples to achieve\nimproved retrieval performance. Our code is available online at:\nhttps://github.com/FaySokli/SB-MoE.", "AI": {"tldr": "\u63d0\u51faSB-MoE\u65b9\u6cd5\uff0c\u5728DRM\u7684\u6700\u7ec8Transformer\u5c42\u540e\u6dfb\u52a0\u5355\u4e2aMoE\u5757\uff0c\u76f8\u6bd4\u4f20\u7edf\u6bcf\u5c42\u90fd\u52a0MoE\u7684\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u5728\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edfMoE\u65b9\u6cd5\u53c2\u6570\u8fc7\u591a\u7684\u95ee\u9898\u3002", "method": "\u5728DRM\u7684\u6700\u7ec8Transformer\u5c42\u540e\u5f15\u5165\u5355\u4e2aMoE\u5757\uff0c\u4f7f\u7528\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u9886\u57df\u5185\u5fae\u8c03\u548c\u96f6\u6837\u672c\u6cdb\u5316\u8bc4\u4f30\u3002", "result": "SB-MoE\u5728\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5728\u53c2\u6570\u8f83\u591a\u7684\u6a21\u578b\u4e0a\u9700\u8981\u66f4\u591a\u8bad\u7ec3\u6837\u672c\u624d\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "SB-MoE\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684DRM\u589e\u5f3a\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2510.15706", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.15706", "abs": "https://arxiv.org/abs/2510.15706", "authors": ["Italo Luis da Silva", "Hanqi Yan", "Lin Gui", "Yulan He"], "title": "GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery", "comment": "9 pages, 6 figures, 3 tables, EMNLP 2025 Demo paper", "summary": "Large Language Models (LLMs) show strong reasoning and text generation\ncapabilities, prompting their use in scientific literature analysis, including\nnovelty assessment. While evaluating novelty of scientific papers is crucial\nfor peer review, it requires extensive knowledge of related work, something not\nall reviewers have. While recent work on LLM-assisted scientific literature\nanalysis supports literature comparison, existing approaches offer limited\ntransparency and lack mechanisms for result traceability via an information\nretrieval module. To address this gap, we introduce $\\textbf{GraphMind}$, an\neasy-to-use interactive web tool designed to assist users in evaluating the\nnovelty of scientific papers or drafted ideas. Specially, $\\textbf{GraphMind}$\nenables users to capture the main structure of a scientific paper, explore\nrelated ideas through various perspectives, and assess novelty via providing\nverifiable contextual insights. $\\textbf{GraphMind}$ enables users to annotate\nkey elements of a paper, explore related papers through various relationships,\nand assess novelty with contextual insight. This tool integrates external APIs\nsuch as arXiv and Semantic Scholar with LLMs to support annotation, extraction,\nretrieval and classification of papers. This combination provides users with a\nrich, structured view of a scientific idea's core contributions and its\nconnections to existing work. $\\textbf{GraphMind}$ is available at\nhttps://oyarsa.github.io/graphmind and a demonstration video at\nhttps://youtu.be/wKbjQpSvwJg. The source code is available at\nhttps://github.com/oyarsa/graphmind.", "AI": {"tldr": "GraphMind\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u79d1\u5b66\u8bba\u6587\u7684\u65b0\u9896\u6027\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8API\u548cLLMs\u652f\u6301\u8bba\u6587\u7684\u6ce8\u91ca\u3001\u63d0\u53d6\u3001\u68c0\u7d22\u548c\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709LLM\u8f85\u52a9\u79d1\u5b66\u6587\u732e\u5206\u6790\u65b9\u6cd5\u900f\u660e\u5ea6\u6709\u9650\uff0c\u7f3a\u4e4f\u901a\u8fc7\u4fe1\u606f\u68c0\u7d22\u6a21\u5757\u5b9e\u73b0\u7ed3\u679c\u53ef\u8ffd\u6eaf\u6027\u7684\u673a\u5236\u3002", "method": "\u5f00\u53d1GraphMind\u5de5\u5177\uff0c\u96c6\u6210arXiv\u548cSemantic Scholar\u7b49\u5916\u90e8API\u4e0eLLMs\uff0c\u652f\u6301\u7528\u6237\u6ce8\u91ca\u8bba\u6587\u5173\u952e\u5143\u7d20\uff0c\u901a\u8fc7\u591a\u79cd\u5173\u7cfb\u63a2\u7d22\u76f8\u5173\u8bba\u6587\uff0c\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4e0a\u4e0b\u6587\u6d1e\u5bdf\u3002", "result": "GraphMind\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5de5\u5177\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u6355\u83b7\u79d1\u5b66\u8bba\u6587\u7684\u4e3b\u8981\u7ed3\u6784\uff0c\u4ece\u591a\u4e2a\u89d2\u5ea6\u63a2\u7d22\u76f8\u5173\u60f3\u6cd5\uff0c\u5e76\u901a\u8fc7\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4e0a\u4e0b\u6587\u6d1e\u5bdf\u6765\u8bc4\u4f30\u65b0\u9896\u6027\u3002", "conclusion": "GraphMind\u901a\u8fc7\u7ed3\u5408\u5916\u90e8API\u548cLLMs\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u79d1\u5b66\u8bba\u6587\u6838\u5fc3\u8d21\u732e\u53ca\u5176\u4e0e\u73b0\u6709\u5de5\u4f5c\u8054\u7cfb\u7684\u4e30\u5bcc\u7ed3\u6784\u5316\u89c6\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.15722", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15722", "abs": "https://arxiv.org/abs/2510.15722", "authors": ["Da Li", "Zecheng Fang", "Qiang Yan", "Wei Huang", "Xuanpu Luo"], "title": "The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation", "comment": "CCIR2025", "summary": "Retrieval-Augmented Generation has made significant progress in the field of\nnatural language processing. By combining the advantages of information\nretrieval and large language models, RAG can generate relevant and contextually\nappropriate responses based on items retrieved from reliable sources. This\ntechnology has demonstrated outstanding performance across multiple domains,\nbut its application in the legal field remains in its exploratory phase. In\nthis paper, we introduce our approach for \"Legal Knowledge Retrieval and\nGeneration\" in CCIR CUP 2025, which leverages large language models and\ninformation retrieval systems to provide responses based on laws in response to\nuser questions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728CCIR CUP 2025\u4e2d\u63d0\u51fa\u7684\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u4e0e\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6587\u56de\u7b54\u7528\u6237\u95ee\u9898\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u4ecd\u5904\u4e8e\u63a2\u7d22\u9636\u6bb5\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u4e0e\u751f\u6210\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\uff0c\u4ece\u53ef\u9760\u6765\u6e90\u68c0\u7d22\u6cd5\u5f8b\u6761\u6587\uff0c\u5e76\u57fa\u4e8e\u68c0\u7d22\u5185\u5bb9\u751f\u6210\u76f8\u5173\u4e14\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u56de\u7b54\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6587\u4e3a\u7528\u6237\u95ee\u9898\u63d0\u4f9b\u76f8\u5173\u56de\u7b54\uff0c\u5c55\u793a\u4e86\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6cd5\u5f8b\u77e5\u8bc6\u68c0\u7d22\u4e0e\u751f\u6210\u65b9\u6cd5\u4e3aRAG\u6280\u672f\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.15729", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.15729", "abs": "https://arxiv.org/abs/2510.15729", "authors": ["Chao Wang", "Yixin Song", "Jinhui Ye", "Chuan Qin", "Dazhong Shen", "Lingfeng Liu", "Xiang Wang", "Yanyong Zhang"], "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens", "comment": "Accepted by NeurIPS 2025", "summary": "Recently, large language models (LLMs) have been explored for integration\nwith collaborative filtering (CF)-based recommendation systems, which are\ncrucial for personalizing user experiences. However, a key challenge is that\nLLMs struggle to interpret the latent, non-semantic embeddings produced by CF\napproaches, limiting recommendation effectiveness and further applications. To\naddress this, we propose FACE, a general interpretable framework that maps CF\nembeddings into pre-trained LLM tokens. Specifically, we introduce a\ndisentangled projection module to decompose CF embeddings into concept-specific\nvectors, followed by a quantized autoencoder to convert continuous embeddings\ninto LLM tokens (descriptors). Then, we design a contrastive alignment\nobjective to ensure that the tokens align with corresponding textual signals.\nHence, the model-agnostic FACE framework achieves semantic alignment without\nfine-tuning LLMs and enhances recommendation performance by leveraging their\npre-trained capabilities. Empirical results on three real-world recommendation\ndatasets demonstrate performance improvements in benchmark models, with\ninterpretability studies confirming the interpretability of the descriptors.\nCode is available in https://github.com/YixinRoll/FACE.", "AI": {"tldr": "FACE\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u5c06\u534f\u540c\u8fc7\u6ee4\u5d4c\u5165\u6620\u5c04\u5230\u9884\u8bad\u7ec3LLM\u7684token\u4e2d\uff0c\u901a\u8fc7\u89e3\u8026\u6295\u5f71\u548c\u91cf\u5316\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u800c\u4e0d\u9700\u8981\u5fae\u8c03LLM\u3002", "motivation": "\u89e3\u51b3LLM\u96be\u4ee5\u89e3\u91caCF\u65b9\u6cd5\u4ea7\u751f\u7684\u6f5c\u5728\u975e\u8bed\u4e49\u5d4c\u5165\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u8350\u6548\u679c\u548c\u8fdb\u4e00\u6b65\u5e94\u7528\u3002", "method": "\u63d0\u51faFACE\u6846\u67b6\uff1a1\uff09\u89e3\u8026\u6295\u5f71\u6a21\u5757\u5206\u89e3CF\u5d4c\u5165\u4e3a\u6982\u5ff5\u7279\u5b9a\u5411\u91cf\uff1b2\uff09\u91cf\u5316\u81ea\u7f16\u7801\u5668\u5c06\u8fde\u7eed\u5d4c\u5165\u8f6c\u6362\u4e3aLLM token\uff1b3\uff09\u5bf9\u6bd4\u5bf9\u9f50\u76ee\u6807\u786e\u4fddtoken\u4e0e\u6587\u672c\u4fe1\u53f7\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u57fa\u51c6\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0c\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u8bc1\u5b9e\u4e86\u63cf\u8ff0\u7b26\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6a21\u578b\u65e0\u5173\u7684FACE\u6846\u67b6\u65e0\u9700\u5fae\u8c03LLM\u5373\u53ef\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u901a\u8fc7\u5229\u7528\u5176\u9884\u8bad\u7ec3\u80fd\u529b\u589e\u5f3a\u63a8\u8350\u6027\u80fd\u3002"}}

{"id": "2602.03992", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.03992", "abs": "https://arxiv.org/abs/2602.03992", "authors": ["Gabriel de Souza P. Moreira", "Ronay Ak", "Mengyao Xu", "Oliver Holworthy", "Benedikt Schifferer", "Zhiding Yu", "Yauhen Babakhin", "Radek Osmulski", "Jiarui Cai", "Ryan Chesler", "Bo Liu", "Even Oldridge"], "title": "Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems have been popular for generative applications, powering language models by injecting external knowledge. Companies have been trying to leverage their large catalog of documents (e.g. PDFs, presentation slides) in such RAG pipelines, whose first step is the retrieval component. Dense retrieval has been a popular approach, where embedding models are used to generate a dense representation of the user query that is closer to relevant content embeddings. More recently, VLM-based embedding models have become popular for visual document retrieval, as they preserve visual information and simplify the indexing pipeline compared to OCR text extraction.\n  Motivated by the growing demand for visual document retrieval, we introduce Nemotron ColEmbed V2, a family of models that achieve state-of-the-art performance on the ViDoRe benchmarks. We release three variants - with 3B, 4B, and 8B parameters - based on pre-trained VLMs: NVIDIA Eagle 2 with Llama 3.2 3B backbone, Qwen3-VL-4B-Instruct and Qwen3-VL-8B-Instruct, respectively. The 8B model ranks first on the ViDoRe V3 leaderboard as of February 03, 2026, achieving an average NDCG@10 of 63.42.\n  We describe the main techniques used across data processing, training, and post-training - such as cluster-based sampling, hard-negative mining, bidirectional attention, late interaction, and model merging - that helped us build our top-performing models. We also discuss compute and storage engineering challenges posed by the late interaction mechanism and present experiments on how to balance accuracy and storage with lower dimension embeddings.", "AI": {"tldr": "Nemotron ColEmbed V2\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u7684VLM\u5d4c\u5165\u6a21\u578b\u5bb6\u65cf\uff0c\u5728ViDoRe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c8B\u6a21\u578b\u5728ViDoRe V3\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u66f4\u597d\u7684VLM\u5d4c\u5165\u6a21\u578b\u6765\u66ff\u4ee3\u4f20\u7edf\u7684OCR\u6587\u672c\u63d0\u53d6\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u7559\u89c6\u89c9\u4fe1\u606f\u5e76\u7b80\u5316\u7d22\u5f15\u6d41\u7a0b\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684VLMs\u6784\u5efa\u4e86\u4e09\u4e2a\u53d8\u4f53\uff083B\u30014B\u30018B\u53c2\u6570\uff09\uff0c\u91c7\u7528\u4e86\u96c6\u7fa4\u91c7\u6837\u3001\u56f0\u96be\u8d1f\u6837\u672c\u6316\u6398\u3001\u53cc\u5411\u6ce8\u610f\u529b\u3001\u5ef6\u8fdf\u4ea4\u4e92\u548c\u6a21\u578b\u878d\u5408\u7b49\u6280\u672f\u3002", "result": "8B\u6a21\u578b\u5728ViDoRe V3\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\uff0c\u5e73\u5747NDCG@10\u8fbe\u523063.42\uff0c\u5c55\u793a\u4e86\u5728\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "Nemotron ColEmbed V2\u7cfb\u5217\u6a21\u578b\u4e3a\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u79cd\u6280\u672f\u4f18\u5316\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5ef6\u8fdf\u4ea4\u4e92\u673a\u5236\u5e26\u6765\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5de5\u7a0b\u6311\u6218\u3002"}}
{"id": "2602.04225", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04225", "abs": "https://arxiv.org/abs/2602.04225", "authors": ["Yinan Zhang", "Zhixi Chen", "Jiazheng Jing", "Zhiqi Shen"], "title": "Following the TRAIL: Predicting and Explaining Tomorrow's Hits with a Fine-Tuned LLM", "comment": null, "summary": "Large Language Models (LLMs) have been widely applied across multiple domains for their broad knowledge and strong reasoning capabilities. However, applying them to recommendation systems is challenging since it is hard for LLMs to extract user preferences from large, sparse user-item logs, and real-time per-user ranking over the full catalog is too time-consuming to be practical. Moreover, many existing recommender systems focus solely on ranking items while overlooking explanations, which could help improve predictive accuracy and make recommendations more convincing to users. Inspired by recent works that achieve strong recommendation performance by forecasting near-term item popularity, we propose TRAIL (TRend and explAnation Integrated Learner). TRAIL is a fine-tuned LLM that jointly predicts short-term item popularity and generates faithful natural-language explanations. It employs contrastive learning with positive and negative pairs to align its scores and explanations with structured trend signals, yielding accurate and explainable popularity predictions. Extensive experiments show that TRAIL outperforms strong baselines and produces coherent, well-grounded explanations.", "AI": {"tldr": "TRAIL\u662f\u4e00\u4e2a\u5fae\u8c03\u7684LLM\uff0c\u8054\u5408\u9884\u6d4b\u77ed\u671f\u7269\u54c1\u6d41\u884c\u5ea6\u5e76\u751f\u6210\u5fe0\u5b9e\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u5206\u6570\u548c\u89e3\u91ca\u4e0e\u7ed3\u6784\u5316\u8d8b\u52bf\u4fe1\u53f7\u3002", "motivation": "LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e94\u7528\u56f0\u96be\uff1a\u96be\u4ee5\u4ece\u5927\u89c4\u6a21\u7a00\u758f\u7528\u6237-\u7269\u54c1\u65e5\u5fd7\u4e2d\u63d0\u53d6\u7528\u6237\u504f\u597d\uff0c\u5b9e\u65f6\u5168\u76ee\u5f55\u6392\u5e8f\u8017\u65f6\uff1b\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u591a\u53ea\u5173\u6ce8\u6392\u5e8f\u800c\u5ffd\u89c6\u89e3\u91ca\uff0c\u89e3\u91ca\u53ef\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u7528\u6237\u53ef\u4fe1\u5ea6\u3002", "method": "TRAIL\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f7f\u7528\u6b63\u8d1f\u6837\u672c\u5bf9\uff0c\u5c06\u6a21\u578b\u5206\u6570\u548c\u89e3\u91ca\u4e0e\u7ed3\u6784\u5316\u8d8b\u52bf\u4fe1\u53f7\u5bf9\u9f50\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u6d41\u884c\u5ea6\u9884\u6d4b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eTRAIL\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u80fd\u751f\u6210\u8fde\u8d2f\u4e14\u6709\u5145\u5206\u4f9d\u636e\u7684\u89e3\u91ca\u3002", "conclusion": "TRAIL\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u77ed\u671f\u7269\u54c1\u6d41\u884c\u5ea6\u548c\u751f\u6210\u5fe0\u5b9e\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u3002"}}
{"id": "2602.04263", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04263", "abs": "https://arxiv.org/abs/2602.04263", "authors": ["Joohyung Yun", "Doyup Lee", "Wook-Shin Han"], "title": "LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval", "comment": "Project page: https://lilac-emnlp2025.github.io/", "summary": "Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. An effective multimodal retriever needs to handle two main challenges: (1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and (2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. To address these challenges, we propose LILaC, a multimodal retrieval framework featuring two core innovations. First, we introduce a layered component graph, explicitly representing multimodal information at two layers - each representing coarse and fine granularity - facilitating efficient yet precise reasoning. Second, we develop a late-interaction-based subgraph retrieval method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction. Extensive experiments demonstrate that LILaC achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning. We make the artifacts publicly available at github.com/joohyung00/lilac.", "AI": {"tldr": "LILaC\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u7ec4\u4ef6\u56fe\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u5b50\u56fe\u68c0\u7d22\u65b9\u6cd5\uff0c\u89e3\u51b3\u56fa\u5b9a\u7c92\u5ea6\u68c0\u7d22\u5355\u5143\u548c\u591a\u8df3\u63a8\u7406\u95ee\u9898\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u56fa\u5b9a\u5355\u4e00\u7c92\u5ea6\u68c0\u7d22\u5355\u5143\u5bfc\u81f4\u7684\u65e0\u5173\u5185\u5bb9\u5e72\u6270\uff1b2) \u652f\u6301\u591a\u8df3\u63a8\u7406\u9700\u8981\u6709\u6548\u6355\u6349\u6587\u6863\u5185\u548c\u8de8\u6587\u6863\u7ec4\u4ef6\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "method": "\u63d0\u51faLILaC\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u5206\u5c42\u7ec4\u4ef6\u56fe\uff0c\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u4e24\u4e2a\u5c42\u6b21\u663e\u5f0f\u8868\u793a\u591a\u6a21\u6001\u4fe1\u606f\uff1b2) \u57fa\u4e8e\u665a\u4ea4\u4e92\u7684\u5b50\u56fe\u68c0\u7d22\u65b9\u6cd5\uff0c\u5148\u8bc6\u522b\u7c97\u7c92\u5ea6\u8282\u70b9\u8fdb\u884c\u9ad8\u6548\u5019\u9009\u751f\u6210\uff0c\u518d\u901a\u8fc7\u665a\u4ea4\u4e92\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLILaC\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "conclusion": "LILaC\u901a\u8fc7\u5206\u5c42\u8868\u793a\u548c\u57fa\u4e8e\u8fb9\u7f18\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u4e2d\u7684\u7c92\u5ea6\u95ee\u9898\u548c\u591a\u8df3\u63a8\u7406\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2602.04278", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04278", "abs": "https://arxiv.org/abs/2602.04278", "authors": ["Lin Wang", "Yang Zhang", "Jingfan Chen", "Xiaoyan Zhao", "Fengbin Zhu", "Qing Li", "Tat-Seng Chua"], "title": "MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation", "comment": null, "summary": "The integration of reinforcement learning (RL) into large language models (LLMs) has opened new opportunities for recommender systems by eliciting reasoning and improving user preference modeling. However, RL-based LLM recommendation faces significant efficiency challenges, making full-data training costly. Existing data selection methods define sample value based on learnability or representativeness, yet their loss- or gradient-driven or dataset coverage-driven criteria often misalign with RL learning dynamics, resulting in suboptimal performance. To address this, we propose MiniRec, a data selection framework tailored for RL-based LLM recommendation. MiniRec evaluates sample learnability using key RL signals -- rewards -- pruning samples that are too easy (too high reward) or too difficult (consistently low reward). It assesses representativeness by aligning sample gradients with the approximated \"ideal\" global RL optimization trajectory, selecting samples that mainly drive model updates, and it also enforces diversity to reduce redundancy. Combined with a curriculum learning strategy from easy to hard samples, MiniRec significantly reduces training cost while largely preserving performance. Extensive experiments demonstrate MiniRec's effectiveness, highlighting the importance of reward-aligned, trajectory-informed data selection in RL-based LLM recommendation.", "AI": {"tldr": "MiniRec\uff1a\u9488\u5bf9\u57fa\u4e8eRL\u7684LLM\u63a8\u8350\u7cfb\u7edf\u7684\u9ad8\u6548\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u5bf9\u9f50\u548c\u8f68\u8ff9\u611f\u77e5\u7684\u6837\u672c\u9009\u62e9\uff0c\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLM\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08\u57fa\u4e8e\u53ef\u5b66\u4e60\u6027\u6216\u4ee3\u8868\u6027\uff09\u4e0eRL\u5b66\u4e60\u52a8\u6001\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73", "method": "\u63d0\u51faMiniRec\u6846\u67b6\uff1a1) \u4f7f\u7528\u5956\u52b1\u4fe1\u53f7\u8bc4\u4f30\u6837\u672c\u53ef\u5b66\u4e60\u6027\uff0c\u4fee\u526a\u592a\u7b80\u5355\uff08\u5956\u52b1\u8fc7\u9ad8\uff09\u6216\u592a\u96be\uff08\u5956\u52b1\u6301\u7eed\u4f4e\uff09\u7684\u6837\u672c\uff1b2) \u901a\u8fc7\u6837\u672c\u68af\u5ea6\u4e0e\u8fd1\u4f3c\"\u7406\u60f3\"\u5168\u5c40RL\u4f18\u5316\u8f68\u8ff9\u7684\u5bf9\u9f50\u8bc4\u4f30\u4ee3\u8868\u6027\uff1b3) \u5f3a\u5236\u591a\u6837\u6027\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b4) \u7ed3\u5408\u4ece\u6613\u5230\u96be\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eMiniRec\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u6027\u80fd\uff0c\u51f8\u663e\u4e86\u5956\u52b1\u5bf9\u9f50\u3001\u8f68\u8ff9\u611f\u77e5\u6570\u636e\u9009\u62e9\u5728\u57fa\u4e8eRL\u7684LLM\u63a8\u8350\u4e2d\u7684\u91cd\u8981\u6027", "conclusion": "MiniRec\u4e3a\u57fa\u4e8eRL\u7684LLM\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u5bf9\u9f50\u548c\u4f18\u5316\u8f68\u8ff9\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4e0eRL\u5b66\u4e60\u52a8\u6001\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u6210\u672c\u7684\u5927\u5e45\u964d\u4f4e"}}
{"id": "2602.04451", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04451", "abs": "https://arxiv.org/abs/2602.04451", "authors": ["Yi Sun", "Jinyu Xu", "Qing Xie", "Jiachen Li", "Yanchun Ma", "Yongjian Liu"], "title": "SDR-CIR: Semantic Debias Retrieval Framework for Training-Free Zero-Shot Composed Image Retrieval", "comment": null, "summary": "Composed Image Retrieval (CIR) aims to retrieve a target image from a query composed of a reference image and modification text. Recent training-free zero-shot methods often employ Multimodal Large Language Models (MLLMs) with Chain-of-Thought (CoT) to compose a target image description for retrieval. However, due to the fuzzy matching nature of ZS-CIR, the generated description is prone to semantic bias relative to the target image. We propose SDR-CIR, a training-free Semantic Debias Ranking method based on CoT reasoning. First, Selective CoT guides the MLLM to extract visual content relevant to the modification text during image understanding, thereby reducing visual noise at the source. We then introduce a Semantic Debias Ranking with two steps, Anchor and Debias, to mitigate semantic bias. In the Anchor step, we fuse reference image features with target description features to reinforce useful semantics and supplement omitted cues. In the Debias step, we explicitly model the visual semantic contribution of the reference image to the description and incorporate it into the similarity score as a penalty term. By supplementing omitted cues while suppressing redundancy, SDR-CIR mitigates semantic bias and improves retrieval performance. Experiments on three standard CIR benchmarks show that SDR-CIR achieves state-of-the-art results among one-stage methods while maintaining high efficiency. The code is publicly available at https://github.com/suny105/SDR-CIR.", "AI": {"tldr": "SDR-CIR\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u8bed\u4e49\u53bb\u504f\u6392\u5e8f\u7684\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u601d\u7ef4\u94fe\u548c\u4e24\u6b65\u53bb\u504f\u7b56\u7565\u51cf\u5c11\u8bed\u4e49\u504f\u5dee\uff0c\u5728\u4e09\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e00\u6d41\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76ee\u6807\u56fe\u50cf\u63cf\u8ff0\u8fdb\u884c\u68c0\u7d22\uff0c\u4f46\u7531\u4e8e\u6a21\u7cca\u5339\u914d\u7279\u6027\uff0c\u751f\u6210\u7684\u63cf\u8ff0\u5bb9\u6613\u4ea7\u751f\u8bed\u4e49\u504f\u5dee\uff0c\u5f71\u54cd\u68c0\u7d22\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSDR-CIR\u65b9\u6cd5\uff1a1\uff09\u9009\u62e9\u6027\u601d\u7ef4\u94fe\u5f15\u5bfcMLLM\u63d0\u53d6\u4e0e\u4fee\u6539\u6587\u672c\u76f8\u5173\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u51cf\u5c11\u6e90\u5934\u7684\u89c6\u89c9\u566a\u58f0\uff1b2\uff09\u8bed\u4e49\u53bb\u504f\u6392\u5e8f\u5305\u62ec\u951a\u5b9a\u6b65\u9aa4\uff08\u878d\u5408\u53c2\u8003\u56fe\u50cf\u7279\u5f81\u548c\u76ee\u6807\u63cf\u8ff0\u7279\u5f81\uff09\u548c\u53bb\u504f\u6b65\u9aa4\uff08\u5efa\u6a21\u53c2\u8003\u56fe\u50cf\u5bf9\u63cf\u8ff0\u7684\u89c6\u89c9\u8bed\u4e49\u8d21\u732e\u4f5c\u4e3a\u60e9\u7f5a\u9879\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6CIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSDR-CIR\u5728\u5355\u9636\u6bb5\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7387\u3002", "conclusion": "SDR-CIR\u901a\u8fc7\u8865\u5145\u88ab\u5ffd\u7565\u7684\u7ebf\u7d22\u540c\u65f6\u6291\u5236\u5197\u4f59\u4fe1\u606f\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u8bed\u4e49\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.04460", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04460", "abs": "https://arxiv.org/abs/2602.04460", "authors": ["Junwei Yin", "Senjie Kou", "Changhao Li", "Shuli Wang", "Xue Wei", "Yinqiu Huang", "Yinhua Zhu", "Haitao Wang", "Xingxing Wang"], "title": "DOS: Dual-Flow Orthogonal Semantic IDs for Recommendation in Meituan", "comment": "Accepted by WWW2026 (short paper)", "summary": "Semantic IDs serve as a key component in generative recommendation systems. They not only incorporate open-world knowledge from large language models (LLMs) but also compress the semantic space to reduce generation difficulty. However, existing methods suffer from two major limitations: (1) the lack of contextual awareness in generation tasks leads to a gap between the Semantic ID codebook space and the generation space, resulting in suboptimal recommendations; and (2) suboptimal quantization methods exacerbate semantic loss in LLMs. To address these issues, we propose Dual-Flow Orthogonal Semantic IDs (DOS) method. Specifically, DOS employs a user-item dual flow-framework that leverages collaborative signals to align the Semantic ID codebook space with the generation space. Furthermore, we introduce an orthogonal residual quantization scheme that rotates the semantic space to an appropriate orientation, thereby maximizing semantic preservation. Extensive offline experiments and online A/B testing demonstrate the effectiveness of DOS. The proposed method has been successfully deployed in Meituan's mobile application, serving hundreds of millions of users.", "AI": {"tldr": "\u63d0\u51faDOS\u65b9\u6cd5\u89e3\u51b3\u8bed\u4e49ID\u5728\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0d\u8db3\u5bfc\u81f4\u8bed\u4e49ID\u7801\u672c\u7a7a\u95f4\u4e0e\u751f\u6210\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u4ee5\u53ca\u6b21\u4f18\u91cf\u5316\u65b9\u6cd5\u52a0\u5267LLM\u8bed\u4e49\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49ID\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1) \u751f\u6210\u4efb\u52a1\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u8bed\u4e49ID\u7801\u672c\u7a7a\u95f4\u4e0e\u751f\u6210\u7a7a\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u4ea7\u751f\u6b21\u4f18\u63a8\u8350\uff1b2) \u6b21\u4f18\u7684\u91cf\u5316\u65b9\u6cd5\u52a0\u5267\u4e86LLM\u7684\u8bed\u4e49\u635f\u5931\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u6b63\u4ea4\u8bed\u4e49ID\uff08DOS\uff09\u65b9\u6cd5\uff1a1) \u91c7\u7528\u7528\u6237-\u7269\u54c1\u53cc\u6d41\u6846\u67b6\uff0c\u5229\u7528\u534f\u540c\u4fe1\u53f7\u5bf9\u9f50\u8bed\u4e49ID\u7801\u672c\u7a7a\u95f4\u4e0e\u751f\u6210\u7a7a\u95f4\uff1b2) \u5f15\u5165\u6b63\u4ea4\u6b8b\u5dee\u91cf\u5316\u65b9\u6848\uff0c\u5c06\u8bed\u4e49\u7a7a\u95f4\u65cb\u8f6c\u5230\u9002\u5f53\u65b9\u5411\u4ee5\u6700\u5927\u5316\u8bed\u4e49\u4fdd\u7559\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u8bc1\u660e\u4e86DOS\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5df2\u5728\u7f8e\u56e2\u79fb\u52a8\u5e94\u7528\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u4e3a\u6570\u4ebf\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u3002", "conclusion": "DOS\u65b9\u6cd5\u901a\u8fc7\u53cc\u6d41\u6846\u67b6\u548c\u6b63\u4ea4\u91cf\u5316\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49ID\u5728\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u7a7a\u95f4\u4e0e\u751f\u6210\u7a7a\u95f4\u7684\u5bf9\u9f50\u4ee5\u53ca\u8bed\u4e49\u635f\u5931\u7684\u6700\u5c0f\u5316\u3002"}}
{"id": "2602.04567", "categories": ["cs.IR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.04567", "abs": "https://arxiv.org/abs/2602.04567", "authors": ["Aleksandr Poslavsky", "Alexander D'yakonov", "Yuriy Dorn", "Andrey Zimovnov"], "title": "VK-LSVD: A Large-Scale Industrial Dataset for Short-Video Recommendation", "comment": "Accepted to The ACM Web Conference 2026 (WWW '26). Preprint of conference paper. 7 pages, 2 (7) figures, 4 tables. Dataset available at: https://huggingface.co/datasets/deepvk/VK-LSVD", "summary": "Short-video recommendation presents unique challenges, such as modeling rapid user interest shifts from implicit feedback, but progress is constrained by a lack of large-scale open datasets that reflect real-world platform dynamics. To bridge this gap, we introduce the VK Large Short-Video Dataset (VK-LSVD), the largest publicly available industrial dataset of its kind. VK-LSVD offers an unprecedented scale of over 40 billion interactions from 10 million users and almost 20 million videos over six months, alongside rich features including content embeddings, diverse feedback signals, and contextual metadata. Our analysis supports the dataset's quality and diversity. The dataset's immediate impact is confirmed by its central role in the live VK RecSys Challenge 2025. VK-LSVD provides a vital, open dataset to use in building realistic benchmarks to accelerate research in sequential recommendation, cold-start scenarios, and next-generation recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86VK-LSVD\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00\u77ed\u89c6\u9891\u63a8\u8350\u5de5\u4e1a\u6570\u636e\u96c6\uff0c\u5305\u542b400\u4ebf\u6b21\u4ea4\u4e92\u30011000\u4e07\u7528\u6237\u548c2000\u4e07\u89c6\u9891\uff0c\u65e8\u5728\u89e3\u51b3\u8be5\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u77ed\u89c6\u9891\u63a8\u8350\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u4ece\u9690\u5f0f\u53cd\u9988\u4e2d\u5efa\u6a21\u5feb\u901f\u53d8\u5316\u7684\u7528\u6237\u5174\u8da3\uff0c\u4f46\u8be5\u9886\u57df\u8fdb\u5c55\u53d7\u5230\u7f3a\u4e4f\u53cd\u6620\u771f\u5b9e\u5e73\u53f0\u52a8\u6001\u7684\u5927\u89c4\u6a21\u5f00\u653e\u6570\u636e\u96c6\u7684\u9650\u5236\u3002", "method": "\u901a\u8fc7\u6536\u96c6VK\u5e73\u53f0\u7684\u5b9e\u9645\u6570\u636e\u6784\u5efaVK-LSVD\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7400\u4ebf\u6b21\u4ea4\u4e92\u30011000\u4e07\u7528\u6237\u548c\u8fd12000\u4e07\u89c6\u9891\u76846\u4e2a\u6708\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e30\u5bcc\u7279\u5f81\u5305\u62ec\u5185\u5bb9\u5d4c\u5165\u3001\u591a\u6837\u5316\u53cd\u9988\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u5143\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5206\u6790\u8bc1\u5b9e\u4e86\u5176\u8d28\u91cf\u548c\u591a\u6837\u6027\uff0c\u5e76\u5df2\u7acb\u5373\u5e94\u7528\u4e8eVK RecSys Challenge 2025\u7ade\u8d5b\u4e2d\uff0c\u6210\u4e3a\u8be5\u9886\u57df\u7684\u6838\u5fc3\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "VK-LSVD\u4e3a\u6784\u5efa\u771f\u5b9e\u57fa\u51c6\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5c06\u52a0\u901f\u5e8f\u5217\u63a8\u8350\u3001\u51b7\u542f\u52a8\u573a\u666f\u548c\u4e0b\u4e00\u4ee3\u63a8\u8350\u7cfb\u7edf\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2602.04579", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04579", "abs": "https://arxiv.org/abs/2602.04579", "authors": ["Sameh Khattab", "Marie Bauer", "Lukas Heine", "Till Rostalski", "Jens Kleesiek", "Julian Friedrich"], "title": "AIANO: Enhancing Information Retrieval with AI-Augmented Annotation", "comment": null, "summary": "The rise of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) has rapidly increased the need for high-quality, curated information retrieval datasets. These datasets, however, are currently created with off-the-shelf annotation tools that make the annotation process complex and inefficient. To streamline this process, we developed a specialized annotation tool - AIANO. By adopting an AI-augmented annotation workflow that tightly integrates human expertise with LLM assistance, AIANO enables annotators to leverage AI suggestions while retaining full control over annotation decisions. In a within-subject user study ($n = 15$), participants created question-answering datasets using both a baseline tool and AIANO. AIANO nearly doubled annotation speed compared to the baseline while being easier to use and improving retrieval accuracy. These results demonstrate that AIANO's AI-augmented approach accelerates and enhances dataset creation for information retrieval tasks, advancing annotation capabilities in retrieval-intensive domains.", "AI": {"tldr": "AIANO\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\u6570\u636e\u96c6\u6807\u6ce8\u7684\u5de5\u5177\uff0c\u91c7\u7528AI\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0eLLM\u8f85\u52a9\u7d27\u5bc6\u7ed3\u5408\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5de5\u5177\u5c06\u6807\u6ce8\u901f\u5ea6\u63d0\u9ad8\u8fd1\u4e00\u500d\u3002", "motivation": "\u968f\u7740LLM\u548cRAG\u7684\u5174\u8d77\uff0c\u5bf9\u9ad8\u8d28\u91cf\u3001\u7cbe\u5fc3\u7b56\u5212\u7684\u4fe1\u606f\u68c0\u7d22\u6570\u636e\u96c6\u7684\u9700\u6c42\u8fc5\u901f\u589e\u957f\u3002\u7136\u800c\uff0c\u76ee\u524d\u8fd9\u4e9b\u6570\u636e\u96c6\u4f7f\u7528\u73b0\u6210\u7684\u6807\u6ce8\u5de5\u5177\u521b\u5efa\uff0c\u4f7f\u5f97\u6807\u6ce8\u8fc7\u7a0b\u590d\u6742\u4e14\u4f4e\u6548\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u6807\u6ce8\u5de5\u5177AIANO\uff0c\u91c7\u7528AI\u589e\u5f3a\u7684\u6807\u6ce8\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7d27\u5bc6\u96c6\u6210\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u4e0eLLM\u8f85\u52a9\uff0c\u4f7f\u6807\u6ce8\u8005\u80fd\u591f\u5229\u7528AI\u5efa\u8bae\u540c\u65f6\u4fdd\u6301\u5bf9\u6807\u6ce8\u51b3\u7b56\u7684\u5b8c\u5168\u63a7\u5236\u3002", "result": "\u572815\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u4e2d\uff0cAIANO\u76f8\u6bd4\u57fa\u7ebf\u5de5\u5177\u5c06\u6807\u6ce8\u901f\u5ea6\u63d0\u9ad8\u8fd1\u4e00\u500d\uff0c\u540c\u65f6\u66f4\u6613\u4e8e\u4f7f\u7528\u5e76\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u3002", "conclusion": "AIANO\u7684AI\u589e\u5f3a\u65b9\u6cd5\u52a0\u901f\u5e76\u589e\u5f3a\u4e86\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u7684\u6570\u636e\u96c6\u521b\u5efa\uff0c\u63a8\u8fdb\u4e86\u68c0\u7d22\u5bc6\u96c6\u578b\u9886\u57df\u7684\u6807\u6ce8\u80fd\u529b\u3002"}}
{"id": "2602.04690", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.04690", "abs": "https://arxiv.org/abs/2602.04690", "authors": ["Junjie Chen", "Haitao Li", "Qilei Zhang", "Zhenghua Li", "Ya Zhang", "Quan Zhou", "Cheng Luo", "Yiqun Liu", "Dongsheng Guo", "Qingyao Ai"], "title": "Multi-Source Retrieval and Reasoning for Legal Sentencing Prediction", "comment": null, "summary": "Legal judgment prediction (LJP) aims to predict judicial outcomes from case facts and typically includes law article, charge, and sentencing prediction. While recent methods perform well on the first two subtasks, legal sentencing prediction (LSP) remains difficult due to its need for fine-grained objective knowledge and flexible subjective reasoning. To address these limitations, we propose $MSR^2$, a framework that integrates multi-source retrieval and reasoning in LLMs with reinforcement learning. $MSR^2$ enables LLMs to perform multi-source retrieval based on reasoning needs and applies a process-level reward to guide intermediate subjective reasoning steps. Experiments on two real-world datasets show that $MSR^2$ improves both accuracy and interpretability in LSP, providing a promising step toward practical legal AI. Our code is available at https://anonymous.4open.science/r/MSR2-FC3B.", "AI": {"tldr": "MSR\u00b2\u6846\u67b6\u901a\u8fc7\u591a\u6e90\u68c0\u7d22\u4e0e\u5f3a\u5316\u5b66\u4e60\u589e\u5f3aLLMs\u5728\u6cd5\u5f8b\u91cf\u5211\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u6cd5\u5f8b\u91cf\u5211\u9884\u6d4b\uff08LSP\uff09\u76f8\u6bd4\u6cd5\u5f8b\u6761\u6587\u548c\u7f6a\u540d\u9884\u6d4b\u66f4\u4e3a\u56f0\u96be\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u5ba2\u89c2\u77e5\u8bc6\u548c\u7075\u6d3b\u7684\u4e3b\u89c2\u63a8\u7406\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73", "method": "\u63d0\u51faMSR\u00b2\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u6e90\u68c0\u7d22\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9LLMs\u6839\u636e\u63a8\u7406\u9700\u6c42\u8fdb\u884c\u591a\u6e90\u68c0\u7d22\uff0c\u5e76\u5e94\u7528\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6765\u6307\u5bfc\u4e2d\u95f4\u4e3b\u89c2\u63a8\u7406\u6b65\u9aa4", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMSR\u00b2\u5728\u6cd5\u5f8b\u91cf\u5211\u9884\u6d4b\u4e2d\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "MSR\u00b2\u4e3a\u5b9e\u7528\u6cd5\u5f8bAI\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u4e00\u6b65\uff0c\u901a\u8fc7\u6574\u5408\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u5e94\u5bf9\u6cd5\u5f8b\u91cf\u5211\u9884\u6d4b\u7684\u6311\u6218"}}
{"id": "2602.04711", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.04711", "abs": "https://arxiv.org/abs/2602.04711", "authors": ["Sagie Dekel", "Moshe Tennenholtz", "Oren Kurland"], "title": "Addressing Corpus Knowledge Poisoning Attacks on RAG Using Sparse Attention", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is a highly effective paradigm for keeping LLM-based responses up-to-date and reducing the likelihood of hallucinations. Yet, RAG was recently shown to be quite vulnerable to corpus knowledge poisoning: an attacker injects misleading documents to the corpus to steer an LLMs' output to an undesired response. We argue that the standard causal attention mechanism in LLMs enables harmful cross-document interactions, specifically in cases of attacks. Accordingly, we introduce a novel defense approach for RAG: Sparse Document Attention RAG (SDAG). This is a block-sparse attention mechanism that disallows cross-attention between retrieved documents. SDAG requires a minimal inference-time change to the attention mask; furthermore, no fine-tuning or additional architectural changes are needed. We present an empirical evaluation of LLM-based question answering (QA) with a variety of attack strategies on RAG. We show that our SDAG method substantially outperforms the standard causal attention mechanism in terms of attack success rate. We further demonstrate the clear merits of integrating SDAG with state-of-the-art RAG defense methods. Specifically, the integration results in performance that is statistically significantly better than the state-of-the-art.", "AI": {"tldr": "SDAG\u63d0\u51fa\u4e00\u79cd\u7a00\u758f\u6587\u6863\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u963b\u6b62\u68c0\u7d22\u6587\u6863\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u9632\u5fa1RAG\u4e2d\u7684\u77e5\u8bc6\u5e93\u4e2d\u6bd2\u653b\u51fb\uff0c\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u4e14\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "RAG\u867d\u7136\u80fd\u4fdd\u6301LLM\u54cd\u5e94\u65f6\u6548\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u4f46\u6613\u53d7\u77e5\u8bc6\u5e93\u4e2d\u6bd2\u653b\u51fb\uff08\u653b\u51fb\u8005\u6ce8\u5165\u8bef\u5bfc\u6587\u6863\u64cd\u63a7LLM\u8f93\u51fa\uff09\u3002\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u5728\u653b\u51fb\u573a\u666f\u4e0b\u4f1a\u5bfc\u81f4\u6709\u5bb3\u7684\u8de8\u6587\u6863\u4ea4\u4e92\u3002", "method": "\u63d0\u51faSDAG\uff08\u7a00\u758f\u6587\u6863\u6ce8\u610f\u529bRAG\uff09\uff0c\u91c7\u7528\u5757\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7981\u6b62\u68c0\u7d22\u6587\u6863\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u4ec5\u9700\u5728\u63a8\u7406\u65f6\u4fee\u6539\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u67b6\u6784\u6539\u52a8\u3002", "result": "\u5728\u591a\u79cd\u653b\u51fb\u7b56\u7565\u4e0b\u7684LLM\u95ee\u7b54\u8bc4\u4f30\u4e2d\uff0cSDAG\u5728\u653b\u51fb\u6210\u529f\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u3002\u4e0e\u73b0\u6709\u6700\u4f73RAG\u9632\u5fa1\u65b9\u6cd5\u7ed3\u5408\u540e\uff0c\u6027\u80fd\u5728\u7edf\u8ba1\u4e0a\u663e\u8457\u66f4\u4f18\u3002", "conclusion": "SDAG\u901a\u8fc7\u7b80\u5355\u7684\u6ce8\u610f\u529b\u63a9\u7801\u4fee\u6539\u6709\u6548\u9632\u5fa1RAG\u77e5\u8bc6\u5e93\u4e2d\u6bd2\u653b\u51fb\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\uff0c\u4e14\u4e0e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u517c\u5bb9\uff0c\u80fd\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}

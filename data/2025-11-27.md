<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [E-GEO: A Testbed for Generative Engine Optimization in E-Commerce](https://arxiv.org/abs/2511.20867)
*Puneet S. Bagga,Vivek F. Farias,Tamar Korkotashvili,Tianyi Peng,Yuhang Wu*

Main category: cs.IR

TL;DR: 提出了E-GEO基准，这是首个专门为电商生成引擎优化设计的基准，包含7000多个真实的多句消费者产品查询，并开发了轻量级迭代提示优化算法来提升内容在生成引擎中的可见性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，生成引擎正在重塑检索任务，但在电商领域，生成引擎优化实践仍缺乏系统研究，其影响尚未被充分理解。

Method: 构建E-GEO基准数据集，评估15种常见改写启发式方法，并制定GEO为可优化问题，开发轻量级迭代提示优化算法。

Result: 优化的提示揭示了一个稳定、领域无关的模式，表明存在'普遍有效'的GEO策略，该算法显著优于基线方法。

Conclusion: E-GEO为电商生成引擎优化提供了首个系统基准，揭示了普遍有效的优化模式，推动了该领域的研究发展。

Abstract: With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a "universally effective" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.

</details>


### [2] [Generating Querying Code from Text for Multi-Modal Electronic Health Record](https://arxiv.org/abs/2511.20904)
*Mengliang ZHang*

Main category: cs.IR

TL;DR: 构建了TQGen数据集和TQGen-EHRQuery框架，用于电子健康记录中的自然语言到查询生成，通过医疗知识模块和问题模板匹配模块解决复杂医学术语和多样化查询的挑战。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含大量结构化表格和非结构化临床文本，查询相关信息需要复杂的数据库操作，增加了临床医生的工作负担。复杂的表关系和专业术语限制了查询准确性。

Method: 提出TQGen-EHRQuery框架，包含医疗知识模块和问题模板匹配模块。引入工具集概念，将文本处理模块封装为可调用工具，提高处理效率和灵活性。

Result: 通过广泛实验验证了数据集和工作流程的有效性，展示了其在增强EHR系统信息查询方面的潜力。

Conclusion: TQGen数据集和TQGen-EHRQuery框架能够有效提升电子健康记录系统的信息查询能力，解决了复杂医学术语和多样化查询的挑战。

Abstract: Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \textbf{T}ables and clinical \textbf{T}ext for natural language-to-query \textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.

</details>


### [3] [Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval](https://arxiv.org/abs/2511.21121)
*Anup Roy,Rishabh Gyanendra Upadhyay,Animesh Rameshbhai Panara,Robin Mills*

Main category: cs.IR

TL;DR: VisionRAG是一个免OCR、模型无关的多模态检索系统，通过金字塔索引框架直接处理文档图像，保留布局和空间线索，显著降低内存开销并提高检索效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于OCR的文档检索流程维护成本高、对布局变化敏感且容易丢失空间线索，而现有的视觉优先检索方法存在内存开销大和模型依赖性强的问题。

Method: 采用三通道金字塔索引框架，直接对文档图像进行索引，使用全局页面摘要、章节标题、视觉热点和事实级线索构建语义向量，通过互逆排序融合进行检索。

Result: 在金融文档基准测试中，FinanceBench上达到0.8051准确率，TAT DQA上达到0.9629召回率，每页仅存储17-27个向量。

Conclusion: 免OCR、基于摘要指导的多模态检索是传统文本提取流程的实用且可扩展的替代方案。

Abstract: Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.
  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.
  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.

</details>


### [4] [FITRep: Attention-Guided Item Representation via MLLMs](https://arxiv.org/abs/2511.21389)
*Guoxiao Zhang,Ao Li,Tan Qu,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: FITRep是一个基于注意力引导的白盒物品表示框架，通过提取层次化语义概念、结构保持降维和FAISS聚类，解决多模态物品去重中的局部结构坍塌问题，在美团广告系统中显著提升了点击率和千次展示收益。


<details>
  <summary>Details</summary>
Motivation: 在线平台存在大量视觉和文本相似的近重复物品，导致用户体验下降。现有的多模态大语言模型方法将表示视为黑盒，忽略了结构关系（如主要vs辅助元素），存在局部结构坍塌问题。

Method: FITRep框架包含三个组件：(1)概念层次信息提取(CHIE)，使用MLLMs提取层次化语义概念；(2)结构保持降维(SPDR)，基于UMAP的自适应方法进行高效信息压缩；(3)FAISS聚类(FBC)，使用FAISS为每个物品分配唯一聚类ID。

Result: 在美团广告系统的在线A/B测试中，FITRep实现了+3.60%的点击率(CTR)提升和+4.25%的千次展示收益(CPM)提升。

Conclusion: FITRep通过注意力引导的白盒表示方法有效解决了多模态物品去重中的局部结构坍塌问题，在实际应用中表现出显著效果和商业价值。

Abstract: Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.

</details>


### [5] [RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction](https://arxiv.org/abs/2511.21394)
*Guoxiao Zhang,Tan Qu,Ao Li,DongLin Ni,Qianlong Xie,Xingxing Wang*

Main category: cs.IR

TL;DR: RIA是一个统一的端到端重排序框架，通过共享表示和四个关键组件，在保持低延迟的同时提升推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有重排序方法将排序和重排序解耦，导致列表评估模型在严格延迟约束下存在组合稀疏性和表示能力有限的问题。

Method: 提出RIA框架，包含四个组件：UCDT用于细粒度用户-物品-上下文建模；CUHT用于位置敏感偏好学习；LMH用于捕捉层次化物品依赖；EC用于推理时的效率-效果平衡。

Result: 在公开和工业数据集上超越最先进模型，AUC和LogLoss显著提升。在美团广告系统在线A/B测试中，CTR提升1.69%，CPM提升4.54%。

Conclusion: RIA通过统一架构有效解决了排序和重排序解耦问题，实现了效果和效率的平衡，在实际系统中验证了其价值。

Abstract: Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.

</details>

{"id": "2512.05119", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.05119", "abs": "https://arxiv.org/abs/2512.05119", "authors": ["Rongyang Zhang", "Yuqing Huang", "Chengqiang Lu", "Qimeng Wang", "Yan Gao", "Yi Wu", "Yao Hu", "Yin Xu", "Wei Wang", "Hao Wang", "Enhong Chen"], "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "comment": "26 pages, 6 figures, NeurIPS 2025 D&B Track poster", "summary": "In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.", "AI": {"tldr": "\u63d0\u51fa\u4e86RAG-IGBench\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG-IG\uff09\u7684\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5305\u542b\u65b0\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u89c6\u89c9\u589e\u5f3a\u7684\u56de\u7b54\u80fd\u663e\u8457\u63d0\u5347\u7406\u89e3\u548c\u8bb0\u5fc6\uff0c\u4f46\u73b0\u6709\u4ea4\u9519\u5185\u5bb9\u751f\u6210\u8d28\u91cf\u4e0d\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u4ea4\u9519\u5e8f\u5217\u7684\u57fa\u51c6\uff0c\u73b0\u6709\u57fa\u51c6\u591a\u4f7f\u7528\u5355\u6a21\u6001\u6307\u6807\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u56fe\u50cf-\u6587\u672c\u7ec4\u5408\u8f93\u51fa\u7684\u590d\u6742\u6027\u3002", "method": "\u6784\u5efaRAG-IGBench\u57fa\u51c6\uff0c\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u6700\u65b0\u516c\u5f00\u5185\u5bb9\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5f15\u5165\u521b\u65b0\u8bc4\u4f30\u6307\u6807\u8861\u91cf\u6587\u672c\u8d28\u91cf\u3001\u56fe\u50cf\u8d28\u91cf\u53ca\u5176\u4e00\u81f4\u6027\uff1b\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u68c0\u7d22\u673a\u5236\u7ed3\u5408\uff0c\u4f7f\u6a21\u578b\u80fd\u8bbf\u95ee\u5916\u90e8\u56fe\u50cf-\u6587\u672c\u4fe1\u606f\u751f\u6210\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u5185\u5bb9\u3002", "result": "\u5728RAG-IGBench\u4e0a\u5bf9\u6700\u5148\u8fdbMLLMs\uff08\u5f00\u6e90\u548c\u4e13\u6709\uff09\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u6df1\u5165\u5206\u6790\u6a21\u578b\u80fd\u529b\u548c\u5c40\u9650\u6027\uff1b\u9a8c\u8bc1\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\uff1b\u5728\u8bad\u7ec3\u96c6\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u63d0\u5347\uff0c\u8bc1\u5b9e\u6570\u636e\u96c6\u8d28\u91cf\u548c\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "RAG-IGBench\u4e3a\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u6709\u6548\u8bc4\u4f30\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.05334", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.05334", "abs": "https://arxiv.org/abs/2512.05334", "authors": ["Samaneh Mohtadi", "Kevin Roitero", "Stefano Mizzaro", "Gianluca Demartini"], "title": "The Effect of Document Summarization on LLM-Based Relevance Judgments", "comment": null, "summary": "Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u6587\u672c\u6458\u8981\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u76f8\u5173\u6027\u5224\u65ad\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9IR\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6458\u8981\u5224\u65ad\u5728\u7cfb\u7edf\u6392\u5e8f\u7a33\u5b9a\u6027\u4e0a\u4e0e\u5168\u6587\u5224\u65ad\u76f8\u5f53\uff0c\u4f46\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "motivation": "\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0cLLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u663e\u793a\u51fa\u6f5c\u529b\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c06\u6587\u6863\u4f5c\u4e3a\u56fa\u5b9a\u5355\u5143\u76f4\u63a5\u8f93\u5165LLM\uff0c\u4f46\u672a\u7814\u7a76\u6587\u672c\u6458\u8981\u5bf9LLM\u5224\u65ad\u53ef\u9760\u6027\u7684\u5f71\u54cd\u53ca\u5176\u5bf9IR\u8bc4\u4f30\u7684\u4e0b\u6e38\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684LLM\u5728\u591a\u4e2aTREC\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4\u8f83\u57fa\u4e8e\u5168\u6587\u7684LLM\u5224\u65ad\u4e0e\u57fa\u4e8e\u4e0d\u540c\u957f\u5ea6LLM\u751f\u6210\u6458\u8981\u7684\u5224\u65ad\u3002\u5206\u6790\u5b83\u4eec\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u3001\u5bf9\u68c0\u7d22\u6548\u679c\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5bf9IR\u7cfb\u7edf\u6392\u5e8f\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u6458\u8981\u5224\u65ad\u5728\u7cfb\u7edf\u6392\u5e8f\u7a33\u5b9a\u6027\u65b9\u9762\u4e0e\u5168\u6587\u5224\u65ad\u76f8\u5f53\uff0c\u4f46\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u6807\u7b7e\u5206\u5e03\u504f\u79fb\u548c\u504f\u5dee\uff0c\u8fd9\u4e9b\u504f\u5dee\u56e0\u6a21\u578b\u548c\u6570\u636e\u96c6\u800c\u5f02\u3002\u6458\u8981\u5224\u65ad\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u6a21\u5f0f\u4e0e\u5168\u6587\u5224\u65ad\u76f8\u4f3c\u3002", "conclusion": "\u6587\u672c\u6458\u8981\u65e2\u662f\u5b9e\u73b0\u66f4\u9ad8\u6548\u5927\u89c4\u6a21IR\u8bc4\u4f30\u7684\u673a\u4f1a\uff0c\u4e5f\u662f\u4e00\u4e2a\u5177\u6709\u91cd\u8981\u65b9\u6cd5\u8bba\u610f\u4e49\u7684\u9009\u62e9\uff0c\u4f1a\u5f71\u54cd\u81ea\u52a8\u5224\u65ad\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u4eba\u5458\u9700\u8981\u6743\u8861\u6458\u8981\u5e26\u6765\u7684\u6548\u7387\u63d0\u5347\u4e0e\u53ef\u80fd\u5f15\u5165\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002"}}
{"id": "2512.05411", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05411", "abs": "https://arxiv.org/abs/2512.05411", "authors": ["Pranav Pushkar Mishra", "Kranti Prakash Yeole", "Ramyashree Keshavamurthy", "Mokshit Bharat Surana", "Fatemeh Sarayloo"], "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems", "comment": "7 pages, 3 figures, 3 tables", "summary": "In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.", "AI": {"tldr": "\u4f7f\u7528LLM\u8fdb\u884c\u5143\u6570\u636e\u589e\u5f3a\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u5206\u5757\u7b56\u7565\u63d0\u5347RAG\u7cfb\u7edf\u6587\u6863\u68c0\u7d22\u6548\u679c\uff0c\u9012\u5f52\u5206\u5757+TF-IDF\u52a0\u6743\u5d4c\u5165\u8fbe\u523082.5%\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u4ece\u5927\u578b\u590d\u6742\u77e5\u8bc6\u5e93\u4e2d\u9ad8\u6548\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u5bf9\u8fd0\u8425\u751f\u4ea7\u529b\u548c\u51b3\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709RAG\u7cfb\u7edf\u5728\u6587\u6863\u68c0\u7d22\u65b9\u9762\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u9700\u8981\u63d0\u5347\u8bed\u4e49\u8868\u793a\u548c\u68c0\u7d22\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4f7f\u7528LLM\u8fdb\u884c\u5143\u6570\u636e\u589e\u5f3a\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u6d41\u6c34\u7ebf\u52a8\u6001\u751f\u6210\u6587\u6863\u7247\u6bb5\u7684\u5143\u6570\u636e\u3002\u6bd4\u8f83\u4e09\u79cd\u5206\u5757\u7b56\u7565\uff08\u8bed\u4e49\u3001\u9012\u5f52\u3001\u6734\u7d20\uff09\uff0c\u7ed3\u5408\u9ad8\u7ea7\u5d4c\u5165\u6280\u672f\uff0c\u4f7f\u7528\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u751f\u6210\u57fa\u51c6\u771f\u503c\uff0c\u901a\u8fc7\u547d\u4e2d\u7387\u548c\u5143\u6570\u636e\u4e00\u81f4\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5143\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u7eaf\u5185\u5bb9\u57fa\u7ebf\uff1a\u9012\u5f52\u5206\u5757+TF-IDF\u52a0\u6743\u5d4c\u5165\u8fbe\u523082.5%\u51c6\u786e\u7387\uff08\u7eaf\u8bed\u4e49\u65b9\u6cd5\u4e3a73.3%\uff09\uff1b\u6734\u7d20\u5206\u5757+\u524d\u7f00\u878d\u5408\u8fbe\u5230\u6700\u9ad8\u547d\u4e2d\u7387@10\u4e3a0.925\u3002\u5143\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u4e86\u5411\u91cf\u805a\u7c7b\u8d28\u91cf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u68c0\u7d22\u5ef6\u8fdf\u3002", "conclusion": "\u5143\u6570\u636e\u589e\u5f3a\u662f\u63d0\u5347RAG\u7cfb\u7edf\u6548\u679c\u7684\u5173\u952e\u4f18\u5316\u65b9\u6cd5\uff0c\u4e3a\u4f01\u4e1a\u73af\u5883\u90e8\u7f72\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u7684\u6587\u6863\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u8bc1\u660e\u4e86\u5143\u6570\u636e\u589e\u5f3a\u5728\u589e\u5f3aRAG\u6709\u6548\u6027\u65b9\u9762\u7684\u5f3a\u5927\u4f5c\u7528\u3002"}}
{"id": "2512.05967", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05967", "abs": "https://arxiv.org/abs/2512.05967", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiov\u00ec"], "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "comment": null, "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684RAG\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u5b9e\u4f53\u94fe\u63a5\u7684\u4e8b\u5b9e\u4fe1\u53f7\u6765\u63d0\u9ad8\u610f\u5927\u5229\u8bed\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5728LLM\u65f6\u4ee3\uff0c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684RAG\u7cfb\u7edf\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u7ecf\u5e38\u56e0\u672f\u8bed\u6b67\u4e49\u800c\u65e0\u6cd5\u4fdd\u8bc1\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u9700\u8981\u6539\u8fdb\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u4e8b\u5b9e\u7cbe\u786e\u6027\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7684RAG\u67b6\u6784\uff0c\u6574\u5408\u57fa\u4e8eWikidata\u7684\u5b9e\u4f53\u94fe\u63a5\u6a21\u5757\uff0c\u5b9e\u73b0\u4e09\u79cd\u91cd\u6392\u5e8f\u7b56\u7565\uff1a\u6df7\u5408\u5206\u6570\u52a0\u6743\u6a21\u578b\u3001\u4e92\u9006\u6392\u540d\u878d\u5408\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u5668\u3002", "result": "\u5728\u7279\u5b9a\u9886\u57df\u4e0a\u4e0b\u6587\u4e2d\uff0c\u57fa\u4e8e\u4e92\u9006\u6392\u540d\u878d\u5408\u7684\u6df7\u5408\u65b9\u6848\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u65b9\u6cd5\uff1b\u5728\u901a\u7528\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0c\u4ea4\u53c9\u7f16\u7801\u5668\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u5b9e\u4e86\u9886\u57df\u4e0d\u5339\u914d\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9886\u57df\u9002\u5e94\u548c\u6df7\u5408\u6392\u5e8f\u7b56\u7565\u5bf9\u63d0\u9ad8RAG\u4e8b\u5b9e\u7cbe\u786e\u6027\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u5b9e\u4f53\u611f\u77e5RAG\u7cfb\u7edf\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u81ea\u9002\u5e94\u53ef\u9760\u7684AI\u8f85\u5bfc\u5de5\u5177\u3002"}}

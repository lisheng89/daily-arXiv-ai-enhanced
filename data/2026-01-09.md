<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Correct and Weight: A Simple Yet Effective Loss for Implicit Feedback Recommendation](https://arxiv.org/abs/2601.04291)
*Minglei Yin,Chuanbo Hu,Bin Liu,Neil Zhenqiang Gong,Yanfang,Ye,Xin Li*

Main category: cs.IR

TL;DR: 提出CW损失函数，通过修正负采样分布和动态重加权机制解决隐式反馈中的假阴性问题，提升推荐系统性能。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈推荐系统面临假阴性问题的持续挑战——未观测到的用户-物品交互不一定表示负面偏好，这会影响模型训练效果。

Method: 1. 基于正-未标记学习思想，通过可观测的通用数据分布(p)和正交互分布(p^+)来近似真实负分布(p-)，修正负采样过程；2. 引入动态重加权机制，根据模型当前预测调整每个负实例的重要性，对置信度高的负样本施加更大排序间隔，对可能是假阴性的不确定负样本降低惩罚。

Result: 在四个大规模稀疏基准数据集上的实验表明，该方法在多个排序指标上一致且显著优于一系列最先进的损失函数。

Conclusion: CW损失函数优雅高效，无需复杂的数据采样修改或显著计算开销，可广泛应用于现有推荐模型，有效解决了隐式反馈中的假阴性问题。

Abstract: Learning from implicit feedback has become the standard paradigm for modern recommender systems. However, this setting is fraught with the persistent challenge of false negatives, where unobserved user-item interactions are not necessarily indicative of negative preference. To address this issue, this paper introduces a novel and principled loss function, named Corrected and Weighted (CW) loss, that systematically corrects for the impact of false negatives within the training objective. Our approach integrates two key techniques. First, inspired by Positive-Unlabeled learning, we debias the negative sampling process by re-calibrating the assumed negative distribution. By theoretically approximating the true negative distribution (p-) using the observable general data distribution (p) and the positive interaction distribution (p^+), our method provides a more accurate estimate of the likelihood that a sampled unlabeled item is truly negative. Second, we introduce a dynamic re-weighting mechanism that modulates the importance of each negative instance based on the model's current prediction. This scheme encourages the model to enforce a larger ranking margin between positive items and confidently predicted (i.e., easy) negative items, while simultaneously down-weighting the penalty on uncertain negatives that have a higher probability of being false negatives. A key advantage of our approach is its elegance and efficiency; it requires no complex modifications to the data sampling process or significant computational overhead, making it readily applicable to a wide array of existing recommendation models. Extensive experiments conducted on four large-scale, sparse benchmark datasets demonstrate the superiority of our proposed loss. The results show that our method consistently and significantly outperforms a suite of state-of-the-art loss functions across multiple ranking-oriented metrics.

</details>


### [2] [The Overlooked Role of Graded Relevance Thresholds in Multilingual Dense Retrieval](https://arxiv.org/abs/2601.04395)
*Tomer Wullach,Ori Shapira,Amir DN Cohen*

Main category: cs.IR

TL;DR: 研究发现，在密集检索模型微调中，使用分级相关性分数而非二元标签，并通过校准阈值来适应不同语言和任务，能显著提升性能、减少训练数据需求并缓解标注噪声。


<details>
  <summary>Details</summary>
Motivation: 当前密集检索模型通常使用二元相关性判断进行对比学习微调，但相关性本质上是分级的。研究者希望探索分级相关性分数及其转换为二元标签的阈值如何影响多语言密集检索性能。

Method: 使用带有LLM标注相关性分数的多语言数据集，分析单语言、多语言混合和跨语言检索场景。研究不同阈值对模型性能的影响，并探索阈值如何系统性地随语言和任务变化。

Result: 研究发现最优阈值在不同语言和任务间存在系统性差异，通常反映资源水平差异。精心选择的阈值能提升效果、减少微调数据需求并缓解标注噪声，而选择不当则会降低性能。

Conclusion: 分级相关性是密集检索中宝贵但未充分利用的信号，阈值校准应被视为微调流程中的原则性组件，需要针对不同语言和任务进行系统优化。

Abstract: Dense retrieval models are typically fine-tuned with contrastive learning objectives that require binary relevance judgments, even though relevance is inherently graded. We analyze how graded relevance scores and the threshold used to convert them into binary labels affect multilingual dense retrieval. Using a multilingual dataset with LLM-annotated relevance scores, we examine monolingual, multilingual mixture, and cross-lingual retrieval scenarios. Our findings show that the optimal threshold varies systematically across languages and tasks, often reflecting differences in resource level. A well-chosen threshold can improve effectiveness, reduce the amount of fine-tuning data required, and mitigate annotation noise, whereas a poorly chosen one can degrade performance. We argue that graded relevance is a valuable but underutilized signal for dense retrieval, and that threshold calibration should be treated as a principled component of the fine-tuning pipeline.

</details>


### [3] [Re-Rankers as Relevance Judges](https://arxiv.org/abs/2601.04455)
*Chuan Meng,Jiqun Liu,Mohammad Aliannejadi,Fengran Mo,Jeff Dalton,Maarten de Rijke*

Main category: cs.IR

TL;DR: 将重排序模型改造为相关性判断工具，通过两种策略（二进制token输出和分数阈值化）实现，在TREC数据集上表现优于现有LLM基准，但存在自我偏好和跨模型偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究将LLM预测相关性判断视为独立任务，而相关性预测在重排序任务中已有深入研究。缺乏将成熟重排序方法应用于相关性判断的研究，导致资源浪费和重复开发。本文旨在填补这一空白。

Method: 在"重排序器作为相关性判断器"框架下，设计两种适应策略：1) 使用重排序器生成的二进制token（如"true"/"false"）作为直接判断；2) 通过阈值化将连续重排序分数转换为二进制标签。在TREC-DL 2019-2023数据集上对8个重排序器（3个家族，220M到32B参数）进行实验。

Result: 重排序器作为相关性判断器在两种策略下，约40%-50%的情况下能超越现有最先进的LLM基准UMBRELA。同时发现这些判断器表现出强烈的自我偏好（偏向自身和同家族重排序器）以及跨家族偏见。

Conclusion: 重排序器可以有效地改造为相关性判断工具，性能优于专门设计的LLM方法，但需要注意其存在的评估偏见问题。这为相关性判断任务提供了新的思路，避免了重复开发。

Abstract: Using large language models (LLMs) to predict relevance judgments has shown promising results. Most studies treat this task as a distinct research line, e.g., focusing on prompt design for predicting relevance labels given a query and passage. However, predicting relevance judgments is essentially a form of relevance prediction, a problem extensively studied in tasks such as re-ranking. Despite this potential overlap, little research has explored reusing or adapting established re-ranking methods to predict relevance judgments, leading to potential resource waste and redundant development. To bridge this gap, we reproduce re-rankers in a re-ranker-as-relevance-judge setup. We design two adaptation strategies: (i) using binary tokens (e.g., "true" and "false") generated by a re-ranker as direct judgments, and (ii) converting continuous re-ranking scores into binary labels via thresholding. We perform extensive experiments on TREC-DL 2019 to 2023 with 8 re-rankers from 3 families, ranging from 220M to 32B, and analyse the evaluation bias exhibited by re-ranker-based judges. Results show that re-ranker-based relevance judges, under both strategies, can outperform UMBRELA, a state-of-the-art LLM-based relevance judge, in around 40% to 50% of the cases; they also exhibit strong self-preference towards their own and same-family re-rankers, as well as cross-family bias.

</details>


### [4] [Self-MedRAG: a Self-Reflective Hybrid Retrieval-Augmented Generation Framework for Reliable Medical Question Answering](https://arxiv.org/abs/2601.04531)
*Jessica Ryan,Alexander I. Gumilang,Robert Wiliam,Derwin Suhartono*

Main category: cs.IR

TL;DR: Self-MedRAG：结合混合检索与自反思循环的临床问答框架，通过迭代假设验证减少幻觉，在MedQA和PubMedQA上显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗问答中表现出潜力，但存在幻觉和缺乏依据的推理问题，限制了其在高风险临床场景中的可靠性。传统单次检索无法解决需要多步推理的复杂生物医学查询。

Method: 提出Self-MedRAG自反思混合框架：1）混合检索策略结合稀疏检索（BM25）和密集检索（Contriever），通过RRF融合；2）生成器生成答案和推理依据；3）轻量级自反思模块使用NLI或LLM验证；4）若证据不足，系统自主重新表述查询并迭代优化上下文。

Result: 混合检索显著优于单检索基线。自反思循环带来实质性提升：MedQA准确率从80.00%提升至83.33%，PubMedQA从69.10%提升至79.82%。

Conclusion: 混合检索与迭代、基于证据的自反思相结合，能有效减少无依据的断言，增强基于LLM系统的临床可靠性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in medical Question Answering (QA), yet they remain prone to hallucinations and ungrounded reasoning, limiting their reliability in high-stakes clinical scenarios. While Retrieval-Augmented Generation (RAG) mitigates these issues by incorporating external knowledge, conventional single-shot retrieval often fails to resolve complex biomedical queries requiring multi-step inference. To address this, we propose Self-MedRAG, a self-reflective hybrid framework designed to mimic the iterative hypothesis-verification process of clinical reasoning. Self-MedRAG integrates a hybrid retrieval strategy, combining sparse (BM25) and dense (Contriever) retrievers via Reciprocal Rank Fusion (RRF) to maximize evidence coverage. It employs a generator to produce answers with supporting rationales, which are then assessed by a lightweight self-reflection module using Natural Language Inference (NLI) or LLM-based verification. If the rationale lacks sufficient evidentiary support, the system autonomously reformulates the query and iterates to refine the context. We evaluated Self-MedRAG on the MedQA and PubMedQA benchmarks. The results demonstrate that our hybrid retrieval approach significantly outperforms single-retriever baselines. Furthermore, the inclusion of the self-reflective loop yielded substantial gains, increasing accuracy on MedQA from 80.00% to 83.33% and on PubMedQA from 69.10% to 79.82%. These findings confirm that integrating hybrid retrieval with iterative, evidence-based self-reflection effectively reduces unsupported claims and enhances the clinical reliability of LLM-based systems.

</details>


### [5] [Exploring Recommender System Evaluation: A Multi-Modal User Agent Framework for A/B Testing](https://arxiv.org/abs/2601.04554)
*Wenlin Zhang,Xiangyang Li,Qiyuan Ge,Kuicai Dong,Pengyue Jia,Xiaopeng Li,Zijian Zhang,Maolin Wang,Yichao Wang,Huifeng Guo,Ruiming Tang,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 提出基于多模态用户代理的A/B测试替代方案，通过构建推荐沙箱环境和模拟真实用户交互模式，减少传统在线A/B测试的经济成本和时间需求。


<details>
  <summary>Details</summary>
Motivation: 传统在线A/B测试存在经济成本高、用户体验下降、时间需求大等问题，而现有基于LLM的代理缺乏真实环境和视觉感知能力，无法模拟真实用户的感知过程和交互模式。

Method: 构建推荐沙箱环境支持多模态和多页面交互，设计多模态用户代理具备多模态信息感知、细粒度用户偏好建模，集成用户画像、动作记忆检索和疲劳系统来模拟复杂人类决策。

Result: 从模型、数据和特征三个角度验证了A/B Agent作为传统A/B测试替代方案的潜力，发现其生成的数据能有效增强推荐模型的能力。

Conclusion: A/B Agent为解决传统在线A/B测试的挑战提供了有效方案，展示了多模态用户代理在推荐系统评估中的实际应用价值。

Abstract: In recommender systems, online A/B testing is a crucial method for evaluating the performance of different models. However, conducting online A/B testing often presents significant challenges, including substantial economic costs, user experience degradation, and considerable time requirements. With the Large Language Models' powerful capacity, LLM-based agent shows great potential to replace traditional online A/B testing. Nonetheless, current agents fail to simulate the perception process and interaction patterns, due to the lack of real environments and visual perception capability. To address these challenges, we introduce a multi-modal user agent for A/B testing (A/B Agent). Specifically, we construct a recommendation sandbox environment for A/B testing, enabling multimodal and multi-page interactions that align with real user behavior on online platforms. The designed agent leverages multimodal information perception, fine-grained user preferences, and integrates profiles, action memory retrieval, and a fatigue system to simulate complex human decision-making. We validated the potential of the agent as an alternative to traditional A/B testing from three perspectives: model, data, and features. Furthermore, we found that the data generated by A/B Agent can effectively enhance the capabilities of recommendation models. Our code is publicly available at https://github.com/Applied-Machine-Learning-Lab/ABAgent.

</details>


### [6] [Adaptive Retrieval for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2601.04618)
*Jongho Kim,Jaeyoung Kim,Seung-won Hwang,Jihyuk Kim,Yu Jin Kim,Moontae Lee*

Main category: cs.IR

TL;DR: 提出REPAIR框架，利用推理计划作为自适应检索的密集反馈信号，解决推理密集型检索中"桥梁文档"召回不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的reranker管道虽然试图在排序中展现"桥梁文档"（对推理过程有贡献但不直接与初始查询相关的文档），但存在召回率受限的问题。朴素的自适应检索解决方案会导致规划错误传播

Method: 提出REPAIR框架，将推理计划重新用作自适应检索的密集反馈信号。关键区别在于通过选择性自适应检索实现reranking过程中的中途修正，检索支持关键计划的文档

Result: 在推理密集型检索和复杂QA任务上的实验结果表明，该方法比现有基线提高了5.6个百分点

Conclusion: REPAIR框架通过将推理计划作为反馈信号用于自适应检索，有效解决了推理密集型检索中桥梁文档召回不足的问题，显著提升了性能

Abstract: We study leveraging adaptive retrieval to ensure sufficient "bridge" documents are retrieved for reasoning-intensive retrieval. Bridge documents are those that contribute to the reasoning process yet are not directly relevant to the initial query. While existing reasoning-based reranker pipelines attempt to surface these documents in ranking, they suffer from bounded recall. Naive solution with adaptive retrieval into these pipelines often leads to planning error propagation. To address this, we propose REPAIR, a framework that bridges this gap by repurposing reasoning plans as dense feedback signals for adaptive retrieval. Our key distinction is enabling mid-course correction during reranking through selective adaptive retrieval, retrieving documents that support the pivotal plan. Experimental results on reasoning-intensive retrieval and complex QA tasks demonstrate that our method outperforms existing baselines by 5.6%pt.

</details>


### [7] [Succeeding at Scale: Automated Multi-Retriever Fusion and Query-Side Adaptation for Multi-Tenant Search](https://arxiv.org/abs/2601.04646)
*Prateek Jain,Shabari S Nair,Ritesh Goru,Prakhar Agarwal,Ajay Yadav,Yoga Sri Varshan Varadharajan,Constantine Caramanis*

Main category: cs.IR

TL;DR: 提出DevRev Search基准和索引保持适应策略，通过仅微调查询编码器实现检索性能提升，避免文档索引重建


<details>
  <summary>Details</summary>
Motivation: 大规模多租户检索系统面临两大挑战：1) 缺乏标注数据（"暗数据"问题）；2) 模型更新成本高，需要重新索引整个语料库

Method: 1) 构建DevRev Search基准：使用融合候选生成策略和LLM作为裁判进行一致性过滤和相关性标注；2) 提出索引保持适应策略：仅通过LoRA微调查询编码器，保持文档索引不变

Result: 在DevRev Search和SciFact数据集上实验表明，针对查询编码器特定transformer层进行微调能实现最佳质量-效率权衡

Conclusion: 该方法为个性化企业搜索提供了可扩展的路径，解决了多租户环境中的暗数据和索引重建成本问题

Abstract: Large-scale multi-tenant retrieval systems amass vast user query logs yet critically lack the curated relevance labels required for effective domain adaptation. This "dark data" problem is exacerbated by the operational cost of model updates: jointly fine-tuning query and document encoders requires re-indexing the entire corpus, which is prohibitive in multi-tenant environments with thousands of isolated indices. To address these dual challenges, we introduce \textbf{DevRev Search}, a passage retrieval benchmark for technical customer support constructed through a fully automatic pipeline. We employ a \textbf{fusion-based candidate generation} strategy, pooling results from diverse sparse and dense retrievers, and utilize an LLM-as-a-Judge to perform rigorous \textbf{consistency filtering} and relevance assignment. We further propose a practical \textbf{Index-Preserving Adaptation} strategy: by fine-tuning only the query encoder via Low-Rank Adaptation (LoRA), we achieve competitive performance improvements while keeping the document index frozen. Our experiments on DevRev Search and SciFact demonstrate that targeting specific transformer layers in the query encoder yields optimal quality-efficiency trade-offs, offering a scalable path for personalized enterprise search.

</details>


### [8] [PROMISE: Process Reward Models Unlock Test-Time Scaling Laws in Generative Recommendations](https://arxiv.org/abs/2601.04674)
*Chengcheng Guo,Kuo Cai,Yu Zhou,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: Promise框架通过过程奖励模型和引导式波束搜索解决生成式推荐中的语义漂移问题，实现推理时计算扩展，让小模型达到或超越大模型性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法存在语义漂移问题——早期高层token的错误会不可逆转地将生成轨迹导向不相关的语义子空间，严重影响推荐准确性

Method: 提出Promise框架：1) 轻量级过程奖励模型评估中间推理步骤质量；2) PRM引导的波束搜索策略，利用密集反馈动态剪枝错误分支；3) 实现推荐系统的推理时扩展定律

Result: 大规模平台上的离线实验和在线A/B测试表明，Promise有效缓解语义漂移，显著提升推荐准确性，同时支持高效部署，小模型通过增加推理计算可匹配或超越大模型

Conclusion: Promise通过密集逐步验证解决了生成式推荐中的语义漂移问题，实现了推荐系统的推理时扩展定律，为高效部署高质量推荐系统提供了新范式

Abstract: Generative Recommendation has emerged as a promising paradigm, reformulating recommendation as a sequence-to-sequence generation task over hierarchical Semantic IDs. However, existing methods suffer from a critical issue we term Semantic Drift, where errors in early, high-level tokens irreversibly divert the generation trajectory into irrelevant semantic subspaces. Inspired by Process Reward Models (PRMs) that enhance reasoning in Large Language Models, we propose Promise, a novel framework that integrates dense, step-by-step verification into generative models. Promise features a lightweight PRM to assess the quality of intermediate inference steps, coupled with a PRM-guided Beam Search strategy that leverages dense feedback to dynamically prune erroneous branches. Crucially, our approach unlocks Test-Time Scaling Laws for recommender systems: by increasing inference compute, smaller models can match or surpass larger models. Extensive offline experiments and online A/B tests on a large-scale platform demonstrate that Promise effectively mitigates Semantic Drift, significantly improving recommendation accuracy while enabling efficient deployment.

</details>


### [9] [Breaking Robustness Barriers in Cognitive Diagnosis: A One-Shot Neural Architecture Search Perspective](https://arxiv.org/abs/2601.04918)
*Ziwen Wang,Shangshang Yang,Xiaoshan Yu,Haiping Ma,Xingyi Zhang*

Main category: cs.IR

TL;DR: OSCD是一种用于认知诊断的进化多目标一次性神经架构搜索方法，旨在高效鲁棒地提升模型评估学习者能力，通过两阶段训练和搜索过程解决现有模型对噪声敏感和架构设计依赖专家经验的问题。


<details>
  <summary>Details</summary>
Motivation: 现有认知诊断模型存在两个主要问题：1) 忽视观察响应数据中的普遍噪声污染，影响实际部署；2) 过度依赖研究者领域专业知识进行结构设计，未能充分探索架构可能性，限制了模型潜力。

Method: 提出OSCD方法，包含两个阶段：训练阶段构建多样化架构组合的搜索空间，训练权重共享的超网络；搜索阶段将异构噪声场景下的最优架构搜索建模为多目标优化问题，集成帕累托最优解搜索策略和跨场景性能评估框架。

Result: 在真实世界教育数据集上的大量实验验证了OSCD模型发现的认知诊断任务最优架构的有效性和鲁棒性。

Conclusion: OSCD方法能够高效鲁棒地发现认知诊断任务的最优架构，解决了现有模型对噪声敏感和架构设计依赖专家经验的问题，为智能辅导系统提供了更实用的解决方案。

Abstract: With the advancement of network technologies, intelligent tutoring systems (ITS) have emerged to deliver increasingly precise and tailored personalized learning services. Cognitive diagnosis (CD) has emerged as a core research task in ITS, aiming to infer learners' mastery of specific knowledge concepts by modeling the mapping between learning behavior data and knowledge states. However, existing research prioritizes model performance enhancement while neglecting the pervasive noise contamination in observed response data, significantly hindering practical deployment. Furthermore, current cognitive diagnosis models (CDMs) rely heavily on researchers' domain expertise for structural design, which fails to exhaustively explore architectural possibilities, thus leaving model architectures' full potential untapped. To address this issue, we propose OSCD, an evolutionary multi-objective One-Shot neural architecture search method for Cognitive Diagnosis, designed to efficiently and robustly improve the model's capability in assessing learner proficiency. Specifically, OSCD operates through two distinct stages: training and searching. During the training stage, we construct a search space encompassing diverse architectural combinations and train a weight-sharing supernet represented via the complete binary tree topology, enabling comprehensive exploration of potential architectures beyond manual design priors. In the searching stage, we formulate the optimal architecture search under heterogeneous noise scenarios as a multi-objective optimization problem (MOP), and develop an optimization framework integrating a Pareto-optimal solution search strategy with cross-scenario performance evaluation for resolution. Extensive experiments on real-world educational datasets validate the effectiveness and robustness of the optimal architectures discovered by our OSCD model for CD tasks.

</details>


### [10] [Dynamics in Search Engine Query Suggestions for European Politicians](https://arxiv.org/abs/2601.05081)
*Franziska Pradel,Fabian Haak*

Main category: cs.IR

TL;DR: 研究分析Google搜索建议在十个欧洲国家中对欧洲政治家的查询建议，发现建议的稳定性受政治家国籍、职位类型和性别影响，跨国相似性则受领导地位和性别影响。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎常用于在线政治信息搜索，但反映互联网用户潜在兴趣的政治搜索查询建议在不同国家和时间如何变化尚不清楚。本研究旨在系统分析Google对欧洲和国家政治家的搜索查询建议。

Method: 使用在十个国家收集的欧洲政治家搜索查询建议原始数据集，分析查询建议的稳定性（随时间变化）和跨国相似性。考察影响因素包括：政治家原籍国、超国家角色、性别、领导地位等。

Result: 1. 查询建议在政治家原籍国、担任超国家角色的政治家、女性政治家情况下更不稳定；2. 对政治领导人和男性政治家的查询建议在各国间更相似。

Conclusion: 研究揭示了搜索查询建议的时空变化模式，为未来研究欧洲政治家在线信息搜索提供了方向，强调了搜索建议作为反映公众兴趣指标的重要性。

Abstract: Search engines are commonly used for online political information seeking. Yet, it remains unclear how search query suggestions for political searches that reflect the latent interest of internet users vary across countries and over time. We provide a systematic analysis of Google search engine query suggestions for European and national politicians. Using an original dataset of search query suggestions for European politicians collected in ten countries, we find that query suggestions are less stable over time in politicians' countries of origin, when the politicians hold a supranational role, and for female politicians. Moreover, query suggestions for political leaders and male politicians are more similar across countries. We conclude by discussing possible future directions for studying information search about European politicians in online search.

</details>


### [11] [Multivector Reranking in the Era of Strong First-Stage Retrievers](https://arxiv.org/abs/2601.05200)
*Silvio Martinico,Franco Maria Nardini,Cosimo Rulli,Rossano Venturini*

Main category: cs.IR

TL;DR: 该论文提出用单向量文档检索器（学习稀疏检索器）替代多向量检索中的token级收集阶段，形成两阶段检索架构，显著提升效率（24倍加速）同时保持检索质量。


<details>
  <summary>Details</summary>
Motivation: 多向量表示虽然检索效果好，但token级检索成本高，现有收集-精炼策略需要搜索大型token级索引且可能错过最优文档，需要更高效的检索方法。

Method: 1）用学习稀疏检索器（LSR）替代token级收集阶段；2）集成无推理LSR方法减少查询编码时间；3）提出多种重排序配置和两种早期剪枝优化技术。

Result: 两阶段方法比最先进多向量检索系统快24倍以上，检索质量相当或更优，优化技术提升效率1.8倍且不损失质量。

Conclusion: 将多向量检索重新构建为两阶段架构，用单向量检索器收集候选，再用多向量精炼，显著提升效率同时保持检索效果，为实际部署提供实用解决方案。

Abstract: Learned multivector representations power modern search systems with strong retrieval effectiveness, but their real-world use is limited by the high cost of exhaustive token-level retrieval. Therefore, most systems adopt a \emph{gather-and-refine} strategy, where a lightweight gather phase selects candidates for full scoring. However, this approach requires expensive searches over large token-level indexes and often misses the documents that would rank highest under full similarity. In this paper, we reproduce several state-of-the-art multivector retrieval methods on two publicly available datasets, providing a clear picture of the current multivector retrieval field and observing the inefficiency of token-level gathering. Building on top of that, we show that replacing the token-level gather phase with a single-vector document retriever -- specifically, a learned sparse retriever (LSR) -- produces a smaller and more semantically coherent candidate set. This recasts the gather-and-refine pipeline into the well-established two-stage retrieval architecture. As retrieval latency decreases, query encoding with two neural encoders becomes the dominant computational bottleneck. To mitigate this, we integrate recent inference-free LSR methods, demonstrating that they preserve the retrieval effectiveness of the dual-encoder pipeline while substantially reducing query encoding time. Finally, we investigate multiple reranking configurations that balance efficiency, memory, and effectiveness, and we introduce two optimization techniques that prune low-quality candidates early. Empirical results show that these techniques improve retrieval efficiency by up to 1.8$\times$ with no loss in quality. Overall, our two-stage approach achieves over $24\times$ speedup over the state-of-the-art multivector retrieval systems, while maintaining comparable or superior retrieval quality.

</details>

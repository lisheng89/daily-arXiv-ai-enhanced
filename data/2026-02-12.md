<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 13]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [JAG: Joint Attribute Graphs for Filtered Nearest Neighbor Search](https://arxiv.org/abs/2602.10258)
*Haike Xu,Guy Blelloch,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.IR

TL;DR: JAG是一种基于图的过滤最近邻搜索算法，通过引入属性和过滤距离，将二元过滤约束转化为连续导航指导，在各种过滤类型和选择性范围内都能提供稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现有过滤最近邻搜索算法对查询选择性和过滤类型高度敏感，通常只在特定过滤类别或狭窄选择性范围内表现良好，无法适应实际部署中需要泛化到新过滤类型和未知查询选择性的需求。

Method: 提出JAG（联合属性图）算法，引入属性和过滤距离概念，将二元过滤约束转化为连续导航指导。构建同时优化向量相似性和属性邻近性的邻近图，避免导航死胡同。

Result: 在五个数据集和四种过滤类型（标签、范围、子集、布尔）上的实验表明，JAG在吞吐量和召回率稳健性方面显著优于现有最先进的基线方法。

Conclusion: JAG通过联合优化向量相似性和属性邻近性，实现了在各种过滤类型和整个选择性范围内的稳健性能，解决了现有方法泛化能力不足的问题。

Abstract: Despite filtered nearest neighbor search being a fundamental task in modern vector search systems, the performance of existing algorithms is highly sensitive to query selectivity and filter type. In particular, existing solutions excel either at specific filter categories (e.g., label equality) or within narrow selectivity bands (e.g., pre-filtering for low selectivity) and are therefore a poor fit for practical deployments that demand generalization to new filter types and unknown query selectivities. In this paper, we propose JAG (Joint Attribute Graphs), a graph-based algorithm designed to deliver robust performance across the entire selectivity spectrum and support diverse filter types. Our key innovation is the introduction of attribute and filter distances, which transform binary filter constraints into continuous navigational guidance. By constructing a proximity graph that jointly optimizes for both vector similarity and attribute proximity, JAG prevents navigational dead-ends and allows JAG to consistently outperform prior graph-based filtered nearest neighbor search methods. Our experimental results across five datasets and four filter types (Label, Range, Subset, Boolean) demonstrate that JAG significantly outperforms existing state-of-the-art baselines in both throughput and recall robustness.

</details>


### [2] [MLDocRAG: Multimodal Long-Context Document Retrieval Augmented Generation](https://arxiv.org/abs/2602.10271)
*Yongyue Zhang,Yaxiong Wu*

Main category: cs.IR

TL;DR: 该论文提出了MLDocRAG框架，通过构建多模态块-查询图(MCQG)来解决长文档多模态理解中的跨模态异质性和跨页面推理挑战，显著提升了检索质量和问答准确性。


<details>
  <summary>Details</summary>
Motivation: 理解包含段落、图表等多模态块的长文档面临两大挑战：(1)跨模态异质性导致难以定位相关信息；(2)跨页面推理需要聚合分散在不同页面的证据。作者采用以查询为中心的表述，将跨模态和跨页面信息投影到统一的查询表示空间。

Method: 提出MLDocRAG框架，利用多模态块-查询图(MCQG)组织多模态文档内容。MCQG通过多模态文档扩展过程构建：从异构文档块生成细粒度查询，并将这些查询链接到跨模态和跨页面的对应内容。这种基于图的结构支持选择性、以查询为中心的检索和结构化证据聚合。

Result: 在MMLongBench-Doc和LongDocURL数据集上的实验表明，MLDocRAG持续提升了检索质量和答案准确性，证明了其在长上下文多模态理解中的有效性。

Conclusion: MLDocRAG框架通过构建多模态块-查询图，有效解决了长文档多模态理解中的跨模态异质性和跨页面推理问题，为长上下文多模态问答提供了增强的接地性和连贯性。

Abstract: Understanding multimodal long-context documents that comprise multimodal chunks such as paragraphs, figures, and tables is challenging due to (1) cross-modal heterogeneity to localize relevant information across modalities, (2) cross-page reasoning to aggregate dispersed evidence across pages. To address these challenges, we are motivated to adopt a query-centric formulation that projects cross-modal and cross-page information into a unified query representation space, with queries acting as abstract semantic surrogates for heterogeneous multimodal content. In this paper, we propose a Multimodal Long-Context Document Retrieval Augmented Generation (MLDocRAG) framework that leverages a Multimodal Chunk-Query Graph (MCQG) to organize multimodal document content around semantically rich, answerable queries. MCQG is constructed via a multimodal document expansion process that generates fine-grained queries from heterogeneous document chunks and links them to their corresponding content across modalities and pages. This graph-based structure enables selective, query-centric retrieval and structured evidence aggregation, thereby enhancing grounding and coherence in long-context multimodal question answering. Experiments on datasets MMLongBench-Doc and LongDocURL demonstrate that MLDocRAG consistently improves retrieval quality and answer accuracy, demonstrating its effectiveness for long-context multimodal understanding.

</details>


### [3] [Single-Turn LLM Reformulation Powered Multi-Stage Hybrid Re-Ranking for Tip-of-the-Tongue Known-Item Retrieval](https://arxiv.org/abs/2602.10321)
*Debayan Mukhopadhyay,Utshab Kumar Ghosh,Shubham Chatterjee*

Main category: cs.IR

TL;DR: 提出使用通用8B参数LLM进行查询重写，解决模糊描述检索（ToT）问题，通过轻量级预处理显著提升检索效果，后续重排进一步优化性能


<details>
  <summary>Details</summary>
Motivation: 解决"舌尖现象"检索问题——用户用模糊描述检索已知项目，传统伪相关反馈在初始召回率低时效果不佳，需要更有效的查询重写方法

Method: 使用通用8B参数LLM单次调用进行查询重写，不进行领域微调；构建多阶段检索管道：稀疏检索(BM25)、稠密/延迟交互重排(Contriever, E5-large-v2, ColBERTv2)、monoT5交叉编码、列表重排(Qwen 2.5 72B)

Result: 在2025 TREC-ToT数据集上，原始查询效果差，但轻量级预处理使Recall提升20.61%；后续重排使nDCG@10提升33.88%，MRR提升29.92%，MAP@10提升29.98%

Conclusion: 通用LLM的轻量级查询重写是解决ToT检索问题的有效方法，解锁了下游排序器的潜力，且成本效益高，无需专门微调

Abstract: Retrieving known items from vague descriptions, Tip-of-the-Tongue (ToT) retrieval, remains a significant challenge. We propose using a single call to a generic 8B-parameter LLM for query reformulation, bridging the gap between ill-formed ToT queries and specific information needs. This method is particularly effective where standard Pseudo-Relevance Feedback fails due to poor initial recall. Crucially, our LLM is not fine-tuned for ToT or specific domains, demonstrating that gains stem from our prompting strategy rather than model specialization. Rewritten queries feed a multi-stage pipeline: sparse retrieval (BM25), dense/late-interaction reranking (Contriever, E5-large-v2, ColBERTv2), monoT5 cross-encoding, and list-wise reranking (Qwen 2.5 72B). Experiments on 2025 TREC-ToT datasets show that while raw queries yield poor performance, our lightweight pre-retrieval transformation improves Recall by 20.61%. Subsequent reranking improves nDCG@10 by 33.88%, MRR by 29.92%, and MAP@10 by 29.98%, offering a cost-effective intervention that unlocks the potential of downstream rankers. Code and data: https://github.com/debayan1405/TREC-TOT-2025

</details>


### [4] [GeoGR: A Generative Retrieval Framework for Spatio-Temporal Aware POI Recommendation](https://arxiv.org/abs/2602.10411)
*Fangye Wang,Haowen Lin,Yifang Yuan,Siyuan Wang,Xiaojiang Zhou,Song Yang,Pengjie Wang*

Main category: cs.IR

TL;DR: GeoGR是一个针对导航LBS的地理生成推荐框架，通过两阶段设计解决POI推荐中的语义ID建模和LLM对齐问题，在AMAP平台上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于SID的POI推荐方法在复杂稀疏的真实环境中存在两个关键限制：1) 高质量跨类别时空协作关系的SID建模不足；2) 大语言模型与POI推荐任务的对齐不佳。

Method: 两阶段设计：1) 地理感知的SID标记化流程，通过地理约束的共访问POI对、对比学习和迭代优化学习时空协作语义表示；2) 多阶段LLM训练策略，通过模板化持续预训练对齐非原生SID标记，并通过监督微调实现自回归POI生成。

Result: 在多个真实世界数据集上实验显示GeoGR优于现有最先进基线方法。在AMAP平台部署后，服务数百万用户，多个在线指标得到提升，证实了其实际有效性和生产可扩展性。

Conclusion: GeoGR是一个针对导航LBS的地理生成推荐框架，能够感知用户上下文状态变化，实现意图感知的POI推荐，在理论和实践中都表现出优越性能。

Abstract: Next Point-of-Interest (POI) prediction is a fundamental task in location-based services, especially critical for large-scale navigation platforms like AMAP that serve billions of users across diverse lifestyle scenarios. While recent POI recommendation approaches based on SIDs have achieved promising, they struggle in complex, sparse real-world environments due to two key limitations: (1) inadequate modeling of high-quality SIDs that capture cross-category spatio-temporal collaborative relationships, and (2) poor alignment between large language models (LLMs) and the POI recommendation task. To this end, we propose GeoGR, a geographic generative recommendation framework tailored for navigation-based LBS like AMAP, which perceives users' contextual state changes and enables intent-aware POI recommendation. GeoGR features a two-stage design: (i) a geo-aware SID tokenization pipeline that explicitly learns spatio-temporal collaborative semantic representations via geographically constrained co-visited POI pairs, contrastive learning, and iterative refinement; and (ii) a multi-stage LLM training strategy that aligns non-native SID tokens through multiple template-based continued pre-training(CPT) and enables autoregressive POI generation via supervised fine-tuning(SFT). Extensive experiments on multiple real-world datasets demonstrate GeoGR's superiority over state-of-the-art baselines. Moreover, deployment on the AMAP platform, serving millions of users with multiple online metrics boosting, confirms its practical effectiveness and scalability in production.

</details>


### [5] [End-to-End Semantic ID Generation for Generative Advertisement Recommendation](https://arxiv.org/abs/2602.10445)
*Jie Jiang,Xinxun Zhang,Enming Zhang,Yuling Xiong,Jun Zhang,Jingwen Wang,Huan Yu,Yuxiang Wang,Hao Wang,Xiao Yan,Jiawei Jiang*

Main category: cs.IR

TL;DR: UniSID是一个统一的语义ID生成框架，通过端到端联合优化嵌入和语义ID，解决现有生成式推荐中两阶段压缩的语义退化问题，在广告推荐场景中显著提升命中率。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐使用残差量化生成语义ID，存在两个主要问题：1) 两阶段压缩导致目标不对齐和语义退化；2) RQ结构固有的误差累积。这些问题限制了语义ID的质量和推荐效果。

Method: 提出UniSID框架：1) 端到端联合优化嵌入和语义ID，直接从原始广告数据学习；2) 引入多粒度对比学习策略，对齐不同语义ID层级的项目；3) 提出基于摘要的广告重建机制，让语义ID捕获广告上下文中未明确出现的高层语义信息。

Result: 实验表明UniSID在多个下游广告场景中一致优于最先进的语义ID生成方法，命中率指标相比最强基线提升高达4.62%。

Conclusion: UniSID通过统一的端到端框架解决了现有语义ID生成方法的局限性，能够更好地捕获语义信息，显著提升生成式广告推荐的性能。

Abstract: Generative Recommendation (GR) has excelled by framing recommendation as next-token prediction. This paradigm relies on Semantic IDs (SIDs) to tokenize large-scale items into discrete sequences. Existing GR approaches predominantly generate SIDs via Residual Quantization (RQ), where items are encoded into embeddings and then quantized to discrete SIDs. However, this paradigm suffers from inherent limitations: 1) Objective misalignment and semantic degradation stemming from the two-stage compression; 2) Error accumulation inherent in the structure of RQ. To address these limitations, we propose UniSID, a Unified SID generation framework for generative advertisement recommendation. Specifically, we jointly optimize embeddings and SIDs in an end-to-end manner from raw advertising data, enabling semantic information to flow directly into the SID space and thus addressing the inherent limitations of the two-stage cascading compression paradigm. To capture fine-grained semantics, a multi-granularity contrastive learning strategy is introduced to align distinct items across SID levels. Finally, a summary-based ad reconstruction mechanism is proposed to encourage SIDs to capture high-level semantic information that is not explicitly present in advertising contexts. Experiments demonstrate that UniSID consistently outperforms state-of-the-art SID generation methods, yielding up to a 4.62% improvement in Hit Rate metrics across downstream advertising scenarios compared to the strongest baseline.

</details>


### [6] [Compute Only Once: UG-Separation for Efficient Large Recommendation Models](https://arxiv.org/abs/2602.10455)
*Hui Lu,Zheng Chai,Shipeng Bai,Hao Zhang,Zhifang Fan,Kunmin Bai,Yingwen Wu,Bingzheng Wei,Xiang Sun,Ziyan Gong,Tianyi Liu,Hua Chen,Deping Xie,Zhongkai Chen,Zhiliang Guo,Qiwei Chen,Yuchao Zheng*

Main category: cs.IR

TL;DR: UG-Sep框架首次在密集交互推荐模型中实现可复用的用户侧计算，通过掩码机制分离用户-物品信息流，结合信息补偿和量化技术，显著降低推理延迟20%


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统虽然能捕捉复杂特征交互，但带来了高昂的训练和推理成本。现有长序列模型可通过KV缓存复用用户侧计算，但密集特征交互架构中用户和物品特征深度纠缠，难以实现类似复用

Method: 提出用户-物品分离框架，在token混合层引入掩码机制显式解耦用户侧和物品侧信息流，确保部分token保持纯用户侧表征；设计信息补偿策略自适应重建被抑制的用户-物品交互；结合W8A16仅权重量化缓解内存带宽瓶颈

Result: 在字节跳动多个业务场景的离线评估和在线A/B实验中，UG-Sep将推理延迟降低高达20%，且不损害在线用户体验或商业指标

Conclusion: UG-Sep首次在密集交互推荐模型中实现可复用的用户侧计算，通过解耦设计、信息补偿和量化技术，在保持推荐质量的同时显著提升推理效率

Abstract: Driven by scaling laws, recommender systems increasingly rely on large-scale models to capture complex feature interactions and user behaviors, but this trend also leads to prohibitive training and inference costs. While long-sequence models(e.g., LONGER) can reuse user-side computation through KV caching, such reuse is difficult in dense feature interaction architectures(e.g., RankMixer), where user and group (candidate item) features are deeply entangled across layers. In this work, we propose User-Group Separation (UG-Sep), a novel framework that enables reusable user-side computation in dense interaction models for the first time. UG-Sep introduces a masking mechanism that explicitly disentangles user-side and item-side information flows within token-mixing layers, ensuring that a subset of tokens to preserve purely user-side representations across layers. This design enables corresponding token computations to be reused across multiple samples, significantly reducing redundant inference cost. To compensate for potential expressiveness loss induced by masking, we further propose an Information Compensation strategy that adaptively reconstructs suppressed user-item interactions. Moreover, as UG-Sep substantially reduces user-side FLOPs and exposes memory-bound components, we incorporate W8A16 (8-bit weight, 16-bit activation) weight-only quantization to alleviate memory bandwidth bottlenecks and achieve additional acceleration. We conduct extensive offline evaluations and large-scale online A/B experiments at ByteDance, demonstrating that UG-Sep reduces inference latency by up to 20 percent without degrading online user experience or commercial metrics across multiple business scenarios, including feed recommendation and advertising systems.

</details>


### [7] [ChainRec: An Agentic Recommender Learning to Route Tool Chains for Diverse and Evolving Interests](https://arxiv.org/abs/2602.10490)
*Fuchun Li,Qian Li,Xingyu Gao,Bocheng Pan,Yang Wu,Jun Zhang,Huan Yu,Jie Jiang,Jinsheng Xiao,Hailong Shi*

Main category: cs.IR

TL;DR: ChainRec：基于动态工具选择的智能推荐代理，通过规划器自适应选择推理工具，在冷启动和兴趣迁移场景表现优异


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统大多采用固定工作流程，无法适应多样化的用户场景（如冷启动、兴趣迁移）。需要一种能够根据具体情境动态决定证据收集策略的智能代理。

Method: 1. 从专家轨迹构建标准化工具代理库；2. 使用监督微调和偏好优化训练规划器，动态选择工具、决定执行顺序和停止时机；3. 在AgentRecBench基准上进行实验验证。

Result: 在Amazon、Yelp、Goodreads数据集上，ChainRec在HR@{1,3,5}指标上持续超越强基线，尤其在冷启动和兴趣演化场景表现突出。消融实验验证了工具标准化和偏好优化规划的重要性。

Conclusion: ChainRec通过动态工具选择和规划实现了自适应推荐，解决了传统固定流程方法的局限性，为智能推荐代理提供了有效框架。

Abstract: Large language models (LLMs) are increasingly integrated into recommender systems, motivating recent interest in agentic and reasoning-based recommendation. However, most existing approaches still rely on fixed workflows, applying the same reasoning procedure across diverse recommendation scenarios. In practice, user contexts vary substantially-for example, in cold-start settings or during interest shifts, so an agent should adaptively decide what evidence to gather next rather than following a scripted process. To address this, we propose ChainRec, an agentic recommender that uses a planner to dynamically select reasoning tools. ChainRec builds a standardized Tool Agent Library from expert trajectories. It then trains a planner using supervised fine-tuning and preference optimization to dynamically select tools, decide their order, and determine when to stop. Experiments on AgentRecBench across Amazon, Yelp, and Goodreads show that ChainRec consistently improves Avg HR@{1,3,5} over strong baselines, with especially notable gains in cold-start and evolving-interest scenarios. Ablation studies further validate the importance of tool standardization and preference-optimized planning.

</details>


### [8] [Boundary-Aware Multi-Behavior Dynamic Graph Transformer for Sequential Recommendation](https://arxiv.org/abs/2602.10493)
*Jingsong Su,Xuetao Ma,Mingming Li,Qiannan Zhu,Yu Guo*

Main category: cs.IR

TL;DR: 提出MB-DGT模型，通过动态图变换器和多行为损失函数，同时处理图结构动态性和用户行为序列，提升推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统方法难以同时处理图拓扑结构的动态性和用户交互序列模式，且未能充分捕捉多行为边界

Method: 提出边界感知的多行为动态图变换器(MB-DGT)，包含基于变换器的动态图聚合器用于用户偏好建模，以及用户特定的多行为损失函数用于模型优化

Result: 在三个数据集上的实验表明，该模型始终提供卓越的推荐性能

Conclusion: MB-DGT模型通过动态图结构细化和多行为边界感知，能够更全面地表示用户偏好，显著提升推荐效果

Abstract: In the landscape of contemporary recommender systems, user-item interactions are inherently dynamic and sequential, often characterized by various behaviors. Prior research has explored the modeling of user preferences through sequential interactions and the user-item interaction graph, utilizing advanced techniques such as graph neural networks and transformer-based architectures. However, these methods typically fall short in simultaneously accounting for the dynamic nature of graph topologies and the sequential pattern of interactions in user preference models. Moreover, they often fail to adequately capture the multiple user behavior boundaries during model optimization. To tackle these challenges, we introduce a boundary-aware Multi-Behavioral Dynamic Graph Transformer (MB-DGT) model that dynamically refines the graph structure to reflect the evolving patterns of user behaviors and interactions. Our model involves a transformer-based dynamic graph aggregator for user preference modeling, which assimilates the changing graph structure and the sequence of user behaviors. This integration yields a more comprehensive and dynamic representation of user preferences. For model optimization, we implement a user-specific multi-behavior loss function that delineates the interest boundaries among different behaviors, thereby enriching the personalized learning of user preferences. Comprehensive experiments across three datasets indicate that our model consistently delivers remarkable recommendation performance.

</details>


### [9] [Campaign-2-PT-RAG: LLM-Guided Semantic Product Type Attribution for Scalable Campaign Ranking](https://arxiv.org/abs/2602.10577)
*Yiming Che,Mansi Mane,Keerthi Gopalakrishnan,Parisa Kaghazgaran,Murali Mohana Krishna Dandu,Archana Venkatachalapathy,Sinduja Subramaniam,Yokila Arora,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: 提出Campaign-2-PT-RAG框架，使用LLM从营销活动内容推断推广的产品类型，为电商活动排名模型生成大规模训练标签


<details>
  <summary>Details</summary>
Motivation: 电商营销活动排名模型需要大规模的训练标签来识别哪些用户购买是由活动影响导致的，但生成这些标签很困难，因为营销活动使用创意性、主题性的语言，不直接映射到具体产品购买。缺乏清晰的产品级归因限制了监督学习在活动优化中的应用。

Method: Campaign-2-PT-RAG框架：1) 使用LLM解释营销活动内容，捕捉隐含意图；2) 通过语义搜索在平台分类中检索候选产品类型；3) 使用结构化LLM分类器评估每个产品类型的相关性，生成活动特定的产品覆盖集；4) 用户购买匹配这些产品类型时生成正训练标签。

Result: 在内部和合成数据集上的实验显示，该方法在专家标注的活动-产品类型映射验证下，生成高质量标签，精度达到78-90%，同时保持超过99%的召回率。

Conclusion: 该方法将模糊的归因问题转化为可处理的语义对齐任务，为下游任务（如电商生产环境中的活动排名优化）提供可扩展且一致的监督，解决了电商营销活动标签生成的挑战。

Abstract: E-commerce campaign ranking models require large-scale training labels indicating which users purchased due to campaign influence. However, generating these labels is challenging because campaigns use creative, thematic language that does not directly map to product purchases. Without clear product-level attribution, supervised learning for campaign optimization remains limited. We present \textbf{Campaign-2-PT-RAG}, a scalable label generation framework that constructs user--campaign purchase labels by inferring which product types (PTs) each campaign promotes. The framework first interprets campaign content using large language models (LLMs) to capture implicit intent, then retrieves candidate PTs through semantic search over the platform taxonomy. A structured LLM-based classifier evaluates each PT's relevance, producing a campaign-specific product coverage set. User purchases matching these PTs generate positive training labels for downstream ranking models. This approach reframes the ambiguous attribution problem into a tractable semantic alignment task, enabling scalable and consistent supervision for downstream tasks such as campaign ranking optimization in production e-commerce environments. Experiments on internal and synthetic datasets, validated against expert-annotated campaign--PT mappings, show that our LLM-assisted approach generates high-quality labels with 78--90% precision while maintaining over 99% recall.

</details>


### [10] [S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage](https://arxiv.org/abs/2602.10606)
*Jie Jiang,Hongbo Tang,Wenjie Wu,Yangru Huang,Zhenmao Li,Qian Li,Changping Wang,Jun Zhang,Huan Yu*

Main category: cs.IR

TL;DR: S-GRec是一个语义感知推荐框架，通过离线LLM语义评判器和在线轻量生成器的解耦，在不使用实时LLM推理的情况下，将语义监督融入推荐系统，同时保持业务目标优先。


<details>
  <summary>Details</summary>
Motivation: 传统生成式推荐模型仅依赖行为日志训练，缺乏对用户深层意图的监督。虽然LLMs能提供丰富的语义先验，但直接应用于工业推荐面临两大障碍：语义信号可能与平台业务目标冲突，且LLM推理成本过高。

Method: S-GRec采用解耦架构：离线LLM语义评判器（PSJ）生成可解释的方面证据和用户条件聚合，提供语义奖励；在线轻量生成器进行推荐。通过A2PO算法，以业务奖励（如eCPM）为锚点，仅在语义优势与业务目标一致时注入语义监督。

Result: 在公开基准和大规模生产系统中验证了有效性和可扩展性：CTR显著提升，在线A/B测试中GMV提升1.19%，且无需实时LLM推理。

Conclusion: S-GRec成功解决了LLM在工业推荐中的两大障碍，通过解耦架构和策略优化，在保持业务目标优先的前提下有效融入语义监督，实现了效果和效率的平衡。

Abstract: Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\% lift in GMV in online A/B tests, without requiring real-time LLM inference.

</details>


### [11] [A Cognitive Distribution and Behavior-Consistent Framework for Black-Box Attacks on Recommender Systems](https://arxiv.org/abs/2602.10633)
*Hongyue Zhan,Mingming Li,Dongqin Liu,Hui Wang,Yaning Zhang,Xi Zhou,Honglei Lv,Jiao Dai,Jizhong Han*

Main category: cs.IR

TL;DR: 提出双重增强攻击框架：认知分布驱动提取机制+行为感知噪声项目生成策略，显著提升推荐系统的攻击成功率和规避率


<details>
  <summary>Details</summary>
Motivation: 现有黑盒提取攻击主要依赖硬标签或成对学习，忽略排名位置重要性，导致知识转移不完整；纯梯度方法生成的对抗序列缺乏语义一致性，易被检测

Method: 1. 认知分布驱动提取机制：基于首因效应和位置偏差，将离散排名映射为具有位置感知衰减的连续值分布，实现从顺序对齐到认知分布对齐；2. 行为感知噪声项目生成策略：联合优化协同信号和梯度信号，确保语义连贯性和统计隐蔽性

Result: 在多个数据集上的实验表明，该方法在攻击成功率和规避率方面显著优于现有方法

Conclusion: 验证了将认知建模和行为一致性结合对于安全推荐系统的价值，为防御对抗攻击提供了新视角

Abstract: With the growing deployment of sequential recommender systems in e-commerce and other fields, their black-box interfaces raise security concerns: models are vulnerable to extraction and subsequent adversarial manipulation. Existing black-box extraction attacks primarily rely on hard labels or pairwise learning, often ignoring the importance of ranking positions, which results in incomplete knowledge transfer. Moreover, adversarial sequences generated via pure gradient methods lack semantic consistency with real user behavior, making them easily detectable. To overcome these limitations, this paper proposes a dual-enhanced attack framework. First, drawing on primacy effects and position bias, we introduce a cognitive distribution-driven extraction mechanism that maps discrete rankings into continuous value distributions with position-aware decay, thereby advancing from order alignment to cognitive distribution alignment. Second, we design a behavior-aware noisy item generation strategy that jointly optimizes collaborative signals and gradient signals. This ensures both semantic coherence and statistical stealth while effectively promoting target item rankings. Extensive experiments on multiple datasets demonstrate that our approach significantly outperforms existing methods in both attack success rate and evasion rate, validating the value of integrating cognitive modeling and behavioral consistency for secure recommender systems.

</details>


### [12] [EST: Towards Efficient Scaling Laws in Click-Through Rate Prediction via Unified Modeling](https://arxiv.org/abs/2602.10811)
*Mingyang Liu,Yong Bai,Zhangming Chan,Sishuo Chen,Xiang-Rong Sheng,Han Zhu,Jian Xu,Xinyang Chen*

Main category: cs.IR

TL;DR: EST模型通过全统一建模处理原始输入，使用轻量交叉注意力和内容稀疏注意力，实现高效可扩展的CTR预测，在淘宝广告平台显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法通常采用早期行为聚合来保持效率，但这种非统一或部分统一建模会丢弃细粒度token级信号，形成信息瓶颈，限制了模型扩展增益。

Method: 提出高效可扩展Transformer（EST），通过单序列处理所有原始输入实现全统一建模。包含两个模块：1）轻量交叉注意力（LCA），修剪冗余自交互，聚焦高影响跨特征依赖；2）内容稀疏注意力（CSA），利用内容相似性动态选择高信号行为。

Result: EST展现出稳定高效幂律缩放关系，可实现可预测的性能增益。在淘宝展示广告平台部署中，显著超越生产基线，实现3.27% RPM提升和1.22% CTR提升。

Conclusion: EST为工业CTR预测模型提供了实用的可扩展路径，通过全统一建模和专门设计的注意力机制，有效解决了现有方法的信息瓶颈问题，实现了效率和性能的平衡。

Abstract: Efficiently scaling industrial Click-Through Rate (CTR) prediction has recently attracted significant research attention. Existing approaches typically employ early aggregation of user behaviors to maintain efficiency. However, such non-unified or partially unified modeling creates an information bottleneck by discarding fine-grained, token-level signals essential for unlocking scaling gains. In this work, we revisit the fundamental distinctions between CTR prediction and Large Language Models (LLMs), identifying two critical properties: the asymmetry in information density between behavioral and non-behavioral features, and the modality-specific priors of content-rich signals. Accordingly, we propose the Efficiently Scalable Transformer (EST), which achieves fully unified modeling by processing all raw inputs in a single sequence without lossy aggregation. EST integrates two modules: Lightweight Cross-Attention (LCA), which prunes redundant self-interactions to focus on high-impact cross-feature dependencies, and Content Sparse Attention (CSA), which utilizes content similarity to dynamically select high-signal behaviors. Extensive experiments show that EST exhibits a stable and efficient power-law scaling relationship, enabling predictable performance gains with model scale. Deployed on Taobao's display advertising platform, EST significantly outperforms production baselines, delivering a 3.27\% RPM (Revenue Per Mile) increase and a 1.22\% CTR lift, establishing a practical pathway for scalable industrial CTR prediction models.

</details>


### [13] [Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval](https://arxiv.org/abs/2602.10833)
*William Xion,Wolfgang Nejdl*

Main category: cs.IR

TL;DR: 研究发现密集检索器的"来源偏见"（偏好LLM生成文本）主要是训练诱导现象而非固有属性，受训练数据和方式显著影响。


<details>
  <summary>Details</summary>
Motivation: 针对密集检索器中存在的"来源偏见"现象（偏好LLM生成文本），研究旨在探究这种偏见的起源，验证低困惑度是否为主要原因，并分析不同训练阶段和数据源对偏见的影响。

Method: 使用SciFact和NQ320K数据集的并行人类生成和LLM生成文本，比较无监督检查点与使用不同数据微调的模型：领域内人类文本、领域内LLM生成文本和MS MARCO。通过重新附加语言建模头到微调后的密集检索器编码器进行困惑度探测。

Result: 1) 无监督检索器没有统一的pro-LLM偏好，方向和程度取决于数据集；2) MS MARCO微调始终使排名偏向LLM生成文本；3) 领域内微调产生数据集特定且不一致的偏好变化；4) LLM生成语料微调诱导显著的pro-LLM偏见；5) 困惑度探测显示相关性接近随机，削弱了困惑度的解释力。

Conclusion: 来源偏见主要是训练诱导现象而非密集检索器的固有属性，训练数据和微调策略显著影响检索器对LLM生成文本的偏好程度。

Abstract: Dense retrieval is a promising approach for acquiring relevant context or world knowledge in open-domain natural language processing tasks and is now widely used in information retrieval applications. However, recent reports claim a broad preference for text generated by large language models (LLMs). This bias is called "source bias", and it has been hypothesized that lower perplexity contributes to this effect. In this study, we revisit this claim by conducting a controlled evaluation to trace the emergence of such preferences across training stages and data sources. Using parallel human- and LLM-generated counterparts of the SciFact and Natural Questions (NQ320K) datasets, we compare unsupervised checkpoints with models fine-tuned using in-domain human text, in-domain LLM-generated text, and MS MARCO. Our results show the following: 1) Unsupervised retrievers do not exhibit a uniform pro-LLM preference. The direction and magnitude depend on the dataset. 2) Across the settings tested, supervised fine-tuning on MS MARCO consistently shifts the rankings toward LLM-generated text. 3) In-domain fine-tuning produces dataset-specific and inconsistent shifts in preference. 4) Fine-tuning on LLM-generated corpora induces a pronounced pro-LLM bias. Finally, a retriever-centric perplexity probe involving the reattachment of a language modeling head to the fine-tuned dense retriever encoder indicates agreement with relevance near chance, thereby weakening the explanatory power of perplexity. Our study demonstrates that source bias is a training-induced phenomenon rather than an inherent property of dense retrievers.

</details>

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval](https://arxiv.org/abs/2510.12014)
*Eric He,Akash Gupta,Adian Liusie,Vatsal Raina,Piotr Molenda,Shirom Chabra,Vyas Raina*

Main category: cs.IR

TL;DR: 提出一个框架，将强大的视觉-语言模型的偏好排名蒸馏到基于嵌入的系统中，在保持推理时扩展性的同时提升个性化文本-图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 基于嵌入的方法如CLIP在大规模检索中高效，但无法很好地处理产品推荐中常见的抽象或人物驱动的属性；而视觉-语言模型虽然能灵活对齐文本和图像，但受限于上下文窗口无法直接处理大型目录检索。

Method: 通过蒸馏强大的视觉-语言模型的偏好排名到基于嵌入的系统中，转移其精细对齐能力，同时保持基于嵌入方法的推理时扩展性。

Result: 在人物驱动的产品推荐任务实验中，该方法显著优于现有的基于嵌入的基线方法。

Conclusion: 该方法为个性化文本-图像检索提供了一个高效的解决方案，结合了视觉-语言模型的精细对齐能力和基于嵌入方法的扩展性优势。

Abstract: Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

</details>


### [2] [MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation](https://arxiv.org/abs/2510.12054)
*Wenjin Xie,Tao Jia*

Main category: cs.IR

TL;DR: MIARec模型通过重力方法衡量学者间的相互学术影响力，并将其融入图表示学习的消息传播过程中，同时使用多通道聚合方法捕获异构学术网络的全面特征，在科学论文推荐任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的推荐方法在学术网络中学习图表示时，往往忽略了普遍存在的不对称学术影响力，这限制了推荐系统的精确性和质量。

Method: 提出MIARec模型，采用基于重力的方法测量学者间的相互学术影响力，并将其整合到图表示学习的特征聚合过程中；使用多通道聚合方法同时捕获单个关系子网络的个体嵌入和它们的相互依赖嵌入。

Result: 在真实数据集上的大量实验表明，MIARec模型在三个主要评估指标上均优于基线模型。

Conclusion: MIARec模型通过有效建模学术网络中的相互影响力和异构特征，显著提升了科学论文推荐的性能。

Abstract: With the rapid expansion of scientific literature, scholars increasingly
demand precise and high-quality paper recommendations. Among various
recommendation methodologies, graph-based approaches have garnered attention by
effectively exploiting the structural characteristics inherent in scholarly
networks. However, these methods often overlook the asymmetric academic
influence that is prevalent in scholarly networks when learning graph
representations. To address this limitation, this study proposes the
Mutual-Influence-Aware Recommendation (MIARec) model, which employs a
gravity-based approach to measure the mutual academic influence between
scholars and incorporates this influence into the feature aggregation process
during message propagation in graph representation learning. Additionally, the
model utilizes a multi-channel aggregation method to capture both individual
embeddings of distinct single relational sub-networks and their interdependent
embeddings, thereby enabling a more comprehensive understanding of the
heterogeneous scholarly network. Extensive experiments conducted on real-world
datasets demonstrate that the MIARec model outperforms baseline models across
three primary evaluation metrics, indicating its effectiveness in scientific
paper recommendation tasks.

</details>


### [3] [Reinforced Preference Optimization for Recommendation](https://arxiv.org/abs/2510.12211)
*Junfei Tan,Yuxin Chen,An Zhang,Junguang Jiang,Bin Liu,Ziru Xu,Han Zhu,Jian Xu,Bo Zheng,Xiang Wang*

Main category: cs.IR

TL;DR: 提出ReRe框架，通过约束束搜索和改进奖励设计，解决生成式推荐系统中负样本建模不足和奖励稀疏的问题


<details>
  <summary>Details</summary>
Motivation: 当前生成式推荐系统缺乏高质量负样本建模，依赖隐式奖励，需要强化学习方法来改进

Method: 使用约束束搜索提高采样效率，结合规则准确度奖励和辅助排序奖励进行细粒度监督

Result: 在三个真实数据集上超越传统和LLM-based推荐器，在不同骨干模型和规模上均表现稳健

Conclusion: ReRe框架有效提升生成式推荐性能，并为推荐系统中的RLVR设计空间提供系统见解

Abstract: Recent breakthroughs in large language models (LLMs) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is sparse since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to LLM-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and LLM-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.

</details>


### [4] [An Empirical Study for Representations of Videos in Video Question Answering via MLLMs](https://arxiv.org/abs/2510.12299)
*Zhi Li,Yanan Wang,Hao Niu,Julio Vizcarra,Masato Taya*

Main category: cs.IR

TL;DR: 该论文对多模态大语言模型在视频问答任务中的视频表示方法进行了全面实证研究，评估了单模态和多模态组合在准确性和计算效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视频问答方面取得显著进展，但尚不清楚哪种视频表示方法最有效，以及不同模态如何在任务准确性和计算效率之间取得平衡。

Method: 在VideoMME和LongVideoBench两个基准上系统评估了单模态输入（仅问题、字幕、视觉帧、音频信号）以及多模态组合的表现。

Result: 视觉帧显著提高准确性但带来GPU内存和推理延迟的沉重负担，而字幕提供了轻量级但有效的替代方案，尤其适用于长视频。

Conclusion: 研究揭示了效果与效率之间的明确权衡，为设计资源感知的MLLM视频问答系统提供了实用见解。

Abstract: Multimodal large language models have recently achieved remarkable progress
in video question answering (VideoQA) by jointly processing visual, textual,
and audio information. However, it remains unclear which video representations
are most effective for MLLMs, and how different modalities balance task
accuracy against computational efficiency. In this work, we present a
comprehensive empirical study of video representation methods for VideoQA with
MLLMs. We systematically evaluate single modality inputs question only,
subtitles, visual frames, and audio signals as well as multimodal combinations,
on two widely used benchmarks: VideoMME and LongVideoBench. Our results show
that visual frames substantially enhance accuracy but impose heavy costs in GPU
memory and inference latency, while subtitles provide a lightweight yet
effective alternative, particularly for long videos. These findings highlight
clear trade-offs between effectiveness and efficiency and provide practical
insights for designing resource-aware MLLM-based VideoQA systems.

</details>


### [5] [Causal Inspired Multi Modal Recommendation](https://arxiv.org/abs/2510.12325)
*Jie Yang,Chenyang Gu,Zixuan Liu*

Main category: cs.IR

TL;DR: 提出因果启发的多模态推荐框架，通过双通道跨模态扩散模块识别模态混杂因子，使用后门调整和前门调整解决模态混杂和交互偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了两个关键偏差：模态混杂（潜在因素同时驱动多个模态并影响用户偏好）和交互偏差（真实偏好与曝光效应和偶然点击的噪声混合）。

Method: 引入双通道跨模态扩散模块识别隐藏模态混杂因子，使用后门调整（分层匹配和向量量化码本）阻断混杂路径，应用前门调整结合因果拓扑重建构建去混杂因果子图。

Result: 在三个真实电商数据集上的实验表明，该方法显著优于最先进的基线方法，同时保持强可解释性。

Conclusion: 提出的因果启发多模态推荐框架有效解决了模态混杂和交互偏差问题，在推荐性能和可解释性方面均有显著提升。

Abstract: Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

</details>


### [6] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 研究发现ColBERT等密集检索模型中的单层线性投影存在局限性，通过使用更复杂的FFN、GLU块和跳跃连接等投影块可以显著提升检索性能，最佳变体在多个基准测试中平均提升超过2个NDCG@10点。


<details>
  <summary>Details</summary>
Motivation: 探索MaxSim算子对多向量模型梯度流的影响，发现单层线性投影在训练中存在固有局限性，需要改进投影设计来提升模型性能。

Method: 设计并系统评估替代投影块，包括更深的非线性FFN块、GLU块和跳跃连接，通过消融研究分析各参数对性能的影响。

Result: 许多投影变体优于原始线性投影，最佳变体在跨领域检索基准上平均提升超过2 NDCG@10点，效果在不同随机种子下保持稳定。

Conclusion: 替换ColBERT模型的线性层为更复杂的投影块是一个稳健的即插即用升级方案，能显著提升下游检索性能。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [7] [A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning](https://arxiv.org/abs/2510.12369)
*Yang Xiang,Li Fan,Chenke Yin,Chengtao Ji*

Main category: cs.IR

TL;DR: 提出了一个分层量化框架，通过自加权机制实现任务自适应的多尺度聚合，在保持编码器冻结的同时通过轻量级门控过程调节信息流，实现参数高效的下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 现有图标记化方法（线性化、连续和量化）在适应性和效率方面存在局限，特别是当前基于量化的标记器以固定或任务无关的方式组织层次信息，可能过度表示或未充分利用结构线索，且缺乏在不重新训练编码器的情况下动态重新加权不同层次贡献的能力。

Method: 分层量化框架，引入自加权机制进行任务自适应多尺度聚合，保持编码器冻结，通过轻量级门控过程调节信息流。

Result: 在节点分类和链接预测的基准数据集实验中，在可比较的计算预算下，相对于强基线方法实现了持续改进。

Conclusion: 该方法提供了一种参数高效的方式来适应不同的下游任务，同时保持编码器不变，在多个图学习任务中表现出优越性能。

Abstract: Recent progress in language and vision foundation models demonstrates the
importance of discrete token interfaces that transform complex inputs into
compact sequences for large-scale modeling. Extending this paradigm to graphs
requires a tokenization scheme that handles non-Euclidean structures and
multi-scale dependencies efficiently. Existing approaches to graph
tokenization, linearized, continuous, and quantized, remain limited in
adaptability and efficiency. In particular, most current quantization-based
tokenizers organize hierarchical information in fixed or task-agnostic ways,
which may either over-represent or under-utilize structural cues, and lack the
ability to dynamically reweight contributions from different levels without
retraining the encoder. This work presents a hierarchical quantization
framework that introduces a self-weighted mechanism for task-adaptive
aggregation across multiple scales. The proposed method maintains a frozen
encoder while modulating information flow through a lightweight gating process,
enabling parameter-efficient adaptation to diverse downstream tasks.
Experiments on benchmark datasets for node classification and link prediction
demonstrate consistent improvements over strong baselines under comparable
computational budgets.

</details>


### [8] [Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance](https://arxiv.org/abs/2510.12461)
*Andrei Chernov,Haroon Wahab,Oleg Novitskij*

Main category: cs.IR

TL;DR: TextGCN使用参数无关的图卷积层直接处理基于LLM的物品标题嵌入，结合语言语义和图消息传递，实现了最先进的零样本推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注微调LLMs生成推荐或将LLM嵌入集成到下游模型中，本文探索后一种方向，旨在更好地结合语言语义和图结构。

Method: 提出TextGCN，在LLM生成的物品标题嵌入上应用参数无关的图卷积层；并扩展为TextGCN-MLP，加入可训练的多层感知机和对比损失。

Result: TextGCN在零样本推荐中表现最优，显著超越先前方法；TextGCN-MLP在领域内基准测试中达到最优性能，但零样本性能低于TextGCN。

Conclusion: TextGCN架构有效结合语言语义与图消息传递，在零样本推荐中表现卓越；TextGCN-MLP在领域内性能更优，但存在领域专业化与零样本泛化之间的权衡。

Abstract: In recent years, various approaches have been proposed to leverage large
language models (LLMs) for incorporating textual information about items into
recommender systems. Existing methods primarily focus on either fine-tuning
LLMs to generate recommendations or integrating LLM-based embeddings into
downstream models. In this work, we follow the latter direction and propose
\textbf{TextGCN}, which applies parameter-free graph convolution layers
directly over LLM-based item-title embeddings, instead of learning ID-based
embeddings as in traditional methods. By combining language semantics with
graph message passing, this architecture achieves state-of-the-art zero-shot
performance, significantly outperforming prior approaches. Furthermore, we
introduce \textbf{TextGCN-MLP}, which extends TextGCN with a trainable
multilayer perceptron trained using a contrastive loss, achieving
state-of-the-art in-domain performance on recommendation benchmarks. However,
the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,
highlighting the trade-off between in-domain specialization and zero-shot
generalization. We release our code on github at
\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.

</details>


### [9] [SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch](https://arxiv.org/abs/2510.12604)
*Qihang Zhao,Zhongbo Sun,Xiaoyang Zheng,Xian Guo,Siyuan Wang,Zihan Liang,Mingcan Peng,Ben Chen,Chenyi Lei*

Main category: cs.IR

TL;DR: SMILE是一种基于语义ID融合对齐的物品表示增强方法，通过RQ-OPQ编码量化物品内容和协同信息，解决冷启动物品的协同信息不足问题。


<details>
  <summary>Details</summary>
Motivation: 现代搜索和推荐平台中，冷启动物品的协同信息不足加剧了马太效应，挑战平台多样性。现有方法未能考虑协同与内容之间的不对称性以及物品间的细粒度差异。

Method: 使用RQ-OPQ编码量化物品内容和协同信息，进行两步对齐：RQ编码传递物品间共享的协同信号，OPQ编码学习物品的差异化信息。

Result: 在大规模工业数据集上的离线实验显示SMILE的优越性，在线A/B测试证实显著改进：物品CTR +1.66%，买家数 +1.57%，订单量 +2.17%。

Conclusion: SMILE通过语义ID融合对齐有效解决了冷启动物品表示问题，在工业场景中取得了显著效果提升。

Abstract: With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

</details>


### [10] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 本文系统研究了参数化检索增强生成(PRAG)，发现参数化文档仅捕获部分语义信息，单独使用性能不如文本级交互，但与文本文档结合可提升模型理解能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然参数化检索增强生成(PRAG)作为RAG的新形式受到关注，但其参数注入机制仍不明确，需要系统研究来阐明参数注入的作用。

Method: 通过系统研究PRAG，分析参数化文档的语义捕获能力，比较参数化与文本级交互的效果，并探索两者结合的优势。

Result: 参数化文档仅编码部分语义信息，单独使用性能较差，但与文本文档结合可更有效地利用相关信息，提高对噪声输入的鲁棒性，获得比单一来源更好的性能。

Conclusion: 建议联合使用参数化和文本文档，并提升参数化表示的信息含量以推进PRAG的发展。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


### [11] [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/abs/2510.12709)
*Lin Lin,Jiefeng Long,Zhihe Wan,Yuchi Wang,Dingkang Yang,Shuang Yang,Yueyang Yao,Xu Chen,Zirui Guo,Shengqiang Li,Weiran Li,Hanyu Li,Yaling Mou,Yan Qiu,Haiyang Yu,Xiao Liang,Hongsheng Li,Chao Feng*

Main category: cs.IR

TL;DR: SAIL-Embedding是一个全模态嵌入基础模型，通过多阶段训练策略和架构设计解决现有模型在真实应用中的局限性，在检索任务中达到SOTA性能，并在推荐场景中显著提升用户体验指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态嵌入模型在真实世界应用中的挑战，包括有限模态支持、训练机制不稳定和工业领域差距等问题。

Method: 采用多阶段训练方案：内容感知渐进训练增强模型适应性，协作感知推荐增强训练通过知识蒸馏和用户历史兴趣挖掘优化推荐场景表示，同时开发随机专业化和数据集驱动模式匹配增强训练灵活性和泛化性。

Result: 在检索任务中达到SOTA性能，在抖音精选场景中7天LT增益+0.158%，14天LT增益+0.144%，在抖音信息流排序模型中AUC增益+0.08%。

Conclusion: SAIL-Embedding通过精心设计的训练策略成功解决了多模态嵌入模型在工业应用中的关键问题，在多个真实场景中显著提升了推荐系统的性能指标。

Abstract: Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.

</details>

{"id": "2602.04912", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04912", "abs": "https://arxiv.org/abs/2602.04912", "authors": ["James Gao", "Josh Zhou", "Qi Sun", "Ryan Huang", "Steven Yoo"], "title": "Atomic Information Flow: A Network Flow Model for Tool Attributions in RAG Systems", "comment": null, "summary": "Many tool-based Retrieval Augmented Generation (RAG) systems lack precise mechanisms for tracing final responses back to specific tool components -- a critical gap as systems scale to complex multi-agent architectures. We present \\textbf{Atomic Information Flow (AIF)}, a graph-based network flow model that decomposes tool outputs and LLM calls into atoms: indivisible, self-contained units of information. By modeling LLM orchestration as a directed flow of atoms from tool and LLM nodes to a response super-sink, AIF enables granular attribution metrics for AI explainability.\n  Motivated by the max-flow min-cut theorem in network flow theory, we train a lightweight Gemma3 (4B parameter) language model as a context compressor to approximate the minimum cut of tool atoms using flow signals computed offline by AIF. We note that the base Gemma3-4B model struggles to identify critical information with \\textbf{54.7\\%} accuracy on HotpotQA, barely outperforming lexical baselines (BM25). However, post-training on AIF signals boosts accuracy to \\textbf{82.71\\%} (+28.01 points) while achieving \\textbf{87.52\\%} (+1.85\\%) context token compression -- bridging the gap with the Gemma3-27B variant, a model nearly $7\\times$ larger."}
{"id": "2602.05062", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05062", "abs": "https://arxiv.org/abs/2602.05062", "authors": ["Julian Killingback", "Mahta Rafiee", "Madine Manas", "Hamed Zamani"], "title": "Scaling Laws for Embedding Dimension in Information Retrieval", "comment": "9 Pages, 7 figures", "summary": "Dense retrieval, which encodes queries and documents into a single dense vector, has become the dominant neural retrieval approach due to its simplicity and compatibility with fast approximate nearest neighbor algorithms. As the tasks dense retrieval performs grow in complexity, the fundamental limitations of the underlying data structure and similarity metric -- namely vectors and inner-products -- become more apparent. Prior recent work has shown theoretical limitations inherent to single vectors and inner-products that are generally tied to the embedding dimension. Given the importance of embedding dimension for retrieval capacity, understanding how dense retrieval performance changes as embedding dimension is scaled is fundamental to building next generation retrieval models that balance effectiveness and efficiency. In this work, we conduct a comprehensive analysis of the relationship between embedding dimension and retrieval performance. Our experiments include two model families and a range of model sizes from each to construct a detailed picture of embedding scaling behavior. We find that the scaling behavior fits a power law, allowing us to derive scaling laws for performance given only embedding dimension, as well as a joint law accounting for embedding dimension and model size. Our analysis shows that for evaluation tasks aligned with the training task, performance continues to improve as embedding size increases, though with diminishing returns. For evaluation data that is less aligned with the training task, we find that performance is less predictable, with performance degrading with larger embedding dimensions for certain tasks. We hope our work provides additional insight into the limitations of embeddings and their behavior as well as offers a practical guide for selecting model and embedding dimension to achieve optimal performance with reduced storage and compute costs."}
{"id": "2602.05152", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05152", "abs": "https://arxiv.org/abs/2602.05152", "authors": ["Yuntong Hu", "Sha Li", "Naren Ramakrishnan", "Liang Zhao"], "title": "RAG without Forgetting: Continual Query-Infused Key Memory", "comment": "24 pages, 12 figures", "summary": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed."}
{"id": "2602.05216", "categories": ["cs.IR", "cs.AI", "math.HO"], "pdf": "https://arxiv.org/pdf/2602.05216", "abs": "https://arxiv.org/abs/2602.05216", "authors": ["Luke Alexander", "Eric Leonen", "Sophie Szeto", "Artemii Remizov", "Ignacio Tejeda", "Giovanni Inchiostro", "Vasily Ilin"], "title": "Semantic Search over 9 Million Mathematical Theorems", "comment": "Feedback is welcome", "summary": "Searching for mathematical results remains difficult: most existing tools retrieve entire papers, while mathematicians and theorem-proving agents often seek a specific theorem, lemma, or proposition that answers a query. While semantic search has seen rapid progress, its behavior on large, highly technical corpora such as research-level mathematical theorems remains poorly understood. In this work, we introduce and study semantic theorem retrieval at scale over a unified corpus of $9.2$ million theorem statements extracted from arXiv and seven other sources, representing the largest publicly available corpus of human-authored, research-level theorems. We represent each theorem with a short natural-language description as a retrieval representation and systematically analyze how representation context, language model choice, embedding model, and prompting strategy affect retrieval quality. On a curated evaluation set of theorem-search queries written by professional mathematicians, our approach substantially improves both theorem-level and paper-level retrieval compared to existing baselines, demonstrating that semantic theorem search is feasible and effective at web scale. The theorem search tool is available at \\href{https://huggingface.co/spaces/uw-math-ai/theorem-search}{this link}, and the dataset is available at \\href{https://huggingface.co/datasets/uw-math-ai/TheoremSearch}{this link}."}
{"id": "2602.05334", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05334", "abs": "https://arxiv.org/abs/2602.05334", "authors": ["Dawn Lawrie", "James Mayfield", "Eugene Yang", "Andrew Yates", "Sean MacAvaney", "Ronak Pradeep", "Scott Miller", "Paul McNamee", "Luca Soldaini"], "title": "NeuCLIRTech: Chinese Monolingual and Cross-Language Information Retrieval Evaluation in a Challenging Domain", "comment": "14 pages, 6 figures", "summary": "Measuring advances in retrieval requires test collections with relevance judgments that can faithfully distinguish systems. This paper presents NeuCLIRTech, an evaluation collection for cross-language retrieval over technical information. The collection consists of technical documents written natively in Chinese and those same documents machine translated into English. It includes 110 queries with relevance judgments. The collection supports two retrieval scenarios: monolingual retrieval in Chinese, and cross-language retrieval with English as the query language. NeuCLIRTech combines the TREC NeuCLIR track topics of 2023 and 2024. The 110 queries with 35,962 document judgments provide strong statistical discriminatory power when trying to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included so that developers of reranking algorithms are not reliant on BM25 as their first stage retriever. The dataset and artifacts are released on Huggingface Datasets"}
{"id": "2602.05366", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05366", "abs": "https://arxiv.org/abs/2602.05366", "authors": ["Yichen Tang", "Weihang Su", "Yiqun Liu", "Qingyao Ai"], "title": "Multi-Field Tool Retrieval", "comment": "12 pages, 4 figures", "summary": "Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness."}
{"id": "2602.05408", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05408", "abs": "https://arxiv.org/abs/2602.05408", "authors": ["Zihao Guo", "Ligang Zhou", "Zeyang Tang", "Feicheng Li", "Ying Nie", "Zhiming Peng", "Qingyun Sun", "Jianxin Li"], "title": "Rich-Media Re-Ranker: A User Satisfaction-Driven LLM Re-ranking Framework for Rich-Media Search", "comment": null, "summary": "Re-ranking plays a crucial role in modern information search systems by refining the ranking of initial search results to better satisfy user information needs. However, existing methods show two notable limitations in improving user search satisfaction: inadequate modeling of multifaceted user intents and neglect of rich side information such as visual perception signals. To address these challenges, we propose the Rich-Media Re-Ranker framework, which aims to enhance user search satisfaction through multi-dimensional and fine-grained modeling. Our approach begins with a Query Planner that analyzes the sequence of query refinements within a session to capture genuine search intents, decomposing the query into clear and complementary sub-queries to enable broader coverage of users' potential intents. Subsequently, moving beyond primary text content, we integrate richer side information of candidate results, including signals modeling visual content generated by the VLM-based evaluator. These comprehensive signals are then processed alongside carefully designed re-ranking principle that considers multiple facets, including content relevance and quality, information gain, information novelty, and the visual presentation of cover images. Then, the LLM-based re-ranker performs the holistic evaluation based on these principles and integrated signals. To enhance the scenario adaptability of the VLM-based evaluator and the LLM-based re-ranker, we further enhance their capabilities through multi-task reinforcement learning. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines. Notably, the proposed framework has been deployed in a large-scale industrial search system, yielding substantial improvements in online user engagement rates and satisfaction metrics."}
{"id": "2602.05413", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05413", "abs": "https://arxiv.org/abs/2602.05413", "authors": ["Filip Kuƒçera", "Christoph Mandl", "Isao Echizen", "Radu Timofte", "Timo Spinde"], "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models", "comment": "Under Review - Submitted to SIGIR 2026 Resources Track; 8 pages, 6 figures, 4 tables", "summary": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.\n  Code & datasets are available at https://github.com/Media-Bias-Group/SciDef."}
{"id": "2602.05445", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05445", "abs": "https://arxiv.org/abs/2602.05445", "authors": ["Sebastian Bruch", "Martino Fontana", "Franco Maria Nardini", "Cosimo Rulli", "Rossano Venturini"], "title": "Forward Index Compression for Learned Sparse Retrieval", "comment": null, "summary": "Text retrieval using learned sparse representations of queries and documents has, over the years, evolved into a highly effective approach to search. It is thanks to recent advances in approximate nearest neighbor search-with the emergence of highly efficient algorithms such as the inverted index-based Seismic and the graph-based Hnsw-that retrieval with sparse representations became viable in practice. In this work, we scrutinize the efficiency of sparse retrieval algorithms and focus particularly on the size of a data structure that is common to all algorithmic flavors and that constitutes a substantial fraction of the overall index size: the forward index. In particular, we seek compression techniques to reduce the storage footprint of the forward index without compromising search quality or inner product computation latency. In our examination with various integer compression techniques, we report that StreamVByte achieves the best trade-off between memory footprint, retrieval accuracy, and latency. We then improve StreamVByte by introducing DotVByte, a new algorithm tailored to inner product computation. Experiments on MsMarco show that our improvements lead to significant space savings while maintaining retrieval efficiency."}
{"id": "2602.05474", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05474", "abs": "https://arxiv.org/abs/2602.05474", "authors": ["Yicheng Di", "Zhanjie Zhang", "Yun Wangc", "Jinren Liue", "Jiaqi Yanf", "Jiyu Wei", "Xiangyu Chend", "Yuan Liu"], "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation", "comment": null, "summary": "Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\\% performance improvement."}
{"id": "2602.05663", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05663", "abs": "https://arxiv.org/abs/2602.05663", "authors": ["Shiteng Cao", "Junda She", "Ji Liu", "Bin Zeng", "Chengcheng Guo", "Kuo Cai", "Qiang Luo", "Ruiming Tang", "Han Li", "Kun Gai", "Zhiheng Li", "Cheng Yang"], "title": "GLASS: A Generative Recommender for Long-sequence Modeling via SID-Tier and Semantic Search", "comment": "10 pages,3 figures", "summary": "Leveraging long-term user behavioral patterns is a key trajectory for enhancing the accuracy of modern recommender systems. While generative recommender systems have emerged as a transformative paradigm, they face hurdles in effectively modeling extensive historical sequences. To address this challenge, we propose GLASS, a novel framework that integrates long-term user interests into the generative process via SID-Tier and Semantic Search. We first introduce SID-Tier, a module that maps long-term interactions into a unified interest vector to enhance the prediction of the initial SID token. Unlike traditional retrieval models that struggle with massive item spaces, SID-Tier leverages the compact nature of the semantic codebook to incorporate cross features between the user's long-term history and candidate semantic codes. Furthermore, we present semantic hard search, which utilizes generated coarse-grained semantic ID as dynamic keys to extract relevant historical behaviors, which are then fused via an adaptive gated fusion module to recalibrate the trajectory of subsequent fine-grained tokens. To address the inherent data sparsity in semantic hard search, we propose two strategies: semantic neighbor augmentation and codebook resizing. Extensive experiments on two large-scale real-world datasets, TAOBAO-MM and KuaiRec, demonstrate that GLASS outperforms state-of-the-art baselines, achieving significant gains in recommendation quality. Our codes are made publicly available to facilitate further research in generative recommendation."}
{"id": "2602.05734", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05734", "abs": "https://arxiv.org/abs/2602.05734", "authors": ["Niall McCarroll", "Kevin Curran", "Eugene McNamee", "Angela Clist", "Andrew Brammer"], "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval", "comment": null, "summary": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.\n  This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases."}
{"id": "2602.05787", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05787", "abs": "https://arxiv.org/abs/2602.05787", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Wenbo Yang", "Daiting Shi", "Xueqi Cheng"], "title": "Bagging-Based Model Merging for Robust General Text Embeddings", "comment": "12 pages, 4 figures", "summary": "General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings."}
{"id": "2602.05945", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.05945", "abs": "https://arxiv.org/abs/2602.05945", "authors": ["Zhouhang Xie", "Bo Peng", "Zhankui He", "Ziqi Chen", "Alice Han", "Isabella Ye", "Benjamin Coleman", "Noveen Sachdeva", "Fernando Pereira", "Julian McAuley", "Wang-Cheng Kang", "Derek Zhiyuan Cheng", "Beidou Wang", "Randolph Brown"], "title": "AgenticTagger: Structured Item Representation for Recommendation with LLM Agents", "comment": null, "summary": "High-quality representations are a core requirement for effective recommendation. In this work, we study the problem of LLM-based descriptor generation, i.e., keyphrase-like natural language item representation generation frameworks with minimal constraints on downstream applications. We propose AgenticTagger, a framework that queries LLMs for representing items with sequences of text descriptors. However, open-ended generation provides little control over the generation space, leading to high cardinality, low-performance descriptors that renders downstream modeling challenging. To this end, AgenticTagger features two core stages: (1) a vocabulary building stage where a set of hierarchical, low-cardinality, and high-quality descriptors is identified, and (2) a vocabulary assignment stage where LLMs assign in-vocabulary descriptors to items. To effectively and efficiently ground vocabulary in the item corpus of interest, we design a multi-agent reflection mechanism where an architect LLM iteratively refines the vocabulary guided by parallelized feedback from annotator LLMs that validates the vocabulary against item data. Experiments on public and private data show AgenticTagger brings consistent improvements across diverse recommendation scenarios, including generative and term-based retrieval, ranking, and controllability-oriented, critique-based recommendation."}
{"id": "2602.05975", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05975", "abs": "https://arxiv.org/abs/2602.05975", "authors": ["Tiansheng Hu", "Yilun Zhao", "Canyu Zhang", "Arman Cohan", "Chen Zhao"], "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents", "comment": "Submission to ACL ARR 2026 January", "summary": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively."}

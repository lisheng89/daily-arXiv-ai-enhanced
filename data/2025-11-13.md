<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 3]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Efficient Model-Agnostic Continual Learning for Next POI Recommendation](https://arxiv.org/abs/2511.08941)
*Chenhao Wang,Shanshan Feng,Lisi Chen,Fan Li,Shuo Shang*

Main category: cs.IR

TL;DR: GIRAM是一个用于持续下一个兴趣点推荐的模型无关框架，通过整合上下文感知的持续兴趣和近期兴趣，在动态适应不断变化的用户行为的同时保持历史知识，实现了高效的内存和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的下一个兴趣点推荐方法大多依赖静态数据集和固定模型，无法适应随时间变化的用户行为。需要一种能够持续更新、捕捉用户兴趣变化同时保留历史知识的动态推荐系统。

Method: 提出GIRAM框架，包含四个组件：兴趣记忆库、上下文感知键编码模块、基于生成式键的检索模块、自适应兴趣更新与融合模块。该框架可与现有推荐模型无缝集成。

Result: 在三个真实世界数据集上的实验表明，GIRAM在更新时间和内存消耗方面保持高效率的同时，持续优于最先进的方法。

Conclusion: GIRAM为解决持续下一个POI推荐任务提供了一种有效且高效的解决方案，能够动态适应变化的用户兴趣，同时保持推荐性能。

Abstract: Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.

</details>


### [2] [NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning](https://arxiv.org/abs/2511.09250)
*Jiyuan Wang,Li Zhang,Haipeng Lin,Qile Liu,Gan Huang,Ziyu Li,Zhen Liang,Xia Wu*

Main category: cs.IR

TL;DR: NeuroCLIP是一个针对EEG-图像对比学习的提示调优框架，通过双流视觉嵌入、视觉提示令牌和神经科学启发的对比损失，显著提升了EEG到图像检索的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将CLIP视为静态特征提取器，忽视了其对神经表征的适应性以及EEG-图像对齐中固有的生理-符号鸿沟。

Method: 1. 双流视觉嵌入管道结合动态过滤和令牌级融合生成实例级自适应提示；2. 首次在EEG-图像对齐中引入视觉提示令牌；3. 提出基于神经科学原理的改进对比损失。

Result: 在THINGS-EEG2数据集上，NeuroCLIP在零样本图像检索中达到63.2%的Top-1准确率，比之前最佳方法提升12.3%，并在跨被试条件下表现出强泛化能力。

Conclusion: 生理感知的提示调优在连接脑信号和视觉语义方面具有巨大潜力。

Abstract: Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality- level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.

</details>


### [3] [Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction](https://arxiv.org/abs/2511.09329)
*Andreas Konstantin Kruff,Christin Katharina Kreutz,Timo Breuer,Philipp Schaer,Krisztian Balog*

Main category: cs.IR

TL;DR: Sim4IA-Bench是首个IR社区的用户模拟基准测试套件，包含160个真实搜索会话和模拟器运行数据，用于评估下一查询预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏成熟的度量和基准，验证用户模拟器是否准确反映真实用户行为具有挑战性。

Method: 构建包含160个真实搜索会话的数据集，其中70个会话包含最多62个模拟器运行，分为任务A和任务B来预测用户的下一个搜索查询或话语。

Result: 开发了首个公开可用的基准测试套件，将真实搜索会话与模拟的下一查询预测联系起来，并引入了新的评估度量方法。

Conclusion: Sim4IA-Bench为评估用户模拟方法提供了基础，促进了可重复研究，并推动了信息访问中现实和可解释用户模拟的进一步发展。

Abstract: Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR com- munity. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simu- lation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query pre- dictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.

</details>

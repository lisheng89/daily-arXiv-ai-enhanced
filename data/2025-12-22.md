<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [A Reproducible and Fair Evaluation of Partition-aware Collaborative Filtering](https://arxiv.org/abs/2512.17015)
*Domenico De Gioia,Claudio Pomo,Ludovico Boratto,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 本文对FPSR/FPSR+分区感知协同过滤模型进行了可复现基准测试，发现该模型系列虽具竞争力但并非始终最优，在长尾场景优势明显，揭示了分区、全局组件和枢纽设计带来的准确率-覆盖率权衡。


<details>
  <summary>Details</summary>
Motivation: 基于相似度的协同过滤模型虽然离线性能强且概念简单，但维护密集物品相似度矩阵的二次成本限制了可扩展性。分区感知范式在平衡效果和效率方面显示出潜力，但现有FPSR/FPSR+评估存在可复现性问题，包括使用来源不明的数据分割和遗漏基线模型，难以进行公平比较。

Method: 建立透明、完全可复现的FPSR和FPSR+基准测试框架，对分区感知协同过滤模型进行系统性评估。重点关注模型在不同场景下的性能表现，特别是长尾推荐情况，分析分区策略、全局组件和枢纽设计对推荐效果的影响。

Result: FPSR模型系列并未始终达到最高性能水平，但整体保持竞争力，验证了其设计选择的有效性。在长尾推荐场景中表现出显著优势。研究揭示了分区策略、全局组件和枢纽设计带来的准确率-覆盖率权衡关系。

Conclusion: 分区感知相似度建模在特定场景下最为有益，特别是在长尾推荐中具有优势。研究为可扩展推荐系统设计提供了可操作的指导，并强调了在可复现协议下进行模型评估的重要性。准确率-覆盖率的权衡是分区策略设计中的关键考量因素。

Abstract: Similarity-based collaborative filtering (CF) models have long demonstrated strong offline performance and conceptual simplicity. However, their scalability is limited by the quadratic cost of maintaining dense item-item similarity matrices. Partitioning-based paradigms have recently emerged as an effective strategy for balancing effectiveness and efficiency, enabling models to learn local similarities within coherent subgraphs while maintaining a limited global context. In this work, we focus on the Fine-tuning Partition-aware Similarity Refinement (FPSR) framework, a prominent representative of this family, as well as its extension, FPSR+. Reproducible evaluation of partition-aware collaborative filtering remains challenging, as prior FPSR/FPSR+ reports often rely on splits of unclear provenance and omit some similarity-based baselines, thereby complicating fair comparison. We present a transparent, fully reproducible benchmark of FPSR and FPSR+. Based on our results, the family of FPSR models does not consistently perform at the highest level. Overall, it remains competitive, validates its design choices, and shows significant advantages in long-tail scenarios. This highlights the accuracy-coverage trade-offs resulting from partitioning, global components, and hub design. Our investigation clarifies when partition-aware similarity modeling is most beneficial and offers actionable guidance for scalable recommender system design under reproducible protocols.

</details>


### [2] [Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations](https://arxiv.org/abs/2512.17027)
*Erica Coppolillo,Simone Mungari*

Main category: cs.IR

TL;DR: 对维基百科和Grokipedia两大百科平台的搜索引擎进行首次比较分析，发现两者都会频繁产生与查询弱相关的结果，甚至从无害查询中呈现意外内容，但两个系统对相同查询的推荐结果存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的百科全书Grokipedia的出现，搜索引擎在不同百科系统中的行为尚未得到充分研究。本研究旨在填补这一空白，首次比较分析维基百科和Grokipedia的搜索引擎机制。

Method: 使用近10,000个中性英文单词及其子字符串作为查询，收集超过70,000个搜索引擎结果，分析它们的语义对齐度、重叠度和主题结构。通过主题标注和轨迹分析，进一步研究内容类别的呈现方式和搜索结果在多阶段探索中的演变。

Result: 发现两个平台都会频繁产生与原始查询弱相关的结果，且经常从无害查询中呈现意外内容。尽管有这些共同特性，两个系统对相同查询的推荐结果存在显著差异。主题分布和查询建议方面存在系统性差异。

Conclusion: 意外搜索结果在维基百科和Grokipedia中都是常见现象，但两个平台在主题分布和查询建议方面存在明显差异，揭示了不同百科系统搜索引擎行为的系统性区别。

Abstract: Encyclopedic knowledge platforms are key gateways through which users explore information online. The recent release of Grokipedia, a fully AI-generated encyclopedia, introduces a new alternative to traditional, well-established platforms like Wikipedia. In this context, search engine mechanisms play an important role in guiding users exploratory paths, yet their behavior across different encyclopedic systems remains underexplored. In this work, we address this gap by providing the first comparative analysis of search engine in Wikipedia and Grokipedia.
  Using nearly 10,000 neutral English words and their substrings as queries, we collect over 70,000 search engine results and examine their semantic alignment, overlap, and topical structure. We find that both platforms frequently generate results that are weakly related to the original query and, in many cases, surface unexpected content starting from innocuous queries. Despite these shared properties, the two systems often produce substantially different recommendation sets for the same query. Through topical annotation and trajectory analysis, we further identify systematic differences in how content categories are surfaced and how search engine results evolve over multiple stages of exploration.
  Overall, our findings show that unexpected search engine outcomes are a common feature of both the platforms, even though they exhibit discrepancies in terms of topical distribution and query suggestions.

</details>


### [3] [TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval](https://arxiv.org/abs/2512.17164)
*Yu Yang,Feng Tian,Ping Chen*

Main category: cs.IR

TL;DR: TCDE提出了一种基于大语言模型的主题中心双扩展策略，同时在查询和文档两端进行扩展，通过建立语义桥梁来改善检索效果。


<details>
  <summary>Details</summary>
Motivation: 传统的查询扩展和文档扩展通常单独应用，这可能导致扩展后的查询（或文档）与相关文档（或查询）之间的语义不对齐。为了解决这个严重问题，需要一种能够同时考虑查询和文档扩展的方法。

Method: TCDE使用大语言模型进行主题中心的双扩展：在查询端，LLM识别查询中的不同子主题并为每个子主题生成伪文档；在文档端，LLM将文档提炼为一组核心主题句。这些输出用于扩展原始查询和文档。

Result: 在TREC Deep Learning和BEIR两个具有挑战性的基准测试中，TCDE相比现有的扩展基线方法取得了显著改进。特别是在密集检索任务中，在SciFact数据集上NDCG@10相对提升了2.8%。

Conclusion: 实验结果表明，主题中心和双扩展策略是有效的，通过建立查询和相关文档之间的语义桥梁，能够更好地对齐下游检索模型。

Abstract: Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.

</details>


### [4] [Warmer for Less: A Cost-Efficient Strategy for Cold-Start Recommendations at Pinterest](https://arxiv.org/abs/2512.17277)
*Saeed Ebrahimi,Weijie Jiang,Jaewon Yang,Olafur Gudmundsson,Yucheng Tu,Huizhong Duan*

Main category: cs.IR

TL;DR: Pinterest提出了一套轻量级解决方案来改善冷启动物品推荐，包括残差连接、分数正则化和流形混合技术，在仅增加5%参数的情况下提升了10%新鲜内容参与度。


<details>
  <summary>Details</summary>
Motivation: Pinterest作为视觉发现平台，推荐系统对用户体验至关重要。冷启动物品由于训练数据稀少，在工业级推荐系统中表现不佳，但现有研究很少能有效解决平台规模下的根本问题。

Method: 针对冷启动物品的四个核心挑战提出对应解决方案：1) 设计轻量级架构（仅增加5%参数）；2) 为非历史特征引入残差连接提升重要性；3) 加入分数正则化项缓解冷启动物品得分偏低问题；4) 应用流形混合技术解决标签稀疏性。

Result: 该方案在Pinterest平台上部署后，新鲜内容参与度提升了10%，同时不影响整体参与度和成本，已为超过5.7亿用户提供服务。

Conclusion: 通过针对冷启动物品的系统性解决方案，Pinterest成功提升了推荐系统中新鲜内容的曝光和参与度，证明了在工业规模下有效处理冷启动问题的可行性。

Abstract: Pinterest is a leading visual discovery platform where recommender systems (RecSys) are key to delivering relevant, engaging, and fresh content to our users. In this paper, we study the problem of improving RecSys model predictions for cold-start (CS) items, which appear infrequently in the training data. Although this problem is well-studied in academia, few studies have addressed its root causes effectively at the scale of a platform like Pinterest. By investigating live traffic data, we identified several challenges of the CS problem and developed a corresponding solution for each: First, industrial-scale RecSys models must operate under tight computational constraints. Since CS items are a minority, any related improvements must be highly cost-efficient. To address this, our solutions were designed to be lightweight, collectively increasing the total parameters by only 5%. Second, CS items are represented only by non-historical (e.g., content or attribute) features, which models often treat as less important. To elevate their significance, we introduce a residual connection for the non-historical features. Third, CS items tend to receive lower prediction scores compared to non-CS items, reducing their likelihood of being surfaced. We mitigate this by incorporating a score regularization term into the model. Fourth, the labels associated with CS items are sparse, making it difficult for the model to learn from them. We apply the manifold mixup technique to address this data sparsity. Implemented together, our methods increased fresh content engagement at Pinterest by 10% without negatively impacting overall engagement and cost, and have been deployed to serve over 570 million users on Pinterest.

</details>


### [5] [The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability](https://arxiv.org/abs/2512.17389)
*Guangneng Hu*

Main category: cs.IR

TL;DR: LRWorld是一个包含38K样本、23M token的推荐系统基准测试，用于评估LLM在推荐任务中的能力边界，涵盖关联性、个性化、知识性三个维度，包含10个因素和31个任务。


<details>
  <summary>Details</summary>
Motivation: LLM在推荐系统中作为知识增强器或零样本排序器显示出潜力，但LLM（语言世界知识）与推荐系统（个性化行为世界）之间存在巨大语义鸿沟。研究社区缺乏一个全面的基准来评估LLM在推荐系统中的局限性和边界，以便得出可靠结论。

Method: 提出LRWorld基准，从广泛使用的公共推荐数据集中精心编译和生成超过38K个高质量样本和23M token。将LLM在推荐系统中的心智世界分为三个主要尺度（关联性、个性化、知识性），涵盖10个因素和31个测量任务。

Result: 对数十个LLM的综合实验表明：1) LLM仍不能很好地捕捉深度神经个性化嵌入，但在浅层基于记忆的物品相似性上表现良好；2) 在推断用户兴趣时，LLM擅长感知物品实体关系、实体层次分类和物品关联规则；3) LLM在多模态知识推理（电影海报和产品图像）和对噪声配置文件的鲁棒性方面显示出有前景的能力；4) 没有LLM在10个因素上表现一致良好；5) 模型大小、位置偏差等因素被消融分析。

Conclusion: LRWorld基准为评估LLM在推荐系统中的能力提供了全面框架，揭示了LLM在个性化推荐方面的当前局限性和优势领域，为未来研究提供了重要参考。

Abstract: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.

</details>


### [6] [A Systematic Reproducibility Study of BSARec for Sequential Recommendation](https://arxiv.org/abs/2512.17442)
*Jan Hutter,Hua Chang Bakker,Stan Fris,Madelon Bernardy,Yuanna Liu*

Main category: cs.IR

TL;DR: BSARec通过傅里叶变换增强Transformer的高频信号捕捉能力，但研究发现其效果有限，简单残差连接与数字信号处理方法效果相当，非恒定填充策略对性能提升更关键。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在序列推荐中作为低通滤波器，难以捕捉反映用户短期兴趣的高频信号，需要改进模型的高频信号处理能力。

Method: 1. 复现BSARec模型，在Transformer编码器中加入频率层，通过傅里叶变换重新缩放高频分量；2. 提出量化用户历史频率的指标，评估不同用户组的推荐性能；3. 比较不同数字信号处理技术（傅里叶变换vs离散小波变换）；4. 探索不同的填充策略。

Result: 1. BSARec在某些数据集上优于其他序列推荐方法；2. 离散小波变换相比傅里叶变换仅有轻微改进；3. 数字信号处理方法相比简单残差连接没有明显优势；4. 非恒定填充策略显著提升推荐性能，而恒定填充会阻碍频率重缩放器捕捉高频信号。

Conclusion: BSARec的高频增强机制效果有限，实际性能提升主要来自非恒定填充策略而非复杂的数字信号处理技术，简单残差连接可能已足够捕捉高频信号。

Abstract: In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.

</details>


### [7] [Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application](https://arxiv.org/abs/2512.17462)
*Olivier Jeunen,Schaun Wheeler*

Main category: cs.IR

TL;DR: 评估金融应用中基于智能体的个性化消息系统在税务申报期间对用户退订行为和转化时间的影响，结果显示智能体消息使退订事件减少21%并促进提前申报


<details>
  <summary>Details</summary>
Motivation: 营销和产品个性化是信息检索方法在商业领域的重要应用场景，近年来基于智能体的方法逐渐受到关注。本研究旨在评估智能体个性化在金融服务应用客户沟通系统中的行为和留存效果

Method: 在2025年全国税务申报期间，通过为期两个月的随机对照试验，比较基于智能体的消息方法与传统的基于规则的营销系统，重点关注退订行为和转化时间两个主要结果指标

Result: 实证结果显示：智能体主导的消息使退订事件相对于传统系统减少了21%（±0.01），并在国家截止日期前的几周内增加了提前申报行为

Conclusion: 自适应、用户级别的决策系统能够调节参与强度，同时改善长期留存指标，证明了智能体个性化在金融服务沟通中的有效性

Abstract: Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\% ($\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.

</details>


### [8] [Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure](https://arxiv.org/abs/2512.17733)
*Jingmao Zhang,Zhiting Zhao,Yunqi Lin,Jianghong Ma,Tianjun Wei,Haijun Zhang,Xiaofeng Zhang*

Main category: cs.IR

TL;DR: 提出Cadence框架，通过因果去混淆的共购买关系和反事实曝光来增强推荐多样性，同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖共现关系，容易受到商品流行度和用户属性偏差的影响，导致嵌入质量和性能下降。同时，多样性作为推荐质量的关键方面，现有研究关注有限，缺乏因果视角和理论基础。

Method: 1) 计算无偏非对称共购买关系(UACR)，排除商品流行度和用户属性影响，构建去混淆的有向商品图；2) 利用UACR识别与用户交互商品有强因果相关性但尚未接触的多样化商品类别，在高曝光场景下模拟其行为

Result: 在真实数据集上的实验表明，该方法在多样性和准确性方面均优于最先进的多样性模型，验证了其有效性、可迁移性和效率

Conclusion: Cadence框架通过因果去混淆和反事实曝光，能够显著增强推荐多样性同时保持相关性，为推荐系统提供了新的因果视角和理论基础

Abstract: Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.

</details>

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SBAN: A Framework \& Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining](https://arxiv.org/abs/2510.18936)
*Hamed Jelodar,Mohammad Meymani,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: SBAN是一个大规模多维度数据集，包含300多万个样本，涵盖二进制代码、汇编指令、自然语言描述和源代码四个层次，用于提升大语言模型在软件代码分析中的预训练和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代码分析数据集通常只关注单一表示形式，缺乏跨表示形式的学习能力。SBAN旨在通过多模态结构连接低级机器表示和高级人类语义，为构建智能代码推理系统提供基础。

Method: 构建包含300多万个样本的数据集，其中290万个良性样本和67.2万个恶意软件样本，每个样本都提供二进制代码、汇编指令、自然语言描述和源代码四种表示形式。

Result: 创建了一个大规模、多模态的软件代码数据集，支持跨表示学习、软件语义理解和自动化恶意软件检测等研究任务。

Conclusion: SBAN数据集为软件行为挖掘、安全分析改进以及大语言模型在软件代码挖掘中的预训练和微调任务提供了新的机会，是构建智能代码推理系统的坚实基础。

Abstract: This paper introduces SBAN (Source code, Binary, Assembly, and Natural
Language Description), a large-scale, multi-dimensional dataset designed to
advance the pre-training and evaluation of large language models (LLMs) for
software code analysis. SBAN comprises more than 3 million samples, including
2.9 million benign and 672,000 malware respectively, each represented across
four complementary layers: binary code, assembly instructions, natural language
descriptions, and source code. This unique multimodal structure enables
research on cross-representation learning, semantic understanding of software,
and automated malware detection. Beyond security applications, SBAN supports
broader tasks such as code translation, code explanation, and other software
mining tasks involving heterogeneous data. It is particularly suited for
scalable training of deep models, including transformers and other LLM
architectures. By bridging low-level machine representations and high-level
human semantics, SBAN provides a robust foundation for building intelligent
systems that reason about code. We believe that this dataset opens new
opportunities for mining software behavior, improving security analytics, and
enhancing LLM capabilities in pre-training and fine-tuning tasks for software
code mining.

</details>


### [2] [XGen-Q: An Explainable Domain-Adaptive LLM Framework with Retrieval-Augmented Generation for Software Security](https://arxiv.org/abs/2510.19006)
*Hamed Jelodar,Mohammad Meymani,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.IR

TL;DR: XGen-Q是基于Qwen-Coder架构的领域适应大语言模型，专门用于恶意软件检测和分析，通过多阶段提示策略和检索增强生成技术，在复杂代码混淆情况下实现可靠的恶意软件识别和详细取证报告。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测系统对混淆或未知威胁的泛化能力不足，需要更适应性强且可解释的模型。生成式AI和LLM在代码理解方面表现出色，但在网络安全特别是恶意软件检测中的应用仍然有限。

Method: 基于Qwen-Coder架构构建领域适应LLM，在超过100万个恶意软件样本的大规模语料库上进行预训练，涵盖源代码和汇编代码。采用多阶段提示策略结合检索增强生成(RAG)，设计系统暴露模型于多样化混淆模式的训练流程。

Result: 实验结果显示XGen-Q相比竞争基线显著降低了困惑度，并在新型恶意软件样本上表现出强大性能。

Conclusion: 基于LLM的方法在可解释和鲁棒的恶意软件分析方面具有前景，XGen-Q展示了这种方法的有效性。

Abstract: Generative AI and large language models (LLMs) have shown strong capabilities
in code understanding, but their use in cybersecurity, particularly for malware
detection and analysis, remains limited. Existing detection systems often fail
to generalize to obfuscated or previously unseen threats, underscoring the need
for more adaptable and explainable models. To address this challenge, we
introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and
pretrained on a large-scale corpus of over one million malware samples,
spanning both source and assembly code. XGen-Q uses a multi-stage prompt
strategy combined with retrieval-augmented generation (RAG) to deliver reliable
malware identification and detailed forensic reporting, even in the presence of
complex code obfuscation. To further enhance generalization, we design a
training pipeline that systematically exposes the model to diverse obfuscation
patterns. Experimental results show that XGen-Q achieves significantly lower
perplexity than competitive baselines and exhibits strong performance on novel
malware samples, demonstrating the promise of LLM-based approaches for
interpretable and robust malware analysis.

</details>


### [3] [C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers for Generative Search](https://arxiv.org/abs/2510.19221)
*Yingchen Zhang,Ruqing Zhang,Jiafeng Guo,Wenjun Peng,Sen Li,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: C2T-ID是一种结合数值标识符和文本标识符优势的文档标识符设计方法，通过层次聚类构建语义数值docid，再用高频关键词替换数值标签，平衡语义表达能力和搜索空间约束。


<details>
  <summary>Details</summary>
Motivation: 解决生成式检索中文档标识符设计的挑战：数值标识符无法利用大语言模型的自然语言理解能力，而纯文本标识符会扩大解码空间且对早期错误敏感。

Method: 1) 通过层次聚类构建语义数值docid；2) 提取高频元数据关键词并迭代替换数值标签；3) 可选的两级语义平滑步骤增强C2T-ID的流畅性。

Result: 在Natural Questions和淘宝产品搜索上的实验表明，C2T-ID显著优于原子标识符、语义码本和纯文本docid基线方法。

Conclusion: C2T-ID在平衡语义表达能力和搜索空间约束方面表现出色，有效解决了生成式检索中的文档标识符设计问题。

Abstract: Designing document identifiers (docids) that carry rich semantic information
while maintaining tractable search spaces is a important challenge in
generative retrieval (GR). Popular codebook methods address this by building a
hierarchical semantic tree and constraining generation to its child nodes, yet
their numeric identifiers cannot leverage the large language model's pretrained
natural language understanding. Conversely, using text as docid provides more
semantic expressivity but inflates the decoding space, making the system
brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i)
first construct semantic numerical docid via hierarchical clustering; (ii) then
extract high-frequency metadata keywords and iteratively replace each numeric
label with its cluster's top-K keywords; and (iii) an optional two-level
semantic smoothing step further enhances the fluency of C2T-ID. Experiments on
Natural Questions and Taobao's product search demonstrate that C2T-ID
significantly outperforms atomic, semantic codebook, and pure-text docid
baselines, demonstrating its effectiveness in balancing semantic expressiveness
with search space constraints.

</details>


### [4] [CoRECT: A Framework for Evaluating Embedding Compression Techniques at Scale](https://arxiv.org/abs/2510.19340)
*L. Caspari,M. Dinzinger,K. Gosh Dastidar,C. Fellicious,J. Mitrović,M. Granitzer*

Main category: cs.IR

TL;DR: CoRECT框架用于大规模评估嵌入压缩方法，在100M段落上非学习压缩方法能显著减小索引大小且性能损失统计不显著，但最佳压缩方法选择仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略语料复杂性对稠密检索性能的影响，而语料大小和文档长度是重要因素，需要系统评估压缩方法在不同语料上的表现。

Method: 提出CoRECT框架，包含新策划的数据集集合，对8种代表性压缩方法进行基准测试，支持大规模嵌入压缩评估。

Result: 非学习压缩方法在100M段落上能实现显著的索引大小减少，性能损失统计不显著，但不同模型间性能存在差异。

Conclusion: 压缩方法选择具有挑战性，需要CoRECT框架来确保一致比较和明智选择，所有代码、数据和结果已开源。

Abstract: Dense retrieval systems have proven to be effective across various
benchmarks, but require substantial memory to store large search indices.
Recent advances in embedding compression show that index sizes can be greatly
reduced with minimal loss in ranking quality. However, existing studies often
overlook the role of corpus complexity -- a critical factor, as recent work
shows that both corpus size and document length strongly affect dense retrieval
performance. In this paper, we introduce CoRECT (Controlled Retrieval
Evaluation of Compression Techniques), a framework for large-scale evaluation
of embedding compression methods, supported by a newly curated dataset
collection. To demonstrate its utility, we benchmark eight representative types
of compression methods. Notably, we show that non-learned compression achieves
substantial index size reduction, even on up to 100M passages, with
statistically insignificant performance loss. However, selecting the optimal
compression method remains challenging, as performance varies across models.
Such variability highlights the necessity of CoRECT to enable consistent
comparison and informed selection of compression methods. All code, data, and
results are available on GitHub and HuggingFace.

</details>


### [5] [Top-P Masking for Cross Language Information Retrieval](https://arxiv.org/abs/2510.19758)
*Joseph Casale,Andrew Silverschotz,Joseph DeSimone*

Main category: cs.IR

TL;DR: 提出使用Top-P动态掩码（类似于大语言模型中的核采样）替代Top-K掩码方案，在跨语言信息检索任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: Top-K掩码方案被提出作为信息检索任务中促进稀疏表示的简单替代方法，但现有算法如BLADE仅将其作为后处理阶段使用。

Method: 采用Top-P动态掩码方法，类似于大语言模型中的核采样技术，替代传统的Top-K掩码方案。

Result: 在跨语言信息检索任务中，Top-P动态掩码方法比Top-K掩码表现更好。

Conclusion: Top-P动态掩码是比Top-K掩码更有效的稀疏表示方法，在跨语言信息检索领域具有更好的性能。

Abstract: Top-K masking schemes have been proposed as a method to promote sparse
representations in Information Retrieval (IR) tasks, as a simple alternative to
Floating Point Operations per Second (FLOPS) regularization. Algorithms such as
Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as
a post-processing stage. We propose using Top-P Dynamic Masking similar to
Nucleus Sampling in Large Language Models, and demonstrate better performance
than Top-K masking. Specifically, we evaluate our methods in the domain of
Cross Language Information Retrieval (CLIR)

</details>

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation](https://arxiv.org/abs/2601.21105)
*Joyce Zhou,Weijie Zhou,Doug Turnbull,Thorsten Joachims*

Main category: cs.IR

TL;DR: 本文介绍了SteerEval评估框架，用于测试自然语言推荐系统对用户引导指令的响应能力，超越了传统基于属性的评估，涵盖了更丰富的引导形式。


<details>
  <summary>Details</summary>
Motivation: 自然语言用户档案虽然提高了可解释性和可引导性，但现有评估方法主要关注明显属性（如电影类型），未能捕捉更丰富的用户控制形式，这限制了推荐系统真正实现用户引导的潜力。

Method: 提出了SteerEval评估框架，使用从类型到内容警告等多种干预措施来测量更细致多样的引导能力，评估了一系列预训练的自然语言推荐器，并比较了不同档案和推荐干预对引导效果的影响。

Result: 评估了自然语言推荐器的引导能力，分析了在相对小众主题上引导的潜力和局限性，比较了不同干预措施对引导效果的影响，为可引导推荐器设计提供了实证基础。

Conclusion: 基于研究结果提出了实用的设计建议，并讨论了可引导推荐器设计的未来方向，强调需要更全面的评估框架来真正实现用户对推荐系统的控制。

Abstract: Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.

</details>


### [2] [A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning](https://arxiv.org/abs/2601.21162)
*Jiate Liu,Zebin Chen,Shaobo Qiao,Mingchen Ju,Danting Zhang,Bocheng Han,Shuyue Yu,Xin Shu,Jingling Wu,Dong Wen,Xin Cao,Guanfeng Liu,Zhengyi Yang*

Main category: cs.IR

TL;DR: A2RAG是一个自适应和智能的图检索增强生成框架，通过动态调整检索策略和结合图结构与文本证据，在降低计算成本的同时提高多跳问答的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统Graph-RAG在实际部署中面临两个主要瓶颈：1) 混合难度工作负载下，固定检索策略要么在简单查询上浪费成本，要么无法处理困难的多跳问题；2) 提取损失问题，图抽象会丢失源文本中的细粒度限定信息。

Method: A2RAG结合自适应控制器和智能检索器：控制器验证证据充分性并仅在必要时触发针对性细化；检索器逐步提升检索力度，并将图信号映射回原始文本，以应对提取损失和不完整图结构。

Result: 在HotpotQA和2WikiMultiHopQA数据集上的实验显示，A2RAG在Recall@2指标上分别获得+9.9和+11.8的绝对提升，同时将令牌消耗和端到端延迟降低约50%。

Conclusion: A2RAG通过自适应控制和智能检索机制，有效解决了Graph-RAG在实际部署中的成本和可靠性问题，为多跳问答提供了高效且鲁棒的解决方案。

Abstract: Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.

</details>


### [3] [Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance](https://arxiv.org/abs/2601.21611)
*Baopu Qiu,Hao Chen,Yuanrong Wu,Changtong Zan,Chao Wei,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: 提出MPCoT+LRKD框架，通过多视角思维链推理和潜在推理知识蒸馏，提升电商搜索相关性建模的准确性和实时性


<details>
  <summary>Details</summary>
Motivation: 现有LLM相关性模型存在两个关键局限：1) 单视角思维链推理无法捕捉电商相关性的多维度特性；2) 知识蒸馏方法丢弃了推理结构，只作为辅助信号，丧失了推理效用

Method: 提出多视角思维链(MPCoT)生成多样化推理，结合SFT和DPO构建更强推理器；引入潜在推理知识蒸馏(LRKD)，为学生模型配备轻量级推理提取器，实现高效低延迟推理

Result: 在服务数千万用户的电商搜索广告平台上进行离线和在线A/B测试，方法在商业性能和用户体验方面均显示出显著优势

Conclusion: MPCoT+LRKD框架通过充分利用思维链语义，解决了电商搜索相关性建模中的多维度推理和实时部署挑战，实现了准确性和效率的双重提升

Abstract: Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.

</details>


### [4] [Influence Guided Sampling for Domain Adaptation of Text Retrievers](https://arxiv.org/abs/2601.21759)
*Meet Doshi,Vishwajeet Kumar,Yulong Li,Jaydeep Sen*

Main category: cs.IR

TL;DR: Inf-DDS：一种基于强化学习的轻量级自适应训练数据采样框架，通过影响力奖励信号动态调整数据集权重，显著提升检索模型性能并降低GPU计算成本


<details>
  <summary>Details</summary>
Motivation: 通用开放域稠密检索系统通常使用多样化的语料库和搜索任务进行训练，但如何最优地采样这些训练数据尚未得到充分研究。传统的均匀采样、按实例数量比例采样或依赖专家监督的方法存在局限，训练数据采样策略对模型性能有重大影响。

Method: 提出Inf-DDS框架，使用强化学习自适应地重新加权训练数据集。该框架通过影响力奖励信号指导，迭代优化采样策略，优先选择能最大化目标开发集性能的数据集。相比现有梯度采样方法，该方法GPU消耗更轻量。

Result: 在广泛的文本检索任务中，Inf-DDS显著提升了检索性能：训练多语言bge-m3模型时NDCG@10绝对提升5.03，训练all-MiniLM-L6-v2时NDCG@10绝对提升0.94。同时GPU计算成本降低1.5-4倍，即使从专家分配的权重开始训练也能获得改进。

Conclusion: Inf-DDS提供了一种高效、轻量级的自适应训练数据采样方法，能够显著提升稠密检索模型的性能并降低计算成本，为大规模混合数据集训练提供了有效的解决方案。

Abstract: General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.

</details>


### [5] [OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce](https://arxiv.org/abs/2601.21770)
*Kun Zhang,Jingming Zhang,Wei Cheng,Yansong Cheng,Jiaqi Zhang,Hao Lu,Xu Zhang,Haixiang Gan,Jiangxia Cao,Tenglong Wang,Ximing Zhang,Boyang Xia,Kuo Cai,Shiyao Wang,Hongjian Dou,Jinkai Yu,Mingxing Wen,Qiang Luo,Dongxu Liang,Chenyi Lei,Jun Wang,Runan Liu,Zhaojie Liu,Ruiming Tang,Tingting Gao,Shaoguo Liu,Yuqing Ding,Hui Kong,Han Li,Guorui Zhou,Wenwu Ou,Kun Gai*

Main category: cs.IR

TL;DR: 快手电商OneMall：端到端生成式推荐框架，统一多场景（商品卡片、短视频、直播）推荐，通过语义分词器、Transformer架构和强化学习实现，显著提升各场景业务指标。


<details>
  <summary>Details</summary>
Motivation: 电商平台存在多种商品分发场景（商品卡片、短视频、直播），传统推荐系统难以统一处理。需要构建端到端的生成式推荐框架来统一这些场景，提升推荐效果和用户体验。

Method: 1. 电商语义分词器：捕获真实世界语义和跨场景业务特定商品关系；2. Transformer架构：使用Query-Former压缩长序列、Cross-Attention融合多行为序列、Sparse MoE实现可扩展自回归生成；3. 强化学习管道：连接检索和排序模型，用排序模型作为奖励信号优化端到端策略检索模型。

Result: 在所有电商场景均取得显著提升：商品卡片GMV提升13.01%，短视频订单提升15.32%，直播订单提升2.78%。已在快手部署，服务超过4亿日活用户。

Conclusion: OneMall成功构建了统一的端到端生成式推荐框架，有效解决了多场景电商推荐问题，显著提升了业务指标，证明了生成式推荐在电商领域的实用价值。

Abstract: In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\% GMV in product-card, +15.32\% Orders in Short-Video, and +2.78\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.

</details>


### [6] [The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation](https://arxiv.org/abs/2601.21805)
*Yuhan Zhao,Weixin Chen,Li Chen,Weike Pan*

Main category: cs.IR

TL;DR: 跨域推荐会加剧群体不公平性，本文提出CDFA框架解决跨域差异转移和信息增益不公平分配问题


<details>
  <summary>Details</summary>
Motivation: 跨域推荐虽然能提升推荐质量，但研究发现会无意中加剧群体层面的不公平性，需要深入分析原因并提出解决方案

Method: 提出跨域公平增强（CDFA）框架：1）通过自适应整合未标记数据平衡跨组训练信号信息量，缓解跨域差异转移；2）通过信息论方法重新分配跨域信息增益，确保公平利益分配

Result: 在多个数据集和基线方法上的实验表明，CDFA框架显著减少了跨域推荐的不公平性，同时不牺牲整体推荐性能，甚至有所提升

Conclusion: 本文揭示了跨域推荐中不公平性的根本原因，并提出了有效的解决方案，为实现公平的跨域推荐系统提供了理论和实践基础

Abstract: Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.

</details>


### [7] [LEMUR: Learned Multi-Vector Retrieval](https://arxiv.org/abs/2601.21853)
*Elias Jääsaari,Ville Hyvönen,Teemu Roos*

Main category: cs.IR

TL;DR: LEMUR是一个高效的多向量相似性搜索框架，通过两层问题简化将多向量搜索转化为单向量ANN搜索，速度比现有方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 多向量表示（如ColBERT）在信息检索中质量优于单向量表示，但检索延迟显著增加，需要设计高效的近似最近邻搜索算法来解决多向量搜索的效率问题。

Method: LEMUR采用两层问题简化：1）将多向量相似性搜索转化为可通过单隐藏层神经网络解决的监督学习问题；2）将该模型的推理简化为在其潜在空间中的单向量相似性搜索，从而利用现有单向量ANNS方法加速检索。

Result: LEMUR在ColBERTv2嵌入、现代多向量文本模型和多向量视觉文档检索模型上评估，比早期多向量相似性搜索方法快一个数量级。

Conclusion: LEMUR提供了一个简单而高效的框架，通过将多向量搜索问题转化为单向量ANN搜索，显著提升了多向量检索系统的效率，同时保持了检索质量。

Abstract: Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.

</details>


### [8] [SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation](https://arxiv.org/abs/2601.21986)
*Yu Cui,Feng Liu,Zhaoxiang Wang,Changwang Zhang,Jun Wang,Can Wang,Jiawei Chen*

Main category: cs.IR

TL;DR: 提出SpecTran，一种基于频谱感知的transformer适配器，用于解决传统SR模型中文本信息注入的维度塌陷和频谱信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 传统序列推荐模型仅学习低维项目ID嵌入，忽略了项目标题或描述等文本信息。现有LLM文本嵌入注入方法存在两大问题：适配器方法有维度塌陷问题，SVD方法则丢弃了大量频谱信息。

Method: 提出SpecTran，一种在频谱域操作的频谱感知transformer适配器，关注完整频谱以选择和聚合信息组件。使用可学习的频谱位置编码将奇异值线索作为归纳偏置，引导注意力关注显著频谱组件并促进嵌入维度的多样性。

Result: 在四个真实世界数据集和三个SR骨干网络上，SpecTran始终优于强基线方法，平均提升9.17%。

Conclusion: SpecTran通过频谱感知的transformer适配器有效解决了现有文本嵌入注入方法的局限性，实现了更好的推荐性能。

Abstract: Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.
  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.

</details>


### [9] [LANCER: LLM Reranking for Nugget Coverage](https://arxiv.org/abs/2601.22008)
*Jia-Huei Ju,François G. Landry,Eugene Yang,Suzan Verberne,Andrew Yates*

Main category: cs.IR

TL;DR: LANCER是一种基于LLM的重排序方法，专门针对长格式RAG任务，通过生成子问题并预测文档回答这些子问题的能力，优化信息覆盖而非仅相关性排序。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法主要针对相关性排序优化，而长格式RAG（如自动报告生成）需要覆盖广泛的相关信息，要求检索提供全面的信息覆盖而不仅仅是相关性。

Method: LANCER使用LLM预测信息需求应回答的子问题，预测哪些文档能回答这些子问题，然后重新排序文档以最大化信息块覆盖。

Result: 实验结果显示LANCER在信息块覆盖指标（α-nDCG和信息覆盖率）上优于其他基于LLM的重排序方法，子问题生成在提升性能中起关键作用。

Conclusion: LANCER通过专注于信息覆盖而非仅相关性排序，有效提升了长格式RAG任务的检索质量，子问题生成是实现这一目标的关键组件。

Abstract: Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.

</details>

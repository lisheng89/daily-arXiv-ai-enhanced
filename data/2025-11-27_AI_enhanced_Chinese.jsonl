{"id": "2511.20867", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20867", "abs": "https://arxiv.org/abs/2511.20867", "authors": ["Puneet S. Bagga", "Vivek F. Farias", "Tamar Korkotashvili", "Tianyi Peng", "Yuhang Wu"], "title": "E-GEO: A Testbed for Generative Engine Optimization in E-Commerce", "comment": null, "summary": "With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a \"universally effective\" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.", "AI": {"tldr": "\u63d0\u51fa\u4e86E-GEO\u57fa\u51c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u7535\u5546\u751f\u6210\u5f15\u64ce\u4f18\u5316\u8bbe\u8ba1\u7684\u57fa\u51c6\uff0c\u5305\u542b7000\u591a\u4e2a\u771f\u5b9e\u7684\u591a\u53e5\u6d88\u8d39\u8005\u4ea7\u54c1\u67e5\u8be2\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\u6765\u63d0\u5347\u5185\u5bb9\u5728\u751f\u6210\u5f15\u64ce\u4e2d\u7684\u53ef\u89c1\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u751f\u6210\u5f15\u64ce\u6b63\u5728\u91cd\u5851\u68c0\u7d22\u4efb\u52a1\uff0c\u4f46\u5728\u7535\u5546\u9886\u57df\uff0c\u751f\u6210\u5f15\u64ce\u4f18\u5316\u5b9e\u8df5\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff0c\u5176\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u6784\u5efaE-GEO\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f3015\u79cd\u5e38\u89c1\u6539\u5199\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5236\u5b9aGEO\u4e3a\u53ef\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u4f18\u5316\u7684\u63d0\u793a\u63ed\u793a\u4e86\u4e00\u4e2a\u7a33\u5b9a\u3001\u9886\u57df\u65e0\u5173\u7684\u6a21\u5f0f\uff0c\u8868\u660e\u5b58\u5728'\u666e\u904d\u6709\u6548'\u7684GEO\u7b56\u7565\uff0c\u8be5\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "E-GEO\u4e3a\u7535\u5546\u751f\u6210\u5f15\u64ce\u4f18\u5316\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u666e\u904d\u6709\u6548\u7684\u4f18\u5316\u6a21\u5f0f\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2511.20904", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.20904", "abs": "https://arxiv.org/abs/2511.20904", "authors": ["Mengliang ZHang"], "title": "Generating Querying Code from Text for Multi-Modal Electronic Health Record", "comment": null, "summary": "Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \\textbf{T}ables and clinical \\textbf{T}ext for natural language-to-query \\textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.", "AI": {"tldr": "\u6784\u5efa\u4e86TQGen\u6570\u636e\u96c6\u548cTQGen-EHRQuery\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u5230\u67e5\u8be2\u751f\u6210\uff0c\u901a\u8fc7\u533b\u7597\u77e5\u8bc6\u6a21\u5757\u548c\u95ee\u9898\u6a21\u677f\u5339\u914d\u6a21\u5757\u89e3\u51b3\u590d\u6742\u533b\u5b66\u672f\u8bed\u548c\u591a\u6837\u5316\u67e5\u8be2\u7684\u6311\u6218\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5305\u542b\u5927\u91cf\u7ed3\u6784\u5316\u8868\u683c\u548c\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6587\u672c\uff0c\u67e5\u8be2\u76f8\u5173\u4fe1\u606f\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u5e93\u64cd\u4f5c\uff0c\u589e\u52a0\u4e86\u4e34\u5e8a\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\u3002\u590d\u6742\u7684\u8868\u5173\u7cfb\u548c\u4e13\u4e1a\u672f\u8bed\u9650\u5236\u4e86\u67e5\u8be2\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faTQGen-EHRQuery\u6846\u67b6\uff0c\u5305\u542b\u533b\u7597\u77e5\u8bc6\u6a21\u5757\u548c\u95ee\u9898\u6a21\u677f\u5339\u914d\u6a21\u5757\u3002\u5f15\u5165\u5de5\u5177\u96c6\u6982\u5ff5\uff0c\u5c06\u6587\u672c\u5904\u7406\u6a21\u5757\u5c01\u88c5\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u63d0\u9ad8\u5904\u7406\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u548c\u5de5\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u589e\u5f3aEHR\u7cfb\u7edf\u4fe1\u606f\u67e5\u8be2\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "TQGen\u6570\u636e\u96c6\u548cTQGen-EHRQuery\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u7684\u4fe1\u606f\u67e5\u8be2\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u533b\u5b66\u672f\u8bed\u548c\u591a\u6837\u5316\u67e5\u8be2\u7684\u6311\u6218\u3002"}}
{"id": "2511.21121", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21121", "abs": "https://arxiv.org/abs/2511.21121", "authors": ["Anup Roy", "Rishabh Gyanendra Upadhyay", "Animesh Rameshbhai Panara", "Robin Mills"], "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval", "comment": null, "summary": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.\n  We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.\n  VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.", "AI": {"tldr": "VisionRAG\u662f\u4e00\u4e2a\u514dOCR\u3001\u6a21\u578b\u65e0\u5173\u7684\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u91d1\u5b57\u5854\u7d22\u5f15\u6846\u67b6\u76f4\u63a5\u5904\u7406\u6587\u6863\u56fe\u50cf\uff0c\u4fdd\u7559\u5e03\u5c40\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eOCR\u7684\u6587\u6863\u68c0\u7d22\u6d41\u7a0b\u7ef4\u62a4\u6210\u672c\u9ad8\u3001\u5bf9\u5e03\u5c40\u53d8\u5316\u654f\u611f\u4e14\u5bb9\u6613\u4e22\u5931\u7a7a\u95f4\u7ebf\u7d22\uff0c\u800c\u73b0\u6709\u7684\u89c6\u89c9\u4f18\u5148\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u5f00\u9500\u5927\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u901a\u9053\u91d1\u5b57\u5854\u7d22\u5f15\u6846\u67b6\uff0c\u76f4\u63a5\u5bf9\u6587\u6863\u56fe\u50cf\u8fdb\u884c\u7d22\u5f15\uff0c\u4f7f\u7528\u5168\u5c40\u9875\u9762\u6458\u8981\u3001\u7ae0\u8282\u6807\u9898\u3001\u89c6\u89c9\u70ed\u70b9\u548c\u4e8b\u5b9e\u7ea7\u7ebf\u7d22\u6784\u5efa\u8bed\u4e49\u5411\u91cf\uff0c\u901a\u8fc7\u4e92\u9006\u6392\u5e8f\u878d\u5408\u8fdb\u884c\u68c0\u7d22\u3002", "result": "\u5728\u91d1\u878d\u6587\u6863\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFinanceBench\u4e0a\u8fbe\u52300.8051\u51c6\u786e\u7387\uff0cTAT DQA\u4e0a\u8fbe\u52300.9629\u53ec\u56de\u7387\uff0c\u6bcf\u9875\u4ec5\u5b58\u50a817-27\u4e2a\u5411\u91cf\u3002", "conclusion": "\u514dOCR\u3001\u57fa\u4e8e\u6458\u8981\u6307\u5bfc\u7684\u591a\u6a21\u6001\u68c0\u7d22\u662f\u4f20\u7edf\u6587\u672c\u63d0\u53d6\u6d41\u7a0b\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.21389", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21389", "abs": "https://arxiv.org/abs/2511.21389", "authors": ["Guoxiao Zhang", "Ao Li", "Tan Qu", "Qianlong Xie", "Xingxing Wang"], "title": "FITRep: Attention-Guided Item Representation via MLLMs", "comment": null, "summary": "Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.", "AI": {"tldr": "FITRep\u662f\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u767d\u76d2\u7269\u54c1\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5c42\u6b21\u5316\u8bed\u4e49\u6982\u5ff5\u3001\u7ed3\u6784\u4fdd\u6301\u964d\u7ef4\u548cFAISS\u805a\u7c7b\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u7269\u54c1\u53bb\u91cd\u4e2d\u7684\u5c40\u90e8\u7ed3\u6784\u574d\u584c\u95ee\u9898\uff0c\u5728\u7f8e\u56e2\u5e7f\u544a\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u70b9\u51fb\u7387\u548c\u5343\u6b21\u5c55\u793a\u6536\u76ca\u3002", "motivation": "\u5728\u7ebf\u5e73\u53f0\u5b58\u5728\u5927\u91cf\u89c6\u89c9\u548c\u6587\u672c\u76f8\u4f3c\u7684\u8fd1\u91cd\u590d\u7269\u54c1\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5c06\u8868\u793a\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u7565\u4e86\u7ed3\u6784\u5173\u7cfb\uff08\u5982\u4e3b\u8981vs\u8f85\u52a9\u5143\u7d20\uff09\uff0c\u5b58\u5728\u5c40\u90e8\u7ed3\u6784\u574d\u584c\u95ee\u9898\u3002", "method": "FITRep\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1)\u6982\u5ff5\u5c42\u6b21\u4fe1\u606f\u63d0\u53d6(CHIE)\uff0c\u4f7f\u7528MLLMs\u63d0\u53d6\u5c42\u6b21\u5316\u8bed\u4e49\u6982\u5ff5\uff1b(2)\u7ed3\u6784\u4fdd\u6301\u964d\u7ef4(SPDR)\uff0c\u57fa\u4e8eUMAP\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u4fe1\u606f\u538b\u7f29\uff1b(3)FAISS\u805a\u7c7b(FBC)\uff0c\u4f7f\u7528FAISS\u4e3a\u6bcf\u4e2a\u7269\u54c1\u5206\u914d\u552f\u4e00\u805a\u7c7bID\u3002", "result": "\u5728\u7f8e\u56e2\u5e7f\u544a\u7cfb\u7edf\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cFITRep\u5b9e\u73b0\u4e86+3.60%\u7684\u70b9\u51fb\u7387(CTR)\u63d0\u5347\u548c+4.25%\u7684\u5343\u6b21\u5c55\u793a\u6536\u76ca(CPM)\u63d0\u5347\u3002", "conclusion": "FITRep\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u767d\u76d2\u8868\u793a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u7269\u54c1\u53bb\u91cd\u4e2d\u7684\u5c40\u90e8\u7ed3\u6784\u574d\u584c\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6548\u679c\u548c\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2511.21394", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21394", "abs": "https://arxiv.org/abs/2511.21394", "authors": ["Guoxiao Zhang", "Tan Qu", "Ao Li", "DongLin Ni", "Qianlong Xie", "Xingxing Wang"], "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction", "comment": null, "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.", "AI": {"tldr": "RIA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u8868\u793a\u548c\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u91cd\u6392\u5e8f\u65b9\u6cd5\u5c06\u6392\u5e8f\u548c\u91cd\u6392\u5e8f\u89e3\u8026\uff0c\u5bfc\u81f4\u5217\u8868\u8bc4\u4f30\u6a21\u578b\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5b58\u5728\u7ec4\u5408\u7a00\u758f\u6027\u548c\u8868\u793a\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRIA\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1aUCDT\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7528\u6237-\u7269\u54c1-\u4e0a\u4e0b\u6587\u5efa\u6a21\uff1bCUHT\u7528\u4e8e\u4f4d\u7f6e\u654f\u611f\u504f\u597d\u5b66\u4e60\uff1bLMH\u7528\u4e8e\u6355\u6349\u5c42\u6b21\u5316\u7269\u54c1\u4f9d\u8d56\uff1bEC\u7528\u4e8e\u63a8\u7406\u65f6\u7684\u6548\u7387-\u6548\u679c\u5e73\u8861\u3002", "result": "\u5728\u516c\u5f00\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u6a21\u578b\uff0cAUC\u548cLogLoss\u663e\u8457\u63d0\u5347\u3002\u5728\u7f8e\u56e2\u5e7f\u544a\u7cfb\u7edf\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cCTR\u63d0\u53471.69%\uff0cCPM\u63d0\u53474.54%\u3002", "conclusion": "RIA\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u6392\u5e8f\u548c\u91cd\u6392\u5e8f\u89e3\u8026\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6548\u679c\u548c\u6548\u7387\u7684\u5e73\u8861\uff0c\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4ef7\u503c\u3002"}}

{"id": "2511.21989", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.21989", "abs": "https://arxiv.org/abs/2511.21989", "authors": ["Nachiket Subbaraman", "Jaskinder Sarai", "Aniruddh Nath", "Lichan Hong", "Lukasz Heldt", "Li Wei", "Zhe Zhao"], "title": "Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation", "comment": "12 pages, 15 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning, generalization, and simulating human-like behavior across a wide range of tasks. These strengths present new opportunities to enhance traditional recommendation systems (RS), especially in the cold-start item scenario where newly introduced items lack interactions. Existing works have used LLMs to address cold-start issues in traditional RS through data augmentation, but they have limitations. One recent work directly addresses this issue by prompting LLMs to generate augmented interaction data between randomly sampled users and cold-start items. Then, they train the traditional RS with augmented data, incorporating collaborative signals for cold-start items. Although they use LLMs to provide cold-start items with feedback, they use partial user histories, which does not allow the LLM to fully emulate the user. Furthermore, randomly selecting users is not optimal for augmentation. To address these challenges, we leverage the LLM as a user and develop a reinforcement learning (RL) framework that trains a policy to select users for augmentation, optimizing for cold-start item performance after augmented training. The policy model learns to select users for cold-start item data augmentation based on their behavioral features and histories. To optimize user selection for cold-start item performance, we employ a policy gradient method that updates the policy in the direction of actions that lead to high rewards. Experiments on Amazon Product Review datasets show substantial gains in cold-start item recall, demonstrating the effectiveness of our method as a scalable, serving-efficient augmentation strategy for modern RS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7528\u6237\u9009\u62e9\u6846\u67b6\uff0c\u5229\u7528LLM\u6a21\u62df\u7528\u6237\u751f\u6210\u51b7\u542f\u52a8\u7269\u54c1\u4ea4\u4e92\u6570\u636e\uff0c\u4f18\u5316\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528LLM\u751f\u6210\u51b7\u542f\u52a8\u7269\u54c1\u7684\u589e\u5f3a\u6570\u636e\u65f6\uff0c\u4ec5\u4f7f\u7528\u90e8\u5206\u7528\u6237\u5386\u53f2\u4e14\u968f\u673a\u9009\u62e9\u7528\u6237\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325LLM\u6a21\u62df\u7528\u6237\u7684\u80fd\u529b\uff0c\u4e5f\u4e0d\u662f\u6700\u4f18\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565", "method": "\u5c06LLM\u4f5c\u4e3a\u7528\u6237\u6a21\u62df\u5668\uff0c\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u7b56\u7565\u6a21\u578b\uff0c\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u7279\u5f81\u548c\u5386\u53f2\u9009\u62e9\u6700\u9002\u5408\u7684\u7528\u6237\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4f18\u5316\u7b56\u7565\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u51b7\u542f\u52a8\u7269\u54c1\u6027\u80fd", "result": "\u5728Amazon\u4ea7\u54c1\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u7269\u54c1\u53ec\u56de\u7387\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u670d\u52a1\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u7528\u6237\u9009\u62e9\u6846\u67b6\u80fd\u591f\u4f18\u5316LLM\u5728\u63a8\u8350\u7cfb\u7edf\u51b7\u542f\u52a8\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u7528\u6237\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u51b7\u542f\u52a8\u7269\u54c1\u7684\u63a8\u8350\u6027\u80fd"}}
{"id": "2511.22240", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22240", "abs": "https://arxiv.org/abs/2511.22240", "authors": ["Philip Zhong", "Kent Chen", "Don Wang"], "title": "Evaluating Embedding Models and Pipeline Optimization for AI Search Quality", "comment": null, "summary": "We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.", "AI": {"tldr": "\u8bc4\u4f30\u4e0d\u540c\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548c\u914d\u7f6e\u5bf9AI\u641c\u7d22\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u7ef4\u5d4c\u5165\u3001\u795e\u7ecf\u91cd\u6392\u5668\u548c\u7ec6\u7c92\u5ea6\u5206\u5757\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u6587\u672c\u5d4c\u5165\u6a21\u578b\u548c\u7ba1\u9053\u914d\u7f6e\u5728AI\u9a71\u52a8\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6700\u4f73\u5b9e\u8df5\u6307\u5bfc\u3002", "method": "\u4f7f\u7528\u672c\u5730LLM\u4ece\u7f8e\u56fd\u5e02\u8bae\u4f1a\u4f1a\u8bae\u8bb0\u5f55\u5408\u621011,975\u4e2a\u67e5\u8be2-\u5206\u5757\u5bf9\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\uff08All-MPNet\u3001BGE\u3001GTE\u3001Qwen\uff09\u3001\u7ef4\u5ea6\u3001\u7d22\u5f15\u65b9\u6cd5\uff08Milvus HNSW/IVF\uff09\u548c\u5206\u5757\u7b56\u7565\uff0c\u91c7\u7528Top-K\u51c6\u786e\u7387\u548cNDCG\u6307\u6807\u8bc4\u4f30\u68c0\u7d22\u6027\u80fd\u3002", "result": "\u9ad8\u7ef4\u5d4c\u5165\u663e\u8457\u63d0\u5347\u641c\u7d22\u8d28\u91cf\uff08\u5982Qwen3-Embedding-8B/4096\u7684Top-3\u51c6\u786e\u7387\u7ea60.571 vs GTE-large/1024\u76840.412\uff09\uff0c\u795e\u7ecf\u91cd\u6392\u5668\uff08\u5982BGE\u4ea4\u53c9\u7f16\u7801\u5668\uff09\u8fdb\u4e00\u6b65\u63d0\u5347\u6392\u5e8f\u51c6\u786e\u7387\uff08Top-3\u8fbe0.527\uff09\uff0c\u7ec6\u7c92\u5ea6\u5206\u5757\uff08512\u5b57\u7b26vs2000\u5b57\u7b26\uff09\u4e5f\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u5d4c\u5165\u7ef4\u5ea6\u3001\u91cd\u6392\u6a21\u578b\u548c\u5206\u5757\u7c92\u5ea6\u662f\u5f71\u54cdAI\u641c\u7d22\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u81ea\u52a8\u5316\u7ba1\u9053\u548c\u8bc4\u4f30\u6d41\u7a0b\u3002"}}
{"id": "2511.22247", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22247", "abs": "https://arxiv.org/abs/2511.22247", "authors": ["Hoang-Bao Le", "Allie Tran", "Binh T. Nguyen", "Liting Zhou", "Cathal Gurrin"], "title": "FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text", "comment": "Accepted at MMM 2026", "summary": "Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.", "AI": {"tldr": "IGROT\u7edf\u4e00\u4e86\u89c6\u89c9\u68c0\u7d22\u548c\u7ec4\u5408\u68c0\u7d22\uff0c\u4f46\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u57fa\u51c6\u548c\u5e73\u8861\u6027\u80fd\u7684\u65b9\u6cd5\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6FIGROTD\u548c\u65b9\u5dee\u5f15\u5bfc\u7279\u5f81\u63a9\u7801VaGFeM\uff0c\u7ed3\u5408\u53cc\u635f\u5931\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "IGROT\uff08\u56fe\u50cf\u5f15\u5bfc\u68c0\u7d22\u4e0e\u53ef\u9009\u6587\u672c\uff09\u7edf\u4e00\u4e86\u89c6\u89c9\u68c0\u7d22\u548c\u7ec4\u5408\u68c0\u7d22\uff0c\u5728Google Image\u548cBing\u7b49\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u8fdb\u5c55\u53d7\u9650\uff1a\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u7684\u57fa\u51c6\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u548c\u7ec4\u5408\u67e5\u8be2\u4e4b\u95f4\u6027\u80fd\u4e0d\u5e73\u8861\uff0c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "1) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6FIGROTD\uff0c\u5305\u542b16,474\u4e2a\u8bad\u7ec3\u4e09\u5143\u7ec4\u548c1,262\u4e2a\u6d4b\u8bd5\u4e09\u5143\u7ec4\uff0c\u6db5\u76d6CIR\u3001SBIR\u548cCSTBIR\u4efb\u52a1\uff1b2) \u63d0\u51fa\u65b9\u5dee\u5f15\u5bfc\u7279\u5f81\u63a9\u7801(VaGFeM)\uff0c\u57fa\u4e8e\u65b9\u5dee\u7edf\u8ba1\u9009\u62e9\u6027\u589e\u5f3a\u5224\u522b\u6027\u7ef4\u5ea6\uff1b3) \u91c7\u7528\u53cc\u635f\u5931\u8bbe\u8ba1(InfoNCE + Triplet)\u6539\u5584\u7ec4\u5408\u63a8\u7406\u3002", "result": "\u5728FIGROTD\u4e0a\u8bad\u7ec3\u7684VaGFeM\u5728\u4e5d\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff1aCIRCO\u4e0a\u8fbe\u523034.8 mAP@10\uff0cSketchy\u4e0a\u8fbe\u523075.7 mAP@200\uff0c\u5c3d\u7ba1\u4f7f\u7528\u66f4\u5c11\u7684\u4e09\u5143\u7ec4\uff0c\u4ecd\u4f18\u4e8e\u66f4\u5f3a\u7684\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684FIGROTD\u6570\u636e\u96c6\u548cVaGFeM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86IGROT\u9886\u57df\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u95ee\u9898\uff0c\u5728\u8f7b\u91cf\u7ea7\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5e73\u8861\u4e14\u7ade\u4e89\u6027\u7684\u6027\u80fd\uff0c\u4e3a\u56fe\u50cf\u5f15\u5bfc\u68c0\u7d22\u4e0e\u53ef\u9009\u6587\u672c\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22253", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22253", "abs": "https://arxiv.org/abs/2511.22253", "authors": ["Hoang-Bao Le", "Allie Tran", "Binh T. Nguyen", "Liting Zhou", "Cathal Gurrin"], "title": "UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries", "comment": "Accepted at ICDM - MMSR Workshop 2025", "summary": "Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.", "AI": {"tldr": "UNION\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u5d4c\u5165\u548c\u7a7a\u6587\u672c\u63d0\u793a\uff0c\u5728\u5c11\u91cf\u6570\u636e\uff085000\u6837\u672c\uff09\u4e0b\u5b9e\u73b0\u56fe\u50cf\u5f15\u5bfc\u68c0\u7d22\uff0c\u5728CIR\u548cSBIR\u4efb\u52a1\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c", "motivation": "\u89e3\u51b3IGROT\uff08\u56fe\u50cf\u5f15\u5bfc\u68c0\u7d22\uff09\u5728\u4f4e\u6570\u636e\u76d1\u7763\u4e0b\u7684\u6311\u6218\uff0c\u7edf\u4e00CIR\uff08\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff09\u548cSBIR\uff08\u8349\u56fe\u56fe\u50cf\u68c0\u7d22\uff09\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff0c\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u53ef\u6cdb\u5316\u7684\u76ee\u6807\u8868\u793aUNION\uff0c\u5c06\u56fe\u50cf\u5d4c\u5165\u4e0e\u7a7a\u6587\u672c\u63d0\u793a\u878d\u5408\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c", "result": "\u5728CIRCO\u6570\u636e\u96c6\u4e0amAP@50\u8fbe\u523038.5\uff0cSketchy\u6570\u636e\u96c6\u4e0amAP@200\u8fbe\u523082.7\uff0c\u8d85\u8d8a\u8bb8\u591a\u9700\u8981\u5927\u91cf\u76d1\u7763\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "UNION\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u5b9e\u73b0\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u4e3a\u56fe\u50cf\u5f15\u5bfc\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f4e\u6570\u636e\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.22263", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22263", "abs": "https://arxiv.org/abs/2511.22263", "authors": ["Taeryun Won", "Tae Kwan Lee", "Hiun Kim", "Hyemin Lee"], "title": "Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title", "comment": null, "summary": "This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86BM25\u3001SPLADE\u548cExpanded-SPLADE\u5728\u5927\u89c4\u6a21\u7f51\u9875\u6587\u6863\u68c0\u7d22\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7a00\u758f\u8868\u793a\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff0c\u901a\u8fc7\u526a\u679d\u7b56\u7565\u4f18\u5316\u540e\uff0cExpanded-SPLADE\u5728\u6548\u679c\u4e0e\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u7f51\u9875\u6587\u6863\u68c0\u7d22\u573a\u666f\u4e2d\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u6a21\u578b\uff08\u4f20\u7edf\u8bcd\u9891\u7edf\u8ba1\u6a21\u578bBM25\u4e0e\u57fa\u4e8e\u7a00\u758f\u8868\u793a\u7684SPLADE\u7cfb\u5217\uff09\u5728\u6548\u679c\u4e0e\u6548\u7387\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u641c\u7d22\u5f15\u64ce\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "1. \u5728\u5343\u4e07\u5230\u767e\u4ebf\u7ea7\u7f51\u9875\u6587\u6863\u6807\u9898\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4BM25\u3001SPLADE\u548cExpanded-SPLADE\u6a21\u578b\uff1b2. \u63d0\u51fa\u6587\u6863\u4e2d\u5fc3\u526a\u679d\u3001top-k\u67e5\u8be2\u8bcd\u9009\u62e9\u3001\u5e26\u8bcd\u9879\u9608\u503c\u7684\u5e03\u5c14\u67e5\u8be2\u7b49\u7b56\u7565\u6765\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff1b3. \u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u68c0\u7d22\u6548\u679c\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "result": "1. SPLADE\u548cExpanded-SPLADE\u5728\u68c0\u7d22\u6548\u679c\u4e0a\u4f18\u4e8eBM25\uff0c\u7279\u522b\u662f\u590d\u6742\u67e5\u8be2\uff1b2. \u7a00\u758f\u8868\u793a\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff1b3. \u526a\u679d\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6548\u7387\u800c\u4e0d\u660e\u663e\u635f\u5931\u6027\u80fd\uff1b4. Expanded-SPLADE\u5728\u6548\u679c\u4e0e\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0c\u5c24\u5176\u9002\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "Expanded-SPLADE\u5728\u5927\u89c4\u6a21\u7f51\u9875\u6587\u6863\u68c0\u7d22\u4e2d\u5b9e\u73b0\u4e86\u6548\u679c\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u526a\u679d\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u90e8\u7f72\u6210\u672c\uff0c\u4e3a\u641c\u7d22\u5f15\u64ce\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.22707", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22707", "abs": "https://arxiv.org/abs/2511.22707", "authors": ["Tianxin Wei", "Xuying Ning", "Xuxing Chen", "Ruizhong Qiu", "Yupeng Hou", "Yan Xie", "Shuang Yang", "Zhigang Hua", "Jingrui He"], "title": "CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation", "comment": null, "summary": "In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.", "AI": {"tldr": "CoFiRec\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5c42\u6b21\u5316tokenization\u6765\u5efa\u6a21\u7528\u6237\u610f\u56fe\u7684\u6e10\u8fdb\u6f14\u5316\u8fc7\u7a0b\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u5c06\u5f02\u6784\u5c5e\u6027\u538b\u7f29\u4e3a\u5355\u4e00\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u7269\u54c1\u8bed\u4e49\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u7528\u6237\u610f\u56fe\u5728\u7f51\u9875\u4ea4\u4e92\u4e2d\u7684\u6e10\u8fdb\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCoFiRec\u6846\u67b6\uff0c\u5c06\u7269\u54c1\u4fe1\u606f\u5206\u89e3\u4e3a\u591a\u4e2a\u8bed\u4e49\u5c42\u6b21\uff08\u4ece\u9ad8\u5c42\u7c7b\u522b\u5230\u8be6\u7ec6\u63cf\u8ff0\uff09\uff0c\u72ec\u7acbtokenize\u6bcf\u4e2a\u5c42\u6b21\u5e76\u4fdd\u6301\u7ed3\u6784\u987a\u5e8f\uff0c\u5728\u81ea\u56de\u5f52\u89e3\u7801\u65f6\u6307\u5bfc\u8bed\u8a00\u6a21\u578b\u4ece\u7c97\u5230\u7ec6\u751f\u6210\u7269\u54c1token\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cCoFiRec\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7406\u8bba\u8bc1\u660e\u7ed3\u6784\u5316tokenization\u80fd\u964d\u4f4e\u751f\u6210\u7269\u54c1\u4e0e\u771f\u5b9e\u7269\u54c1\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "conclusion": "CoFiRec\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7269\u54c1\u8bed\u4e49\u7684\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u610f\u56fe\u7684\u6e10\u8fdb\u6f14\u5316\u3002"}}
{"id": "2511.22855", "categories": ["cs.IR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.22855", "abs": "https://arxiv.org/abs/2511.22855", "authors": ["Zhongming Feng", "Qiling Gao", "Zeping Sui", "Yun Lin", "Michail Matthaiou"], "title": "Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems", "comment": "5 pages", "summary": "This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u4e2d\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u5e94\u5bf9\u591a\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u7a7a\u4e2d\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u6beb\u7c73\u6ce2\u7cfb\u7edf\u9762\u4e34\u7528\u6237\u79fb\u52a8\u6027\u3001\u4e0d\u5b8c\u7f8e\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u786c\u4ef6\u635f\u4f24\u7b49\u591a\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u5b89\u5168\u90e8\u7f72\u548c\u6ce2\u675f\u6210\u5f62\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u957f\u671f\u65e0\u4eba\u673a\u90e8\u7f72\u4e0e\u6bcf\u65f6\u9699\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u89e3\u8026\uff0c\u4f7f\u7528\u6761\u4ef6\u98ce\u9669\u4ef7\u503c\u4f5c\u4e3a\u5206\u5e03\u65e0\u5173\u98ce\u9669\u5ea6\u91cf\uff0c\u7ed3\u5408\u4ee3\u7406\u6a21\u578b\u548c\u4ea4\u66ff\u4f18\u5316\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DRO-CVaR\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5c3e\u90e8\u4fdd\u5bc6\u9891\u8c31\u6548\u7387\uff0c\u5e76\u4fdd\u6301\u8f83\u4f4e\u7684\u4e2d\u65ad\u6982\u7387\uff0c\u5c24\u5176\u5728\u4e25\u91cd\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u8be5\u4e24\u9636\u6bb5\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\u4e3a\u7a7a\u4e2d\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u6beb\u7c73\u6ce2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9c81\u68d2\u5b89\u5168\u90e8\u7f72\u548c\u6ce2\u675f\u6210\u5f62\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u591a\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002"}}
{"id": "2511.22872", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.22872", "abs": "https://arxiv.org/abs/2511.22872", "authors": ["Yuyuan Li", "Junjie Fang", "Fengyuan Yu", "Xichun Sheng", "Tianyu Du", "Xuyang Teng", "Shaowei Jiang", "Linbo Jiang", "Jianan Lin", "Chaochao Chen"], "title": "FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training", "comment": null, "summary": "Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.", "AI": {"tldr": "FedAU2\uff1a\u9488\u5bf9\u7528\u6237\u7ea7\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u5c5e\u6027\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u6297\u8bad\u7ec3\u548c\u53cc\u968f\u673a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u68af\u5ea6\u4fe1\u606f\u6cc4\u9732\u95ee\u9898", "motivation": "\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u4f46\u7528\u6237\u5d4c\u5165\u4e2d\u4ecd\u5305\u542b\u654f\u611f\u5c5e\u6027\u4fe1\u606f\uff0c\u6613\u53d7\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\u3002\u7528\u6237\u7ea7\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u6bd4\u7ec4\u7ea7\u66f4\u5b9e\u7528\u4f46\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u5c5e\u6027\u9057\u5fd8\u65b9\u6cd5", "method": "\u63d0\u51faFedAU2\u65b9\u6cd5\uff1a1\uff09\u81ea\u9002\u5e94\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\uff0c\u6839\u636e\u672c\u5730\u4f18\u5316\u884c\u4e3a\u8c03\u6574\u8bad\u7ec3\u52a8\u6001\uff1b2\uff09\u53cc\u968f\u673a\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6270\u52a8\u5bf9\u6297\u6a21\u578b\uff0c\u9632\u6b62\u68af\u5ea6\u4fe1\u606f\u6cc4\u9732", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedAU2\u5728\u9057\u5fd8\u6548\u679c\u548c\u63a8\u8350\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "FedAU2\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u7ea7\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5c5e\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u63a8\u8350\u6027\u80fd"}}
{"id": "2511.23312", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.23312", "abs": "https://arxiv.org/abs/2511.23312", "authors": ["Gustavo Penha", "Aleksandr V. Petrov", "Claudia Hauff", "Enrico Palumbo", "Ali Vardasbi", "Edoardo D'Amico", "Francesco Fabbri", "Alice Wang", "Praveen Chandar", "Henrik Lindstrom", "Hugues Bouchard", "Mounia Lalmas"], "title": "Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?", "comment": null, "summary": "Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.", "AI": {"tldr": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u9760\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\uff0c\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898", "motivation": "\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff1a\u79bb\u7ebf\u65b9\u6cd5\u5b58\u5728\u66dd\u5149\u504f\u5dee\u3001\u6d41\u884c\u5ea6\u504f\u5dee\u7b49\u95ee\u9898\uff0c\u800cCranfield\u98ce\u683c\u6d4b\u8bd5\u96c6\u5408\u6784\u5efa\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u3002\u9700\u8981\u5bfb\u627e\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528ML-32M-ext\u7535\u5f71\u63a8\u8350\u6570\u636e\u96c6\uff0c\u7814\u7a76LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u8005\u7684\u53ef\u884c\u6027\u3002\u5206\u6790\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22LLM\u8bc4\u4f30\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u64ad\u5ba2\u63a8\u8350\u9886\u57df\u8fdb\u884c\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u3002", "result": "LLM\u8bc4\u4f30\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u6392\u540d\u4e00\u81f4\u6027\u9ad8\uff08Kendall's tau = 0.87\uff09\u3002\u4e30\u5bcc\u7684\u9879\u76ee\u5143\u6570\u636e\u548c\u66f4\u957f\u7684\u7528\u6237\u5386\u53f2\u8bb0\u5f55\u80fd\u63d0\u9ad8\u5bf9\u9f50\u5ea6\u3002\u5de5\u4e1a\u6848\u4f8b\u8bc1\u660eLLM\u8bc4\u4f30\u53ef\u7528\u4e8e\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "LLM\u8bc4\u4f30\u662f\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u7684\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\uff0c\u89e3\u51b3\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}

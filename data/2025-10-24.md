<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts](https://arxiv.org/abs/2510.19986)
*Drew B. Thomas*

Main category: cs.IR

TL;DR: 提出了一种结合大语言模型和向量数据库的检索增强生成方法，用于早期现代宗教图像分类，在分类精度上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像和基于关键字的搜索方法在早期现代宗教图像分类中效果有限，需要一种能够结合视觉和文本元素的更准确分类方法。

Method: 利用大语言模型生成包含视觉和文本元素的完整页面描述，通过混合向量搜索匹配相关的Iconclass代码，采用检索增强生成技术。

Result: 在五个和四个分类级别上分别达到87%和92%的精确度，显著优于传统图像和基于关键字的搜索方法。

Conclusion: 该方法展示了LLMs和RAG在艺术史和数字人文学科研究中的巨大潜力，为大规模早期现代视觉档案分析提供了强大工具。

Abstract: This paper presents a novel methodology for classifying early modern
religious images by using Large Language Models (LLMs) and vector databases in
combination with Retrieval-Augmented Generation (RAG). The approach leverages
the full-page context of book illustrations from the Holy Roman Empire,
allowing the LLM to generate detailed descriptions that incorporate both visual
and textual elements. These descriptions are then matched to relevant Iconclass
codes through a hybrid vector search. This method achieves 87% and 92%
precision at five and four levels of classification, significantly
outperforming traditional image and keyword-based searches. By employing
full-page descriptions and RAG, the system enhances classification accuracy,
offering a powerful tool for large-scale analysis of early modern visual
archives. This interdisciplinary approach demonstrates the growing potential of
LLMs and RAG in advancing research within art history and digital humanities.

</details>


### [2] [Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning](https://arxiv.org/abs/2510.20150)
*Yaochen Zhu,Harald Steck,Dawen Liang,Yinhan He,Jundong Li,Nathan Kallus*

Main category: cs.IR

TL;DR: ConvRec-R1是一个两阶段框架，用于训练基于LLM的对话推荐系统，通过行为克隆数据集和Rank-GRPO优化算法解决LLM在推荐任务中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 预训练LLM在推荐任务中存在生成目录外项目、违反输出格式要求以及排名质量在列表末尾急剧下降的问题，需要专门的对齐方法。

Method: 第一阶段使用Remap-Reflect-Adjust管道构建行为克隆数据集；第二阶段提出Rank-GRPO算法，将推荐列表中的每个排名作为优化单位，重新定义奖励并引入基于几何平均的排名级重要性比率。

Result: 在Reddit-v2数据集上的实验表明，ConvRec-R1比GRPO基线收敛更快，并实现了更高的Recall和NDCG指标。

Conclusion: ConvRec-R1通过两阶段训练框架和专门的Rank-GRPO算法，有效解决了LLM在对话推荐系统中的对齐问题，提高了推荐质量。

Abstract: Large language models (LLMs) are reshaping the recommender system paradigm by
enabling users to express preferences and receive recommendations through
conversations. Yet, aligning LLMs to the recommendation task remains
challenging: pretrained LLMs often generate out-of-catalog items, violate
required output formats, and their ranking quality degrades sharply toward the
end of the generated list. To this end, we propose ConvRec-R1, a two-stage
framework for end-to-end training of LLM-based conversational recommender
systems. In Stage 1, we construct a behavioral-cloning dataset with a
Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded
demonstrations from powerful blackbox LLMs to warm-start the RL training. In
Stage 2, we propose Rank-GRPO, a principled extension of group relative policy
optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats
each rank in the recommendation list as the unit instead of token (too
fine-grained) or sequence (too coarse), redefining rewards to remove non-causal
credit assignment and introducing a rank-level importance ratio based on the
geometric mean of rank-wise token probabilities to stabilize policy updates.
Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges
faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and
datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

</details>


### [3] [Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures](https://arxiv.org/abs/2510.20193)
*Rahul Raja,Arpita Vats*

Main category: cs.IR

TL;DR: 该论文综述了多媒体检索增强问答系统的最新进展，重点关注多模态对齐架构，分析了检索方法、融合技术和答案生成策略，并讨论了跨模态对齐、延迟-准确性权衡等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 传统问答系统主要依赖结构化文本数据，但多媒体内容的快速增长为检索增强问答带来了新的挑战和机遇，需要整合视觉、语言和音频等多模态信息。

Method: 通过分类检索方法、融合技术和答案生成策略来系统分析多媒体问答系统架构，并评估基准数据集、评估协议和性能权衡。

Result: 论文系统梳理了多媒体问答系统的技术路线和发展现状，识别了当前系统在跨模态对齐、语义基础等方面的局限性。

Conclusion: 多媒体检索增强问答系统面临跨模态对齐、延迟-准确性权衡等关键挑战，需要进一步研究以构建更鲁棒和上下文感知的系统。

Abstract: Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.

</details>


### [4] [Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM Recommendation Updates](https://arxiv.org/abs/2510.20260)
*Changping Meng,Hongyi Ling,Jianling Wang,Yifan Liu,Shuzhou Zhang,Dapeng Hong,Mingyan Gao,Onkar Dalal,Ed Chi,Lichan Hong,Haokai Lu,Ningren Han*

Main category: cs.IR

TL;DR: 本文研究LLM推荐系统的更新策略，比较持续微调与RAG方法的优劣，提出结合两者优势的混合更新策略，并在十亿级用户平台上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 用户兴趣和内容的动态变化使得LLM推荐系统需要有效的更新机制，初始微调无法捕捉实时变化，需要研究稳健的更新方法。

Method: 使用LLM驱动的用户兴趣探索系统作为案例，比较持续微调和RAG方法在成本、敏捷性和知识整合等维度的表现，提出混合更新策略。

Result: 在十亿级用户平台的实时A/B实验中，混合方法在用户满意度方面取得了统计显著的改进。

Conclusion: 混合更新策略结合了周期性微调的长期知识适应性和低成本RAG的敏捷性，为维护高质量LLM推荐系统提供了实用且经济高效的框架。

Abstract: Large Language Models (LLMs) empower recommendation systems through their
advanced reasoning and planning capabilities. However, the dynamic nature of
user interests and content poses a significant challenge: While initial
fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to
capture such real-time changes, necessitating robust update mechanisms. This
paper investigates strategies for updating LLM-powered recommenders, focusing
on the trade-offs between ongoing fine-tuning and Retrieval-Augmented
Generation (RAG). Using an LLM-powered user interest exploration system as a
case study, we perform a comparative analysis of these methods across
dimensions like cost, agility, and knowledge incorporation. We propose a hybrid
update strategy that leverages the long-term knowledge adaptation of periodic
fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B
experiments on a billion-user platform that this hybrid approach yields
statistically significant improvements in user satisfaction, offering a
practical and cost-effective framework for maintaining high-quality LLM-powered
recommender systems.

</details>


### [5] [From Generation to Attribution: Music AI Agent Architectures for the Post-Streaming Era](https://arxiv.org/abs/2510.20276)
*Wonil Kim,Hyeongseok Wi,Seungsoon Park,Taejun Kim,Sangeun Keum,Keunhyoung Kim,Taewan Kim,Jongmin Jung,Taehyoung Kim,Gaetan Guerrero,Mael Le Goff,Julie Po,Dongjoo Moon,Juhan Nam,Jongpil Lee*

Main category: cs.IR

TL;DR: 提出基于内容块的音乐AI代理架构，通过细粒度检索和代理编排将归属权直接嵌入创作流程，旨在解决AI音乐生成中的归属、权利管理和经济模型问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑音乐创作，但其快速发展暴露了归属权、权利管理和经济模型的结构性缺陷。现有流媒体系统无法处理AI驱动生产的规模和复杂性。

Method: 设计基于内容块的音乐AI代理架构，将音乐组织为细粒度组件（块）存储在BlockDB中，通过归属层实现透明溯源和实时结算，支持迭代式、基于会话的交互。

Result: 该框架将AI从生成工具转变为公平AI媒体平台的基础设施，支持细粒度归属、公平补偿和参与式互动。

Conclusion: 该方案指向后流媒体范式，音乐不再作为静态目录，而是作为协作和自适应生态系统运作。

Abstract: Generative AI is reshaping music creation, but its rapid growth exposes
structural gaps in attribution, rights management, and economic models. Unlike
past media shifts, from live performance to recordings, downloads, and
streaming, AI transforms the entire lifecycle of music, collapsing boundaries
between creation, distribution, and monetization. However, existing streaming
systems, with opaque and concentrated royalty flows, are ill-equipped to handle
the scale and complexity of AI-driven production. We propose a content-based
Music AI Agent architecture that embeds attribution directly into the creative
workflow through block-level retrieval and agentic orchestration. Designed for
iterative, session-based interaction, the system organizes music into granular
components (Blocks) stored in BlockDB; each use triggers an Attribution Layer
event for transparent provenance and real-time settlement. This framework
reframes AI from a generative tool into infrastructure for a Fair AI Media
Platform. By enabling fine-grained attribution, equitable compensation, and
participatory engagement, it points toward a post-streaming paradigm where
music functions not as a static catalog but as a collaborative and adaptive
ecosystem.

</details>


### [6] [Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation](https://arxiv.org/abs/2510.20455)
*Xiaokai Wei,Jiajun Wu,Daiyao Yi,Reza Shirkavand,Michelle Gong*

Main category: cs.IR

TL;DR: 提出了TO-RoPE，一种改进的旋转位置编码方法，用于生成式推荐系统中同时建模时间顺序和事件索引信息。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过学习的嵌入或相对注意力偏置来注入时间信息，但RoPE方法如果设计得当，可以更有效地联合建模用户行为序列中的时间和顺序信息。

Method: 提出了TO-RoPE系列设计，将索引和时间作为角度源来直接塑造查询-键几何结构，包括早期融合、按维度分割和按头分割三种实现方式。

Result: 在公开数据集和工业数据集上的广泛实验表明，TO-RoPE变体在编码时间和索引方面始终优于现有方法。

Conclusion: 旋转嵌入为生成式推荐提供了一个简单、有原则且易于部署的基础框架。

Abstract: Generative recommenders, typically transformer-based autoregressive models,
predict the next item or action from a user's interaction history. Their
effectiveness depends on how the model represents where an interaction event
occurs in the sequence (discrete index) and when it occurred in wall-clock
time. Prevailing approaches inject time via learned embeddings or relative
attention biases. In this paper, we argue that RoPE-based approaches, if
designed properly, can be a stronger alternative for jointly modeling temporal
and sequential information in user behavior sequences. While vanilla RoPE in
LLMs considers only token order, generative recommendation requires
incorporating both event time and token index. To address this, we propose
Time-and-Order RoPE (TO-RoPE), a family of rotary position embedding designs
that treat index and time as angle sources shaping the query-key geometry
directly. We present three instantiations: early fusion, split-by-dim, and
split-by-head. Extensive experiments on both publicly available datasets and a
proprietary industrial dataset show that TO-RoPE variants consistently improve
accuracy over existing methods for encoding time and index. These results
position rotary embeddings as a simple, principled, and deployment-friendly
foundation for generative recommendation.

</details>


### [7] [Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE](https://arxiv.org/abs/2510.20674)
*Rakshith R,Shubham Sharma,Mohammed Sameer Khan,Ankush Chopra*

Main category: cs.IR

TL;DR: 该研究开发了多语言电商搜索系统，在Query-Category和Query-Item两个多语言相关性任务中，通过数据增强和模型微调，使用Gemma-3 12B模型取得了最佳性能，最终在排行榜上获得第4名。


<details>
  <summary>Details</summary>
Motivation: 解决多语言电商搜索中的Query-Category和Query-Item相关性评估问题，确保对所有目标语言的完整覆盖。

Method: 通过将现有数据集翻译成开发集中缺失的语言来进行数据增强，然后使用多种策略对Gemma-3 12B和Qwen-2.5 14B模型进行微调。

Result: Gemma-3 12B模型在使用原始和翻译数据时获得最佳QC性能，在使用原始、翻译和少数类数据创建时获得最佳QI性能，在私有测试集上平均F1得分为0.8857。

Conclusion: 数据增强和模型微调策略在多语言电商搜索任务中表现有效，Gemma-3 12B模型在两个相关性任务中都取得了最佳性能，最终获得排行榜第4名。

Abstract: This study presents the multilingual e-commerce search system developed by
the Tredence_AICOE team. The competition features two multilingual relevance
tasks: Query-Category (QC) Relevance, which evaluates how well a user's search
query aligns with a product category, and Query-Item (QI) Relevance, which
measures the match between a multilingual search query and an individual
product listing. To ensure full language coverage, we performed data
augmentation by translating existing datasets into languages missing from the
development set, enabling training across all target languages. We fine-tuned
Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies.
The Gemma-3 12B (4-bit) model achieved the best QC performance using original
and translated data, and the best QI performance using original, translated,
and minority class data creation. These approaches secured 4th place on the
final leaderboard, with an average F1-score of 0.8857 on the private test set.

</details>


### [8] [Generative Reasoning Recommendation via LLMs](https://arxiv.org/abs/2510.20815)
*Minjie Hong,Zetong Zhou,Zirun Guo,Ziang Zhang,Ruofan Hu,Weinan Gan,Jieming Zhu,Zhou Zhao*

Main category: cs.IR

TL;DR: GREAM是一个端到端框架，通过将预训练LLM适配为生成式推理推荐模型，统一了理解-推理-预测过程，在三个数据集上表现优于强基线。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理方面表现出色，但原生作为生成式推理推荐模型存在建模差距，因为文本语义与协同过滤信号之间存在内在差异，且用户反馈稀疏随机。

Method: 提出GREAM框架，包含三个组件：协同语义对齐、推理课程激活、稀疏正则化组策略优化，支持两种推理模式：直接序列推荐和顺序推理推荐。

Result: 在三个数据集上的实验表明，GREAM相比强基线取得了持续的性能提升。

Conclusion: GREAM为可验证RL驱动的LLM推荐器提供了一条实用路径，实现了端到端优化和因果透明度。

Abstract: Despite their remarkable reasoning capabilities across diverse domains, large
language models (LLMs) face fundamental challenges in natively functioning as
generative reasoning recommendation models (GRRMs), where the intrinsic
modeling gap between textual semantics and collaborative filtering signals,
combined with the sparsity and stochasticity of user feedback, presents
significant obstacles. This work explores how to build GRRMs by adapting
pre-trained LLMs, which achieves a unified understanding-reasoning-prediction
manner for recommendation tasks. We propose GREAM, an end-to-end framework that
integrates three components: (i) Collaborative-Semantic Alignment, which fuses
heterogeneous textual evidence to construct semantically consistent, discrete
item indices and auxiliary alignment tasks that ground linguistic
representations in interaction semantics; (ii) Reasoning Curriculum Activation,
which builds a synthetic dataset with explicit Chain-of-Thought supervision and
a curriculum that progresses through behavioral evidence extraction, latent
preference modeling, intent inference, recommendation formulation, and denoised
sequence rewriting; and (iii) Sparse-Regularized Group Policy Optimization
(SRPO), which stabilizes post-training via Residual-Sensitive Verifiable Reward
and Bonus-Calibrated Group Advantage Estimation, enabling end-to-end
optimization under verifiable signals despite sparse successes. GREAM natively
supports two complementary inference modes: Direct Sequence Recommendation for
high-throughput, low-latency deployment, and Sequential Reasoning
Recommendation that first emits an interpretable reasoning chain for causal
transparency. Experiments on three datasets demonstrate consistent gains over
strong baselines, providing a practical path toward verifiable-RL-driven LLM
recommenders.

</details>

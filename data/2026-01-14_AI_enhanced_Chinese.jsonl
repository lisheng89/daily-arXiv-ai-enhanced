{"id": "2601.07838", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07838", "abs": "https://arxiv.org/abs/2601.07838", "authors": ["Jinesh Patel", "Arpit Malhotra", "Ajay Pande", "Prateek Caire"], "title": "A survey: Information search time optimization based on RAG (Retrieval Augmentation Generation) chatbot", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) based chatbots are not only useful for information retrieval through questionanswering but also for making complex decisions based on injected private data.we present a survey on how much search time can be saved when retrieving complex information within an organization called \"X Systems\"(a stealth mode company) by using a RAG-based chatbot compared to traditional search methods. We compare the information retrieval time using standard search techniques versus the RAG-based chatbot for the same queries. Our results conclude that RAG-based chatbots not only save time in information retrieval but also optimize the search process effectively. This survey was conducted with a sample of 105 employees across departments, average time spending on information retrieval per query was taken as metric. Comparison shows us, there are average 80-95% improvement on search when use RAG based chatbot than using standard search.", "AI": {"tldr": "RAG\u804a\u5929\u673a\u5668\u4eba\u5728\u7ec4\u7ec7\u5185\u90e8\u4fe1\u606f\u68c0\u7d22\u4e2d\u76f8\u6bd4\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u53ef\u8282\u770180-95%\u7684\u65f6\u95f4", "motivation": "\u8bc4\u4f30RAG\u804a\u5929\u673a\u5668\u4eba\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u68c0\u7d22\u590d\u6742\u4fe1\u606f\u65f6\u76f8\u6bd4\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u7684\u65f6\u95f4\u8282\u7701\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\"X Systems\"\u8fd9\u6837\u7684\u9690\u8eab\u6a21\u5f0f\u516c\u53f8\u4e2d", "method": "\u5728\"X Systems\"\u516c\u53f8\u5bf9105\u540d\u8de8\u90e8\u95e8\u5458\u5de5\u8fdb\u884c\u8c03\u7814\uff0c\u6bd4\u8f83\u76f8\u540c\u67e5\u8be2\u4e0b\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u4e0eRAG\u804a\u5929\u673a\u5668\u4eba\u7684\u4fe1\u606f\u68c0\u7d22\u65f6\u95f4\uff0c\u4ee5\u6bcf\u6b21\u67e5\u8be2\u7684\u5e73\u5747\u68c0\u7d22\u65f6\u95f4\u4e3a\u6307\u6807", "result": "RAG\u804a\u5929\u673a\u5668\u4eba\u76f8\u6bd4\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u5728\u4fe1\u606f\u68c0\u7d22\u65f6\u95f4\u4e0a\u5e73\u5747\u670980-95%\u7684\u6539\u8fdb\uff0c\u663e\u8457\u4f18\u5316\u4e86\u641c\u7d22\u8fc7\u7a0b", "conclusion": "RAG\u804a\u5929\u673a\u5668\u4eba\u4e0d\u4ec5\u80fd\u8282\u7701\u4fe1\u606f\u68c0\u7d22\u65f6\u95f4\uff0c\u8fd8\u80fd\u6709\u6548\u4f18\u5316\u641c\u7d22\u8fc7\u7a0b\uff0c\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.07978", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.07978", "abs": "https://arxiv.org/abs/2601.07978", "authors": ["Benedict Wolff", "Jacopo Bennati"], "title": "Cost and accuracy of long-term graph memory in distributed LLM-based multi-agent systems", "comment": "23 pages, 4 figures, 7 tables", "summary": "Distributed multi-agent systems use large language models to enable collaborative intelligence while preserving privacy, yet systematic evaluations of long-term memory under network constraints remain limited. This study presents a flexible testbed comparing mem0, a vector-based memory framework, and Graphiti, a graph-based knowledge graph, using the LOCOMO long-context benchmark. Experiments were conducted under unconstrained and constrained network conditions, measuring computational, financial, and accuracy metrics. Results indicate that mem0 significantly outperforms Graphiti in efficiency, with faster loading times, lower resource consumption, and minimal network overhead, while accuracy differences are not statistically significant. Applying a statistical pareto efficiency framework, mem0 is identified as the optimal choice that balances cost and accuracy in DMAS.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e24\u79cd\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\uff1a\u5411\u91cf\u57fa\u7684mem0\u548c\u56fe\u57fa\u7684Graphiti\uff0c\u5728\u6709\u65e0\u7f51\u7edc\u7ea6\u675f\u6761\u4ef6\u4e0b\u8bc4\u4f30\u5176\u8ba1\u7b97\u3001\u8d22\u52a1\u548c\u51c6\u786e\u6027\u8868\u73b0\uff0c\u53d1\u73b0mem0\u5728\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8eGraphiti\uff0c\u800c\u51c6\u786e\u6027\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "motivation": "\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u534f\u4f5c\u667a\u80fd\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u7f3a\u4e4f\u5728\u7f51\u7edc\u7ea6\u675f\u4e0b\u5bf9\u957f\u671f\u8bb0\u5fc6\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002\u73b0\u6709\u7814\u7a76\u5bf9mem0\uff08\u5411\u91cf\u57fa\u8bb0\u5fc6\u6846\u67b6\uff09\u548cGraphiti\uff08\u56fe\u57fa\u77e5\u8bc6\u56fe\u8c31\uff09\u5728\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u6bd4\u8f83\u6709\u9650\u3002", "method": "\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u7528LOCOMO\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\uff0c\u5728\u65e0\u7ea6\u675f\u548c\u7ea6\u675f\u7f51\u7edc\u6761\u4ef6\u4e0b\u6bd4\u8f83mem0\u548cGraphiti\u3002\u6d4b\u91cf\u4e86\u8ba1\u7b97\u6548\u7387\uff08\u52a0\u8f7d\u65f6\u95f4\uff09\u3001\u8d22\u52a1\u6210\u672c\uff08\u8d44\u6e90\u6d88\u8017\uff09\u3001\u7f51\u7edc\u5f00\u9500\u548c\u51c6\u786e\u6027\u6307\u6807\u3002", "result": "mem0\u5728\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8eGraphiti\uff1a\u52a0\u8f7d\u65f6\u95f4\u66f4\u5feb\u3001\u8d44\u6e90\u6d88\u8017\u66f4\u4f4e\u3001\u7f51\u7edc\u5f00\u9500\u6700\u5c0f\u3002\u4e24\u79cd\u6846\u67b6\u7684\u51c6\u786e\u6027\u5dee\u5f02\u5728\u7edf\u8ba1\u4e0a\u4e0d\u663e\u8457\u3002\u5e94\u7528\u7edf\u8ba1\u5e15\u7d2f\u6258\u6548\u7387\u6846\u67b6\u5206\u6790\uff0cmem0\u88ab\u786e\u5b9a\u4e3a\u5728\u6210\u672c\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u7684\u9009\u62e9\u3002", "conclusion": "mem0\u662f\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u66f4\u4f18\u7684\u957f\u671f\u8bb0\u5fc6\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u5728\u4fdd\u6301\u4e0eGraphiti\u76f8\u5f53\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6548\u7387\u3001\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u597d\u7684\u7f51\u7edc\u9002\u5e94\u6027\uff0c\u662f\u5e73\u8861\u6210\u672c\u4e0e\u51c6\u786e\u6027\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u9009\u62e9\u3002"}}
{"id": "2601.08148", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08148", "abs": "https://arxiv.org/abs/2601.08148", "authors": ["Seokho Ahn", "Sungbok Shin", "Young-Duk Seo"], "title": "Enriching Semantic Profiles into Knowledge Graph for Recommender Systems Using Large Language Models", "comment": "Accepted at KDD 2026", "summary": "Rich and informative profiling to capture user preferences is essential for improving recommendation quality. However, there is still no consensus on how best to construct and utilize such profiles. To address this, we revisit recent profiling-based approaches in recommender systems along four dimensions: 1) knowledge base, 2) preference indicator, 3) impact range, and 4) subject. We argue that large language models (LLMs) are effective at extracting compressed rationales from diverse knowledge sources, while knowledge graphs (KGs) are better suited for propagating these profiles to extend their reach. Building on this insight, we propose a new recommendation model, called SPiKE. SPiKE consists of three core components: i) Entity profile generation, which uses LLMs to generate semantic profiles for all KG entities; ii) Profile-aware KG aggregation, which integrates these profiles into the KG; and iii) Pairwise profile preference matching, which aligns LLM- and KG-based representations during training. In experiments, we demonstrate that SPiKE consistently outperforms state-of-the-art KG- and LLM-based recommenders in real-world settings.", "AI": {"tldr": "SPiKE\u6a21\u578b\u7ed3\u5408LLMs\u548cKGs\u4f18\u52bf\uff1a\u7528LLMs\u4ece\u77e5\u8bc6\u6e90\u63d0\u53d6\u538b\u7f29\u8bed\u4e49\u6863\u6848\uff0c\u7528KGs\u4f20\u64ad\u6863\u6848\u6269\u5c55\u8986\u76d6\u8303\u56f4\uff0c\u901a\u8fc7\u6863\u6848\u611f\u77e5KG\u805a\u5408\u548c\u6210\u5bf9\u6863\u6848\u504f\u597d\u5339\u914d\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u6863\u6848\u6784\u5efa\u548c\u5229\u7528\u65b9\u6cd5\u7f3a\u4e4f\u5171\u8bc6\uff0c\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\u5316\u6863\u6848\u6765\u6355\u6349\u7528\u6237\u504f\u597d\u4ee5\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u3002LLMs\u64c5\u957f\u4ece\u591a\u6837\u77e5\u8bc6\u6e90\u63d0\u53d6\u538b\u7f29\u63a8\u7406\uff0c\u800cKGs\u66f4\u9002\u5408\u4f20\u64ad\u6863\u6848\u6269\u5c55\u8986\u76d6\u8303\u56f4\u3002", "method": "\u63d0\u51faSPiKE\u6a21\u578b\uff1a1) \u5b9e\u4f53\u6863\u6848\u751f\u6210\uff1a\u7528LLMs\u4e3a\u6240\u6709KG\u5b9e\u4f53\u751f\u6210\u8bed\u4e49\u6863\u6848\uff1b2) \u6863\u6848\u611f\u77e5KG\u805a\u5408\uff1a\u5c06\u8fd9\u4e9b\u6863\u6848\u6574\u5408\u5230KG\u4e2d\uff1b3) \u6210\u5bf9\u6863\u6848\u504f\u597d\u5339\u914d\uff1a\u5728\u8bad\u7ec3\u4e2d\u5bf9\u9f50LLM\u548cKG\u7684\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cSPiKE\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eKG\u548cLLM\u7684\u63a8\u8350\u7cfb\u7edf\u3002", "conclusion": "\u7ed3\u5408LLMs\u7684\u8bed\u4e49\u63d0\u53d6\u80fd\u529b\u548cKGs\u7684\u4f20\u64ad\u80fd\u529b\uff0cSPiKE\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u6863\u6848\u6784\u5efa\u548c\u5229\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2601.08275", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.08275", "abs": "https://arxiv.org/abs/2601.08275", "authors": ["Cong Xu", "Guoliang Li", "Jun Wang", "Wei Zhang"], "title": "Markovian Pre-Trained Transformer for Next-Item Recommendation", "comment": null, "summary": "We introduce the Markovian Pre-trained Transformer (MPT) for next-item recommendation, a transferable model fully pre-trained on synthetic Markov chains, yet capable of achieving state-of-the-art performance by fine-tuning a lightweight adaptor. This counterintuitive success stems from the observation of the `Markovian' nature: advanced sequential recommenders coincidentally rely on the latest interaction to make predictions, while the historical interactions serve mainly as auxiliary cues for inferring the user's general, non-sequential identity. This characteristic necessitates the capabilities of a universal recommendation model to effectively summarize the user sequence, with particular emphasis on the latest interaction. MPT inherently has the potential to be universal and transferable. On the one hand, when trained to predict the next state of Markov chains, it acquires the capabilities to estimate transition probabilities from the context (one adaptive manner for summarizing sequences) and attend to the last state to ensure accurate state transitions. On the other hand, unlike the heterogeneous interaction data, an unlimited amount of controllable Markov chains is available to boost the model capacity. We conduct extensive experiments on five public datasets from three distinct platforms to validate the superiority of Markovian pre-training over traditional recommendation pre-training and recent language pre-training paradigms.", "AI": {"tldr": "MPT\u662f\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u9884\u8bad\u7ec3\u7684\u53ef\u8fc1\u79fb\u63a8\u8350\u6a21\u578b\uff0c\u4ec5\u9700\u5fae\u8c03\u8f7b\u91cf\u9002\u914d\u5668\u5373\u53ef\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5176\u6210\u529f\u6e90\u4e8e\u5e8f\u5217\u63a8\u8350\u7684\"\u9a6c\u5c14\u53ef\u592b\u6027\"\u672c\u8d28\u3002", "motivation": "\u73b0\u6709\u5148\u8fdb\u5e8f\u5217\u63a8\u8350\u5668\u5b9e\u9645\u4e0a\u4e3b\u8981\u4f9d\u8d56\u6700\u8fd1\u4e00\u6b21\u4ea4\u4e92\u8fdb\u884c\u9884\u6d4b\uff0c\u5386\u53f2\u4ea4\u4e92\u4ec5\u4f5c\u4e3a\u63a8\u65ad\u7528\u6237\u4e00\u822c\u8eab\u4efd\u7684\u8f85\u52a9\u7ebf\u7d22\u3002\u8fd9\u79cd\"\u9a6c\u5c14\u53ef\u592b\u6027\"\u8868\u660e\u901a\u7528\u63a8\u8350\u6a21\u578b\u9700\u8981\u6709\u6548\u603b\u7ed3\u7528\u6237\u5e8f\u5217\u5e76\u7279\u522b\u5173\u6ce8\u6700\u65b0\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u9a6c\u5c14\u53ef\u592b\u9884\u8bad\u7ec3\u53d8\u6362\u5668(MPT)\uff0c\u5728\u5408\u6210\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0a\u5b8c\u5168\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u4ece\u4e0a\u4e0b\u6587\u4f30\u8ba1\u8f6c\u79fb\u6982\u7387\u5e76\u5173\u6ce8\u6700\u540e\u72b6\u6001\u7684\u80fd\u529b\u3002\u4e0e\u5f02\u6784\u4ea4\u4e92\u6570\u636e\u4e0d\u540c\uff0c\u65e0\u9650\u91cf\u7684\u53ef\u63a7\u9a6c\u5c14\u53ef\u592b\u94fe\u53ef\u7528\u4e8e\u63d0\u5347\u6a21\u578b\u5bb9\u91cf\u3002", "result": "\u5728\u6765\u81ea\u4e09\u4e2a\u4e0d\u540c\u5e73\u53f0\u7684\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u9a6c\u5c14\u53ef\u592b\u9884\u8bad\u7ec3\u76f8\u5bf9\u4e8e\u4f20\u7edf\u63a8\u8350\u9884\u8bad\u7ec3\u548c\u8fd1\u671f\u8bed\u8a00\u9884\u8bad\u7ec3\u8303\u5f0f\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "MPT\u5177\u6709\u6210\u4e3a\u901a\u7528\u53ef\u8fc1\u79fb\u63a8\u8350\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5176\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6355\u6349\u5e8f\u5217\u63a8\u8350\u7684\u672c\u8d28\u7279\u6027\uff0c\u4ec5\u9700\u5fae\u8c03\u8f7b\u91cf\u9002\u914d\u5668\u5373\u53ef\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2601.08283", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08283", "abs": "https://arxiv.org/abs/2601.08283", "authors": ["Heba Shakeel", "Tanvir Ahmad", "Tanya Liyaqat", "Chandni Saxena"], "title": "AgriLens: Semantic Retrieval in Agricultural Texts Using Topic Modeling and Language Models", "comment": "8 Pages, 1st workshop on Democratizing GenAI and Scalable NLP with HiPC for Societal Impact; 32nd IEEE International Conference on High Performance Computing, Data, & Analytics", "summary": "As the volume of unstructured text continues to grow across domains, there is an urgent need for scalable methods that enable interpretable organization, summarization, and retrieval of information. This work presents a unified framework for interpretable topic modeling, zero-shot topic labeling, and topic-guided semantic retrieval over large agricultural text corpora. Leveraging BERTopic, we extract semantically coherent topics. Each topic is converted into a structured prompt, enabling a language model to generate meaningful topic labels and summaries in a zero-shot manner. Querying and document exploration are supported via dense embeddings and vector search, while a dedicated evaluation module assesses topical coherence and bias. This framework supports scalable and interpretable information access in specialized domains where labeled data is limited.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u4e3b\u9898\u5efa\u6a21\u3001\u96f6\u6837\u672c\u4e3b\u9898\u6807\u6ce8\u548c\u4e3b\u9898\u5f15\u5bfc\u7684\u8bed\u4e49\u68c0\u7d22\uff0c\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u519c\u4e1a\u6587\u672c\u8bed\u6599\u5e93", "motivation": "\u968f\u7740\u5404\u9886\u57df\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u5feb\u901f\u589e\u957f\uff0c\u6025\u9700\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u4fe1\u606f\u7ec4\u7ec7\u3001\u6458\u8981\u548c\u68c0\u7d22\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e13\u95e8\u9886\u57df", "method": "\u4f7f\u7528BERTopic\u63d0\u53d6\u8bed\u4e49\u8fde\u8d2f\u7684\u4e3b\u9898\uff0c\u5c06\u6bcf\u4e2a\u4e3b\u9898\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u8ba9\u8bed\u8a00\u6a21\u578b\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u751f\u6210\u6709\u610f\u4e49\u7684\u4e3b\u9898\u6807\u7b7e\u548c\u6458\u8981\uff0c\u901a\u8fc7\u5bc6\u96c6\u5d4c\u5165\u548c\u5411\u91cf\u641c\u7d22\u652f\u6301\u67e5\u8be2\u548c\u6587\u6863\u63a2\u7d22\uff0c\u5e76\u8bbe\u6709\u4e13\u95e8\u7684\u8bc4\u4f30\u6a21\u5757", "result": "\u8be5\u6846\u67b6\u652f\u6301\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e13\u95e8\u9886\u57df\u8fdb\u884c\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u4fe1\u606f\u8bbf\u95ee", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u5e93\u4e2d\u7684\u4fe1\u606f\u7ec4\u7ec7\u3001\u6458\u8981\u548c\u68c0\u7d22\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u519c\u4e1a\u7b49\u4e13\u95e8\u9886\u57df"}}
{"id": "2601.08345", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08345", "abs": "https://arxiv.org/abs/2601.08345", "authors": ["Piotr Bajger", "Roman Dusek", "Krzysztof Galias", "Pawe\u0142 M\u0142yniec", "Aleksander Wawer", "Pawe\u0142 Zawistowski"], "title": "MLPlatt: Simple Calibration Framework for Ranking Models", "comment": null, "summary": "Ranking models are extensively used in e-commerce for relevance estimation. These models often suffer from poor interpretability and no scale calibration, particularly when trained with typical ranking loss functions. This paper addresses the problem of post-hoc calibration of ranking models. We introduce MLPlatt: a simple yet effective ranking model calibration method that preserves the item ordering and converts ranker outputs to interpretable click-through rate (CTR) probabilities usable in downstream tasks. The method is context-aware by design and achieves good calibration metrics globally, and within strata corresponding to different values of a selected categorical field (such as user country or device), which is often important from a business perspective of an E-commerce platform. We demonstrate the superiority of MLPlatt over existing approaches on two datasets, achieving an improvement of over 10\\% in F-ECE (Field Expected Calibration Error) compared to other methods. Most importantly, we show that high-quality calibration can be achieved without compromising the ranking quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMLPlatt\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u5546\u6392\u540d\u6a21\u578b\u7684\u540e\u5904\u7406\u6821\u51c6\uff0c\u5c06\u6392\u540d\u8f93\u51fa\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u70b9\u51fb\u7387\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9879\u76ee\u6392\u5e8f\u4e0d\u53d8\u3002", "motivation": "\u7535\u5546\u6392\u540d\u6a21\u578b\u901a\u5e38\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u548c\u7f3a\u4e4f\u5c3a\u5ea6\u6821\u51c6\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5178\u578b\u6392\u540d\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u6392\u5e8f\u8d28\u91cf\u548c\u5b9e\u73b0\u826f\u597d\u6821\u51c6\u3002", "method": "\u63d0\u51faMLPlatt\u65b9\u6cd5\uff1a\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6392\u540d\u6a21\u578b\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u8bbe\u8ba1\uff0c\u5c06\u6392\u540d\u5668\u8f93\u51fa\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u70b9\u51fb\u7387\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9879\u76ee\u6392\u5e8f\u4e0d\u53d8\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMLPlatt\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728F-ECE\uff08\u5b57\u6bb5\u671f\u671b\u6821\u51c6\u8bef\u5dee\uff09\u6307\u6807\u4e0a\u63d0\u5347\u8d85\u8fc710%\uff0c\u4e14\u5728\u4e0d\u5f71\u54cd\u6392\u540d\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6821\u51c6\u3002", "conclusion": "MLPlatt\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7535\u5546\u6392\u540d\u6a21\u578b\u7684\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u70b9\u51fb\u7387\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6392\u5e8f\u6027\u80fd\uff0c\u5bf9\u7535\u5546\u5e73\u53f0\u5177\u6709\u91cd\u8981\u5546\u4e1a\u4ef7\u503c\u3002"}}
{"id": "2601.08360", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08360", "abs": "https://arxiv.org/abs/2601.08360", "authors": ["Adithya Parthasarathy", "Aswathnarayan Muthukrishnan Kirubakaran", "Vinoth Punniyamoorthy", "Nachiappan Chockalingam", "Lokesh Butra", "Kabilan Kannan", "Abhirup Mazumder", "Sumit Saha"], "title": "Scalable Sequential Recommendation under Latency and Memory Constraints", "comment": null, "summary": "Sequential recommender systems must model long-range user behavior while operating under strict memory and latency constraints. Transformer-based approaches achieve strong accuracy but suffer from quadratic attention complexity, forcing aggressive truncation of user histories and limiting their practicality for long-horizon modeling. This paper presents HoloMambaRec, a lightweight sequential recommendation architecture that combines holographic reduced representations for attribute-aware embedding with a selective state space encoder for linear-time sequence processing. Item and attribute information are bound using circular convolution, preserving embedding dimensionality while encoding structured metadata. A shallow selective state space backbone, inspired by recent Mamba-style models, enables efficient training and constant-time recurrent inference. Experiments on Amazon Beauty and MovieLens-1M datasets demonstrate that HoloMambaRec consistently outperforms SASRec and achieves competitive performance with GRU4Rec under a constrained 10-epoch training budget, while maintaining substantially lower memory complexity. The design further incorporates forward-compatible mechanisms for temporal bundling and inference-time compression, positioning HoloMambaRec as a practical and extensible alternative for scalable, metadata-aware sequential recommendation.", "AI": {"tldr": "HoloMambaRec\uff1a\u7ed3\u5408\u5168\u606f\u964d\u7ef4\u8868\u793a\u548c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u7f16\u7801\u7684\u8f7b\u91cf\u7ea7\u5e8f\u5217\u63a8\u8350\u67b6\u6784\uff0c\u5728\u6709\u9650\u8bad\u7ec3\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u957f\u5e8f\u5217\u5efa\u6a21", "motivation": "\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u5728\u4e25\u683c\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u5efa\u6a21\u957f\u8303\u56f4\u7528\u6237\u884c\u4e3a\u3002\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u867d\u7136\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5b58\u5728\u4e8c\u6b21\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u622a\u65ad\u7528\u6237\u5386\u53f2\u8bb0\u5f55\uff0c\u9650\u5236\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u5b9e\u7528\u6027\u3002", "method": "1. \u4f7f\u7528\u5168\u606f\u964d\u7ef4\u8868\u793a\u8fdb\u884c\u5c5e\u6027\u611f\u77e5\u5d4c\u5165\uff1b2. \u91c7\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u7f16\u7801\u5668\u8fdb\u884c\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5904\u7406\uff1b3. \u901a\u8fc7\u5faa\u73af\u5377\u79ef\u7ed1\u5b9a\u9879\u76ee\u548c\u5c5e\u6027\u4fe1\u606f\uff1b4. \u57fa\u4e8eMamba\u98ce\u683c\u6a21\u578b\u7684\u6d45\u5c42\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u4e3b\u5e72\uff1b5. \u5305\u542b\u524d\u5411\u517c\u5bb9\u673a\u5236\uff0c\u652f\u6301\u65f6\u95f4\u6346\u7ed1\u548c\u63a8\u7406\u65f6\u538b\u7f29\u3002", "result": "\u5728Amazon Beauty\u548cMovieLens-1M\u6570\u636e\u96c6\u4e0a\uff0cHoloMambaRec\u572810\u4e2aepoch\u7684\u6709\u9650\u8bad\u7ec3\u9884\u7b97\u4e0b\uff0c\u6027\u80fd\u6301\u7eed\u4f18\u4e8eSASRec\uff0c\u4e0eGRU4Rec\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u663e\u8457\u66f4\u4f4e\u7684\u5185\u5b58\u590d\u6742\u5ea6\u3002", "conclusion": "HoloMambaRec\u4e3a\u53ef\u6269\u5c55\u3001\u5143\u6570\u636e\u611f\u77e5\u7684\u5e8f\u5217\u63a8\u8350\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2601.08363", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08363", "abs": "https://arxiv.org/abs/2601.08363", "authors": ["Ziyang Zeng", "Dun Zhang", "Yu Yan", "Xu Sun", "Yudong Zhou", "Yuqing Yang"], "title": "PosIR: Position-Aware Heterogeneous Information Retrieval Benchmark", "comment": "This research is driven by a strong academic interest, and we welcome further exchange and discussion with peers", "summary": "While dense retrieval models have achieved remarkable success, rigorous evaluation of their sensitivity to the position of relevant information (i.e., position bias) remains largely unexplored. Existing benchmarks typically employ position-agnostic relevance labels, conflating the challenge of processing long contexts with the bias against specific evidence locations. To address this challenge, we introduce PosIR (Position-Aware Information Retrieval), a comprehensive benchmark designed to diagnose position bias in diverse retrieval scenarios. PosIR comprises 310 datasets spanning 10 languages and 31 domains, constructed through a rigorous pipeline that ties relevance to precise reference spans, enabling the strict disentanglement of document length from information position. Extensive experiments with 10 state-of-the-art embedding models reveal that: (1) Performance on PosIR in long-context settings correlates poorly with the MMTEB benchmark, exposing limitations in current short-text benchmarks; (2) Position bias is pervasive and intensifies with document length, with most models exhibiting primacy bias while certain models show unexpected recency bias; (3) Gradient-based saliency analysis further uncovers the distinct internal attention mechanisms driving these positional preferences. In summary, PosIR serves as a valuable diagnostic framework to foster the development of position-robust retrieval systems.", "AI": {"tldr": "PosIR\u662f\u4e00\u4e2a\u7528\u4e8e\u8bca\u65ad\u68c0\u7d22\u6a21\u578b\u4e2d\u4f4d\u7f6e\u504f\u89c1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b310\u4e2a\u6570\u636e\u96c6\u300110\u79cd\u8bed\u8a00\u300131\u4e2a\u9886\u57df\uff0c\u901a\u8fc7\u5c06\u76f8\u5173\u6027\u4e0e\u7cbe\u786e\u53c2\u8003\u7247\u6bb5\u7ed1\u5b9a\u6765\u5206\u79bb\u6587\u6863\u957f\u5ea6\u548c\u4fe1\u606f\u4f4d\u7f6e\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u8bc4\u4f30\u901a\u5e38\u4f7f\u7528\u4f4d\u7f6e\u65e0\u5173\u7684\u76f8\u5173\u6027\u6807\u7b7e\uff0c\u6df7\u6dc6\u4e86\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u7684\u6311\u6218\u4e0e\u5bf9\u7279\u5b9a\u8bc1\u636e\u4f4d\u7f6e\u504f\u89c1\u7684\u533a\u5206\u3002\u9700\u8981\u4e13\u95e8\u57fa\u51c6\u6765\u8bca\u65ad\u68c0\u7d22\u6a21\u578b\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\u95ee\u9898\u3002", "method": "\u6784\u5efaPosIR\u57fa\u51c6\uff0c\u5305\u542b310\u4e2a\u8de8\u8bed\u8a00\u548c\u9886\u57df\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e25\u683c\u6d41\u7a0b\u5c06\u76f8\u5173\u6027\u4e0e\u7cbe\u786e\u53c2\u8003\u7247\u6bb5\u7ed1\u5b9a\uff0c\u786e\u4fdd\u6587\u6863\u957f\u5ea6\u4e0e\u4fe1\u606f\u4f4d\u7f6e\u7684\u4e25\u683c\u5206\u79bb\u3002\u4f7f\u752810\u4e2a\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u5206\u6790\u3002", "result": "1) PosIR\u5728\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u4e0eMMTEB\u57fa\u51c6\u76f8\u5173\u6027\u5dee\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u77ed\u6587\u672c\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff1b2) \u4f4d\u7f6e\u504f\u89c1\u666e\u904d\u5b58\u5728\u4e14\u968f\u6587\u6863\u957f\u5ea6\u52a0\u5267\uff0c\u591a\u6570\u6a21\u578b\u663e\u793a\u9996\u56e0\u504f\u89c1\uff0c\u67d0\u4e9b\u6a21\u578b\u663e\u793a\u8fd1\u56e0\u504f\u89c1\uff1b3) \u68af\u5ea6\u663e\u8457\u6027\u5206\u6790\u63ed\u793a\u4e86\u9a71\u52a8\u8fd9\u4e9b\u4f4d\u7f6e\u504f\u597d\u7684\u4e0d\u540c\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u3002", "conclusion": "PosIR\u4f5c\u4e3a\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u5f00\u53d1\u4f4d\u7f6e\u9c81\u68d2\u7684\u68c0\u7d22\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4f4d\u7f6e\u504f\u89c1\u95ee\u9898\u53ca\u5176\u673a\u5236\u3002"}}
{"id": "2601.08497", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08497", "abs": "https://arxiv.org/abs/2601.08497", "authors": ["Jia-Xin He", "Hung-Hsuan Chen"], "title": "GraphFusionSBR: Denoising Multi-Channel Graphs for Session-Based Recommendation", "comment": null, "summary": "Session-based recommendation systems must capture implicit user intents from sessions. However, existing models suffer from issues such as item interaction dominance and noisy sessions. We propose a multi-channel recommendation model, including a knowledge graph channel, a session hypergraph channel, and a session line graph channel, to capture information from multiple sources. Our model adaptively removes redundant edges in the knowledge graph channel to reduce noise. Knowledge graph representations cooperate with hypergraph representations for prediction to alleviate item dominance. We also generate in-session attention for denoising. Finally, we maximize mutual information between the hypergraph and line graph channels as an auxiliary task. Experiments demonstrate that our method enhances the accuracy of various recommendations, including e-commerce and multimedia recommendations. We release the code on GitHub for reproducibility.\\footnote{https://github.com/hohehohe0509/DSR-HK}", "AI": {"tldr": "\u63d0\u51fa\u591a\u901a\u9053\u63a8\u8350\u6a21\u578b\uff08\u77e5\u8bc6\u56fe\u8c31\u901a\u9053\u3001\u4f1a\u8bdd\u8d85\u56fe\u901a\u9053\u3001\u4f1a\u8bdd\u7ebf\u56fe\u901a\u9053\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53bb\u566a\u3001\u7f13\u89e3\u7269\u54c1\u4e3b\u5bfc\u95ee\u9898\u3001\u6700\u5927\u5316\u4e92\u4fe1\u606f\u7b49\u673a\u5236\u63d0\u5347\u4f1a\u8bdd\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u4f1a\u8bdd\u63a8\u8350\u6a21\u578b\u5b58\u5728\u7269\u54c1\u4ea4\u4e92\u4e3b\u5bfc\u548c\u566a\u58f0\u4f1a\u8bdd\u95ee\u9898\uff0c\u9700\u8981\u4ece\u591a\u6e90\u4fe1\u606f\u4e2d\u6355\u6349\u7528\u6237\u9690\u5f0f\u610f\u56fe\u3002", "method": "\u8bbe\u8ba1\u4e09\u901a\u9053\u6a21\u578b\uff1a\u77e5\u8bc6\u56fe\u8c31\u901a\u9053\uff08\u81ea\u9002\u5e94\u53bb\u9664\u5197\u4f59\u8fb9\uff09\u3001\u4f1a\u8bdd\u8d85\u56fe\u901a\u9053\u3001\u4f1a\u8bdd\u7ebf\u56fe\u901a\u9053\uff1b\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8d85\u56fe\u8868\u793a\u534f\u4f5c\u7f13\u89e3\u7269\u54c1\u4e3b\u5bfc\uff1b\u751f\u6210\u4f1a\u8bdd\u5185\u6ce8\u610f\u529b\u53bb\u566a\uff1b\u6700\u5927\u5316\u8d85\u56fe\u4e0e\u7ebf\u56fe\u901a\u9053\u95f4\u7684\u4e92\u4fe1\u606f\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7535\u5546\u548c\u591a\u5a92\u4f53\u63a8\u8350\u7b49\u591a\u79cd\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u901a\u9053\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u591a\u6e90\u4fe1\u606f\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u7f13\u89e3\u7269\u54c1\u4e3b\u5bfc\u95ee\u9898\u663e\u8457\u63d0\u5347\u4f1a\u8bdd\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2601.08611", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.08611", "abs": "https://arxiv.org/abs/2601.08611", "authors": ["Mark Rothermel", "Marcus Kornmann", "Marcus Rohrbach", "Anna Rohrbach"], "title": "VeriTaS: The First Dynamic Benchmark for Multimodal Automated Fact-Checking", "comment": "Preprint under review", "summary": "The growing scale of online misinformation urgently demands Automated Fact-Checking (AFC). Existing benchmarks for evaluating AFC systems, however, are largely limited in terms of task scope, modalities, domain, language diversity, realism, or coverage of misinformation types. Critically, they are static, thus subject to data leakage as their claims enter the pretraining corpora of LLMs. As a result, benchmark performance no longer reliably reflects the actual ability to verify claims. We introduce Verified Theses and Statements (VeriTaS), the first dynamic benchmark for multimodal AFC, designed to remain robust under ongoing large-scale pretraining of foundation models. VeriTaS currently comprises 24,000 real-world claims from 108 professional fact-checking organizations across 54 languages, covering textual and audiovisual content. Claims are added quarterly via a fully automated seven-stage pipeline that normalizes claim formulation, retrieves original media, and maps heterogeneous expert verdicts to a novel, standardized, and disentangled scoring scheme with textual justifications. Through human evaluation, we demonstrate that the automated annotations closely match human judgments. We commit to update VeriTaS in the future, establishing a leakage-resistant benchmark, supporting meaningful AFC evaluation in the era of rapidly evolving foundation models. We will make the code and data publicly available.", "AI": {"tldr": "VeriTaS\u662f\u9996\u4e2a\u52a8\u6001\u591a\u6a21\u6001\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\uff0c\u5305\u542b24,000\u4e2a\u771f\u5b9e\u4e16\u754c\u58f0\u660e\uff0c\u8986\u76d654\u79cd\u8bed\u8a00\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u9053\u5b63\u5ea6\u66f4\u65b0\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u5b58\u5728\u591a\u65b9\u9762\u9650\u5236\uff1a\u4efb\u52a1\u8303\u56f4\u7a84\u3001\u6a21\u6001\u5355\u4e00\u3001\u9886\u57df\u6709\u9650\u3001\u8bed\u8a00\u591a\u6837\u6027\u4e0d\u8db3\u3001\u771f\u5b9e\u6027\u4e0d\u591f\u3001\u865a\u5047\u4fe1\u606f\u7c7b\u578b\u8986\u76d6\u4e0d\u5168\uff0c\u4e14\u90fd\u662f\u9759\u6001\u57fa\u51c6\uff0c\u5bb9\u6613\u56e0\u58f0\u660e\u8fdb\u5165LLM\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u800c\u5bfc\u81f4\u6570\u636e\u6cc4\u9732\uff0c\u4f7f\u5f97\u57fa\u51c6\u6027\u80fd\u65e0\u6cd5\u53ef\u9760\u53cd\u6620\u5b9e\u9645\u6838\u67e5\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86VeriTaS\u52a8\u6001\u57fa\u51c6\uff0c\u5305\u542b24,000\u4e2a\u6765\u81ea108\u4e2a\u4e13\u4e1a\u4e8b\u5b9e\u6838\u67e5\u7ec4\u7ec7\u7684\u771f\u5b9e\u58f0\u660e\uff0c\u8986\u76d654\u79cd\u8bed\u8a00\u548c\u6587\u672c/\u89c6\u542c\u5185\u5bb9\u3002\u91c7\u7528\u5168\u81ea\u52a8\u4e03\u9636\u6bb5\u7ba1\u9053\uff1a\u6807\u51c6\u5316\u58f0\u660e\u8868\u8ff0\u3001\u68c0\u7d22\u539f\u59cb\u5a92\u4f53\u3001\u5c06\u5f02\u6784\u4e13\u5bb6\u88c1\u51b3\u6620\u5c04\u5230\u65b0\u9896\u7684\u6807\u51c6\u5316\u89e3\u8026\u8bc4\u5206\u65b9\u6848\uff08\u542b\u6587\u672c\u7406\u7531\uff09\u3002\u7ba1\u9053\u6bcf\u5b63\u5ea6\u81ea\u52a8\u66f4\u65b0\u6570\u636e\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316\u6807\u6ce8\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002\u5efa\u7acb\u4e86\u9996\u4e2a\u9632\u6570\u636e\u6cc4\u9732\u7684\u57fa\u51c6\uff0c\u652f\u6301\u5728\u57fa\u7840\u6a21\u578b\u5feb\u901f\u6f14\u8fdb\u65f6\u4ee3\u8fdb\u884c\u6709\u610f\u4e49\u7684\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u8bc4\u4f30\u3002\u4ee3\u7801\u548c\u6570\u636e\u5c06\u516c\u5f00\u3002", "conclusion": "VeriTaS\u662f\u9996\u4e2a\u52a8\u6001\u591a\u6a21\u6001\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\uff0c\u901a\u8fc7\u5b63\u5ea6\u66f4\u65b0\u548c\u6807\u51c6\u5316\u5904\u7406\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001\u57fa\u51c6\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u4e3a\u5feb\u901f\u53d1\u5c55\u7684\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e8b\u5b9e\u6838\u67e5\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2601.08705", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08705", "abs": "https://arxiv.org/abs/2601.08705", "authors": ["Miaomiao Cai", "Zhijie Zhang", "Junfeng Fang", "Zhiyong Cheng", "Xiang Wang", "Meng Wang"], "title": "RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors", "comment": null, "summary": "Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.\n  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users' auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.", "AI": {"tldr": "RMBRec\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u591a\u884c\u4e3a\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u539f\u5219\u5904\u7406\u8f85\u52a9\u884c\u4e3a\uff08\u70b9\u51fb\u3001\u52a0\u8d2d\uff09\u4e0e\u76ee\u6807\u884c\u4e3a\uff08\u8d2d\u4e70\uff09\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u591a\u884c\u4e3a\u63a8\u8350\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u8f85\u52a9\u884c\u4e3a\uff08\u5982\u70b9\u51fb\u3001\u52a0\u8d2d\uff09\u901a\u5e38\u5b58\u5728\u566a\u58f0\u3001\u5f31\u76f8\u5173\u6216\u8bed\u4e49\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u504f\u597d\u5b66\u4e60\u504f\u5dee\u548c\u6b21\u4f18\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5904\u7406\u8fd9\u79cd\u884c\u4e3a\u4e0d\u4e00\u81f4\u6027\u7684\u9c81\u68d2\u673a\u5236\u3002", "method": "\u63d0\u51faRMBRec\u6846\u67b6\uff0c\u57fa\u4e8e\u4fe1\u606f\u8bba\u9c81\u68d2\u6027\u539f\u5219\uff1a1\uff09\u8868\u793a\u9c81\u68d2\u6027\u6a21\u5757\uff08RRM\uff09\u901a\u8fc7\u6700\u5927\u5316\u7528\u6237\u8f85\u52a9\u884c\u4e3a\u4e0e\u76ee\u6807\u884c\u4e3a\u8868\u793a\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u6765\u589e\u5f3a\u5c40\u90e8\u8bed\u4e49\u4e00\u81f4\u6027\uff1b2\uff09\u4f18\u5316\u9c81\u68d2\u6027\u6a21\u5757\uff08ORM\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u8de8\u884c\u4e3a\u9884\u6d4b\u98ce\u9669\u65b9\u5dee\u6765\u5f3a\u5236\u5168\u5c40\u7a33\u5b9a\u6027\uff0c\u8fd9\u662f\u5bf9\u4e0d\u53d8\u98ce\u9669\u6700\u5c0f\u5316\u7684\u9ad8\u6548\u8fd1\u4f3c\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRMBRec\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u5404\u79cd\u566a\u58f0\u6270\u52a8\u4e0b\u4fdd\u6301\u663e\u8457\u7a33\u5b9a\u6027\u3002", "conclusion": "RMBRec\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u534f\u4f5c\uff0c\u4ee5\u7406\u8bba\u4e00\u81f4\u7684\u65b9\u5f0f\u6865\u63a5\u4e86\u8868\u793a\u7eaf\u5316\u548c\u4f18\u5316\u4e0d\u53d8\u6027\uff0c\u4e3a\u591a\u884c\u4e3a\u63a8\u8350\u4e2d\u7684\u884c\u4e3a\u4e0d\u4e00\u81f4\u95ee\u9898\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08764", "categories": ["cs.IR", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.08764", "abs": "https://arxiv.org/abs/2601.08764", "authors": ["Haven Kim", "Yupeng Hou", "Julian McAuley"], "title": "FusID: Modality-Fused Semantic IDs for Generative Music Recommendation", "comment": null, "summary": "Generative recommendation systems have achieved significant advances by leveraging semantic IDs to represent items. However, existing approaches that tokenize each modality independently face two critical limitations: (1) redundancy across modalities that reduces efficiency, and (2) failure to capture inter-modal interactions that limits item representation. We introduce FusID, a modality-fused semantic ID framework that addresses these limitations through three key components: (i) multimodal fusion that learns unified representations by jointly encoding information across modalities, (ii) representation learning that brings frequently co-occurring item embeddings closer while maintaining distinctiveness and preventing feature redundancy, and (iii) product quantization that converts the fused continuous embeddings into multiple discrete tokens to mitigate ID conflict. Evaluated on a multimodal next-song recommendation (i.e., playlist continuation) benchmark, FusID achieves zero ID conflicts, ensuring that each token sequence maps to exactly one song, mitigates codebook underutilization, and outperforms baselines in terms of MRR and Recall@k (k = 1, 5, 10, 20).", "AI": {"tldr": "FusID\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u878d\u5408\u7684\u8bed\u4e49ID\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u8de8\u6a21\u6001\u4fe1\u606f\u89e3\u51b3\u73b0\u6709\u72ec\u7acb\u6a21\u6001\u6807\u8bb0\u65b9\u6cd5\u7684\u5197\u4f59\u548c\u4ea4\u4e92\u7f3a\u5931\u95ee\u9898\uff0c\u5728\u97f3\u4e50\u63a8\u8350\u4e2d\u5b9e\u73b0\u96f6ID\u51b2\u7a81\u548c\u66f4\u597d\u7684\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u4f7f\u7528\u8bed\u4e49ID\u8868\u793a\u7269\u54c1\uff0c\u4f46\u72ec\u7acb\u6807\u8bb0\u6bcf\u4e2a\u6a21\u6001\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u8de8\u6a21\u6001\u5197\u4f59\u964d\u4f4e\u6548\u7387\uff1b2) \u65e0\u6cd5\u6355\u6349\u6a21\u6001\u95f4\u4ea4\u4e92\u9650\u5236\u7269\u54c1\u8868\u793a\u80fd\u529b\u3002", "method": "FusID\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u591a\u6a21\u6001\u878d\u5408\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u8de8\u6a21\u6001\u4fe1\u606f\u5b66\u4e60\u7edf\u4e00\u8868\u793a\uff1b2) \u8868\u793a\u5b66\u4e60\uff0c\u4f7f\u9891\u7e41\u5171\u73b0\u7684\u7269\u54c1\u5d4c\u5165\u66f4\u63a5\u8fd1\uff0c\u540c\u65f6\u4fdd\u6301\u533a\u5206\u6027\u5e76\u9632\u6b62\u7279\u5f81\u5197\u4f59\uff1b3) \u4ea7\u54c1\u91cf\u5316\uff0c\u5c06\u878d\u5408\u7684\u8fde\u7eed\u5d4c\u5165\u8f6c\u6362\u4e3a\u591a\u4e2a\u79bb\u6563\u6807\u8bb0\u4ee5\u7f13\u89e3ID\u51b2\u7a81\u3002", "result": "\u5728\u591a\u6a21\u6001\u4e0b\u4e00\u9996\u6b4c\u66f2\u63a8\u8350\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFusID\u5b9e\u73b0\u4e86\u96f6ID\u51b2\u7a81\uff08\u786e\u4fdd\u6bcf\u4e2a\u6807\u8bb0\u5e8f\u5217\u6620\u5c04\u5230\u552f\u4e00\u6b4c\u66f2\uff09\uff0c\u7f13\u89e3\u4e86\u7801\u672c\u5229\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u5728MRR\u548cRecall@k\uff08k=1,5,10,20\uff09\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FusID\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u7684\u8bed\u4e49ID\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5197\u4f59\u548c\u4ea4\u4e92\u7f3a\u5931\u95ee\u9898\uff0c\u5728\u4fdd\u6301ID\u552f\u4e00\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u4e3a\u751f\u6210\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u7269\u54c1\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2601.08816", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08816", "abs": "https://arxiv.org/abs/2601.08816", "authors": ["Weixin Chen", "Yuhan Zhao", "Jingyuan Huang", "Zihe Ye", "Clark Mingxuan Ju", "Tong Zhao", "Neil Shah", "Li Chen", "Yongfeng Zhang"], "title": "MemRec: Collaborative Memory-Augmented Agentic Recommender System", "comment": null, "summary": "The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com", "AI": {"tldr": "MemRec\u662f\u4e00\u4e2a\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u67b6\u6784\u89e3\u8026\u63a8\u7406\u4e0e\u8bb0\u5fc6\u7ba1\u7406\uff0c\u5229\u7528\u4e13\u95e8\u7684LM_Mem\u7ba1\u7406\u52a8\u6001\u534f\u4f5c\u8bb0\u5fc6\u56fe\uff0c\u4e3a\u4e0b\u6e38LLM_Rec\u63d0\u4f9b\u9ad8\u4fe1\u53f7\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u9ad8\u6548\u534f\u4f5c\u589e\u5f3a\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u4ee3\u7406\u4f9d\u8d56\u5b64\u7acb\u8bb0\u5fc6\uff0c\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u534f\u4f5c\u4fe1\u53f7\u3002\u9700\u8981\u89e3\u51b3\u4e24\u4e2a\u6311\u6218\uff1a1) \u4ece\u5e9e\u5927\u56fe\u4e0a\u4e0b\u6587\u4e2d\u63d0\u53d6\u4fe1\u606f\u800c\u4e0d\u589e\u52a0\u63a8\u7406\u4ee3\u7406\u7684\u8ba4\u77e5\u8d1f\u62c5\uff1b2) \u9ad8\u6548\u6f14\u5316\u534f\u4f5c\u8bb0\u5fc6\u800c\u4e0d\u4ea7\u751f\u8fc7\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faMemRec\u6846\u67b6\uff0c\u67b6\u6784\u4e0a\u89e3\u8026\u63a8\u7406\u4e0e\u8bb0\u5fc6\u7ba1\u7406\u3002\u5f15\u5165\u4e13\u95e8\u7684LM_Mem\u7ba1\u7406\u52a8\u6001\u534f\u4f5c\u8bb0\u5fc6\u56fe\uff0c\u901a\u8fc7\u9ad8\u6548\u68c0\u7d22\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u5f02\u6b65\u56fe\u4f20\u64ad\u7ba1\u9053\uff0c\u4e3a\u4e0b\u6e38LLM_Rec\u63d0\u4f9b\u5408\u6210\u7684\u9ad8\u4fe1\u53f7\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMemRec\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u67b6\u6784\u5206\u6790\u786e\u8ba4\u4e86\u5176\u7075\u6d3b\u6027\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e73\u8861\u4e86\u63a8\u7406\u8d28\u91cf\u3001\u6210\u672c\u548c\u9690\u79c1\uff0c\u652f\u6301\u5305\u62ec\u672c\u5730\u5f00\u6e90\u6a21\u578b\u5728\u5185\u7684\u591a\u6837\u5316\u90e8\u7f72\u3002", "conclusion": "MemRec\u901a\u8fc7\u67b6\u6784\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u534f\u4f5c\u8bb0\u5fc6\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u63a8\u8350\u6027\u80fd\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u5728\u667a\u80fd\u4f53\u65f6\u4ee3\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}

{"id": "2510.18104", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18104", "abs": "https://arxiv.org/abs/2510.18104", "authors": ["Joeran Beel", "Bela Gipp", "Tobias Vente", "Moritz Baumgart", "Philipp Meister"], "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs", "comment": null, "summary": "Recommender-systems research has accelerated model and evaluation advances,\nyet largely neglects automating the research process itself. We argue for a\nshift from narrow AutoRecSys tools -- focused on algorithm selection and\nhyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab\n(AutoRecLab) that integrates end-to-end automation: problem ideation,\nliterature analysis, experimental design and execution, result interpretation,\nmanuscript drafting, and provenance logging. Drawing on recent progress in\nautomated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),\nwe outline an agenda for the RecSys community: (1) build open AutoRecLab\nprototypes that combine LLM-driven ideation and reporting with automated\nexperimentation; (2) establish benchmarks and competitions that evaluate agents\non producing reproducible RecSys findings with minimal human input; (3) create\nreview venues for transparently AI-generated submissions; (4) define standards\nfor attribution and reproducibility via detailed research logs and metadata;\nand (5) foster interdisciplinary dialogue on ethics, governance, privacy, and\nfairness in autonomous research. Advancing this agenda can increase research\nthroughput, surface non-obvious insights, and position RecSys to contribute to\nemerging Artificial Research Intelligence. We conclude with a call to organise\na community retreat to coordinate next steps and co-author guidance for the\nresponsible integration of automated research systems."}
{"id": "2510.18239", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18239", "abs": "https://arxiv.org/abs/2510.18239", "authors": ["Yunjiang Jiang", "Ayush Agarwal", "Yang Liu", "Bi Xue"], "title": "LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling", "comment": "16 pages", "summary": "Scaling large recommendation systems requires advancing three major\nfrontiers: processing longer user histories, expanding candidate sets, and\nincreasing model capacity. While promising, transformers' computational cost\nscales quadratically with the user sequence length and linearly with the number\nof candidates. This trade-off makes it prohibitively expensive to expand\ncandidate sets or increase sequence length at inference, despite the\nsignificant performance improvements.\n  We introduce \\textbf{LIME}, a novel architecture that resolves this\ntrade-off. Through two key innovations, LIME fundamentally reduces\ncomputational complexity. First, low-rank ``link embeddings\" enable\npre-computation of attention weights by decoupling user and candidate\ninteractions, making the inference cost nearly independent of candidate set\nsize. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the\ncomplexity with respect to user sequence length from quadratic ($O(N^2)$) to\nlinear ($O(N)$).\n  Experiments on public and industrial datasets show LIME achieves near-parity\nwith state-of-the-art transformers but with a 10$\\times$ inference speedup on\nlarge candidate sets or long sequence lengths. When tested on a major\nrecommendation platform, LIME improved user engagement while maintaining\nminimal inference costs with respect to candidate set size and user history\nlength, establishing a new paradigm for efficient and expressive recommendation\nsystems."}
{"id": "2510.18277", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.18277", "abs": "https://arxiv.org/abs/2510.18277", "authors": ["Nikolaos Belibasakis", "Anastasios Giannaros", "Ioanna Giannoukou", "Spyros Sioutas"], "title": "Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization and Query-Driven Insights", "comment": null, "summary": "The increasing number of data a booking platform such as Booking.com and\nAirBnB offers make it challenging for interested parties to browse through the\navailable accommodations and analyze reviews in an efficient way. Efforts have\nbeen made from the booking platform providers to utilize recommender systems in\nan effort to enable the user to filter the results by factors such as stars,\namenities, cost but most valuable insights can be provided by the unstructured\ntext-based reviews. Going through these reviews one-by-one requires a\nsubstantial amount of time to be devoted while a respectable percentage of the\nreviews won't provide to the user what they are actually looking for.\n  This research publication explores how Large Language Models (LLMs) can\nenhance short rental apartments recommendations by summarizing and mining key\ninsights from user reviews. The web application presented in this paper, named\n\"instaGuide\", automates the procedure of isolating the text-based user reviews\nfrom a property on the Booking.com platform, synthesizing the summary of the\nreviews, and enabling the user to query specific aspects of the property in an\neffort to gain feedback on their personal questions/criteria.\n  During the development of the instaGuide tool, numerous LLM models were\nevaluated based on accuracy, cost, and response quality. The results suggest\nthat the LLM-powered summarization reduces significantly the amount of time the\nusers need to devote on their search for the right short rental apartment,\nimproving the overall decision-making procedure."}
{"id": "2510.18364", "categories": ["cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18364", "abs": "https://arxiv.org/abs/2510.18364", "authors": ["Quim Motger", "Xavier Franch", "Vincenzo Gervasi", "Jordi Marco"], "title": "Evaluating LLM-Based Mobile App Recommendations: An Empirical Study", "comment": "Under review", "summary": "Large Language Models (LLMs) are increasingly used to recommend mobile\napplications through natural language prompts, offering a flexible alternative\nto keyword-based app store search. Yet, the reasoning behind these\nrecommendations remains opaque, raising questions about their consistency,\nexplainability, and alignment with traditional App Store Optimization (ASO)\nmetrics. In this paper, we present an empirical analysis of how widely-used\ngeneral purpose LLMs generate, justify, and rank mobile app recommendations.\nOur contributions are: (i) a taxonomy of 16 generalizable ranking criteria\nelicited from LLM outputs; (ii) a systematic evaluation framework to analyse\nrecommendation consistency and responsiveness to explicit ranking instructions;\nand (iii) a replication package to support reproducibility and future research\non AI-based recommendation systems. Our findings reveal that LLMs rely on a\nbroad yet fragmented set of ranking criteria, only partially aligned with\nstandard ASO metrics. While top-ranked apps tend to be consistent across runs,\nvariability increases with ranking depth and search specificity. LLMs exhibit\nvarying sensitivity to explicit ranking instructions - ranging from substantial\nadaptations to near-identical outputs - highlighting their complex reasoning\ndynamics in conversational app discovery. Our results aim to support end-users,\napp developers, and recommender-systems researchers in navigating the emerging\nlandscape of conversational app discovery."}
{"id": "2510.18527", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.18527", "abs": "https://arxiv.org/abs/2510.18527", "authors": ["Hongru Song", "Yu-an Liu", "Ruqing Zhang", "Jiafeng Guo", "Maarten de Rijke", "Sen Li", "Wenjun Peng", "Fuyu Lv", "Xueqi Cheng"], "title": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search", "comment": "16 pages", "summary": "Product search is a crucial component of modern e-commerce platforms, with\nbillions of user queries every day. In product search systems, first-stage\nretrieval should achieve high recall while ensuring efficient online\ndeployment. Sparse retrieval is particularly attractive in this context due to\nits interpretability and storage efficiency. However, sparse retrieval methods\nsuffer from severe vocabulary mismatch issues, leading to suboptimal\nperformance in product search scenarios.With their potential for semantic\nanalysis, large language models (LLMs) offer a promising avenue for mitigating\nvocabulary mismatch issues and thereby improving retrieval quality. Directly\napplying LLMs to sparse retrieval in product search exposes two key\nchallenges:(1)Queries and product titles are typically short and highly\nsusceptible to LLM-induced hallucinations, such as generating irrelevant\nexpansion terms or underweighting critical literal terms like brand names and\nmodel numbers;(2)The large vocabulary space of LLMs leads to difficulty in\ninitializing training effectively, making it challenging to learn meaningful\nsparse representations in such ultra-high-dimensional spaces.To address these\nchallenges, we propose PROSPER, a framework for PROduct search leveraging LLMs\nas SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that\nalleviates hallucination in lexical expansion by reinforcing underweighted\nliteral terms through a residual compensation mechanism; and (2)A lexical\nfocusing window that facilitates effective training initialization via a\ncoarse-to-fine sparsification strategy.Extensive offline and online experiments\nshow that PROSPER significantly outperforms sparse baselines and achieves\nrecall performance comparable to advanced dense retrievers, while also\nachieving revenue increments online."}

{"id": "2510.12815", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12815", "abs": "https://arxiv.org/abs/2510.12815", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Energy-Guided Diffusion Sampling for Long-Term User Behavior Prediction in Reinforcement Learning-based Recommendation", "comment": "CIKM'25", "summary": "Reinforcement learning-based recommender systems (RL4RS) have gained\nattention for their ability to adapt to dynamic user preferences. However,\nthese systems face challenges, particularly in offline settings, where data\ninefficiency and reliance on pre-collected trajectories limit their broader\napplicability. While offline reinforcement learning methods leverage extensive\ndatasets to address these issues, they often struggle with noisy data and fail\nto capture long-term user preferences, resulting in suboptimal recommendation\npolicies. To overcome these limitations, we propose Diffusion-enhanced\nActor-Critic for Offline RL4RS (DAC4Rec), a novel framework that integrates\ndiffusion processes with reinforcement learning to model complex user\npreferences more effectively. DAC4Rec leverages the denoising capabilities of\ndiffusion models to enhance the robustness of offline RL algorithms and\nincorporates a Q-value-guided policy optimization strategy to better handle\nsuboptimal trajectories. Additionally, we introduce an energy-based sampling\nstrategy to reduce randomness during recommendation generation, ensuring more\ntargeted and reliable outcomes. We validate the effectiveness of DAC4Rec\nthrough extensive experiments on six real-world offline datasets and in an\nonline simulation environment, demonstrating its ability to optimize long-term\nuser preferences. Furthermore, we show that the proposed diffusion policy can\nbe seamlessly integrated into other commonly used RL algorithms in RL4RS,\nhighlighting its versatility and wide applicability.", "AI": {"tldr": "\u63d0\u51faDAC4Rec\u6846\u67b6\uff0c\u5c06\u6269\u6563\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u589e\u5f3a\u79bb\u7ebf\u63a8\u8350\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u91c7\u7528Q\u503c\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u6765\u5904\u7406\u6b21\u4f18\u8f68\u8ff9\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u6570\u636e\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u9884\u6536\u96c6\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u566a\u58f0\u6570\u636e\u4e14\u65e0\u6cd5\u6709\u6548\u6355\u6349\u957f\u671f\u7528\u6237\u504f\u597d\uff0c\u5bfc\u81f4\u63a8\u8350\u7b56\u7565\u4e0d\u7406\u60f3\u3002", "method": "\u96c6\u6210\u6269\u6563\u8fc7\u7a0b\u4e0e\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u80fd\u529b\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u91c7\u7528Q\u503c\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u7b56\u7565\uff1b\u5f15\u5165\u57fa\u4e8e\u80fd\u91cf\u7684\u91c7\u6837\u7b56\u7565\u51cf\u5c11\u63a8\u8350\u751f\u6210\u7684\u968f\u673a\u6027\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u79bb\u7ebf\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6a21\u62df\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u80fd\u591f\u4f18\u5316\u957f\u671f\u7528\u6237\u504f\u597d\uff0c\u4e14\u6269\u6563\u7b56\u7565\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6RL\u7b97\u6cd5\u4e2d\u3002", "conclusion": "DAC4Rec\u6846\u67b6\u901a\u8fc7\u6269\u6563\u589e\u5f3a\u7684actor-critic\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u63a8\u8350\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.12816", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12816", "abs": "https://arxiv.org/abs/2510.12816", "authors": ["Xiaocong Chen", "Siyu Wang", "Lina Yao"], "title": "Maximum In-Support Return Modeling for Dynamic Recommendation with Language Model Prior", "comment": "CIKM'25", "summary": "Reinforcement Learning-based recommender systems (RLRS) offer an effective\nway to handle sequential recommendation tasks but often face difficulties in\nreal-world settings, where user feedback data can be sub-optimal or sparse. In\nthis paper, we introduce MDT4Rec, an offline RLRS framework that builds on the\nDecision Transformer (DT) to address two major challenges: learning from\nsub-optimal histories and representing complex user-item interactions. First,\nMDT4Rec shifts the trajectory stitching procedure from the training phase to\naction inference, allowing the system to shorten its historical context when\nnecessary and thereby ignore negative or unsuccessful past experiences. Second,\nMDT4Rec initializes DT with a pre-trained large language model (LLM) for\nknowledge transfer, replaces linear embedding layers with Multi-Layer\nPerceptrons (MLPs) for more flexible representations, and employs Low-Rank\nAdaptation (LoRA) to efficiently fine-tune only a small subset of parameters.\nWe evaluate MDT4Rec on five public datasets and in an online simulation\nenvironment, demonstrating that it outperforms existing methods.", "AI": {"tldr": "MDT4Rec\u662f\u4e00\u4e2a\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u62fc\u63a5\u4ece\u8bad\u7ec3\u9636\u6bb5\u8f6c\u79fb\u5230\u52a8\u4f5c\u63a8\u65ad\uff0c\u5e76\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\u6765\u6539\u5584\u4ece\u6b21\u4f18\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4ece\u6b21\u4f18\u7528\u6237\u53cd\u9988\u6570\u636e\u4e2d\u5b66\u4e60\u548c\u8868\u793a\u590d\u6742\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u3002", "method": "1) \u5c06\u8f68\u8ff9\u62fc\u63a5\u4ece\u8bad\u7ec3\u9636\u6bb5\u8f6c\u79fb\u5230\u52a8\u4f5c\u63a8\u65ad\u9636\u6bb5\uff0c\u5141\u8bb8\u7cfb\u7edf\u5728\u5fc5\u8981\u65f6\u7f29\u77ed\u5386\u53f2\u4e0a\u4e0b\u6587\uff1b2) \u4f7f\u7528\u9884\u8bad\u7ec3LLM\u521d\u59cb\u5316\u51b3\u7b56\u53d8\u6362\u5668\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff1b3) \u7528\u591a\u5c42\u611f\u77e5\u673a\u66ff\u6362\u7ebf\u6027\u5d4c\u5165\u5c42\uff1b4) \u91c7\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u9ad8\u6548\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u5728\u7ebf\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cMDT4Rec\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MDT4Rec\u901a\u8fc7\u6539\u8fdb\u7684\u8f68\u8ff9\u62fc\u63a5\u7b56\u7565\u548c\u57fa\u4e8eLLM\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u6709\u6548\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u63a8\u8350\u7cfb\u7edf\u5728\u6b21\u4f18\u6570\u636e\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12959", "abs": "https://arxiv.org/abs/2510.12959", "authors": ["Md Aminul Islam", "Elena Zheleva", "Ren Wang"], "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering", "comment": null, "summary": "User historical interaction data is the primary signal for learning user\npreferences in collaborative filtering (CF). However, the training data often\nexhibits a long-tailed distribution, where only a few items have the majority\nof interactions. CF models trained directly on such imbalanced data are prone\nto learning popularity bias, which reduces personalization and leads to\nsuboptimal recommendation quality. Graph Neural Networks (GNNs), while\neffective for CF due to their message passing mechanism, can further propagate\nand amplify popularity bias through their aggregation process. Existing\napproaches typically address popularity bias by modifying training objectives\nbut fail to directly counteract the bias propagated during GNN's neighborhood\naggregation. Applying weights to interactions during aggregation can help\nalleviate this problem, yet it risks distorting model learning due to unstable\nnode representations in the early stages of training. In this paper, we propose\na Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias\nin GNN-based CF and operates directly on pre-trained embeddings without\nrequiring retraining. By estimating interaction-level popularity and removing\npopularity components from node representations via a popularity direction\nvector, PPD reduces bias while preserving user preferences. Experimental\nresults show that our method outperforms state-of-the-art approaches for\npopularity bias correction in GNN-based CF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540e\u5904\u7406\u6d41\u884c\u5ea6\u53bb\u504f\u65b9\u6cd5PPD\uff0c\u7528\u4e8e\u7ea0\u6b63\u57fa\u4e8eGNN\u7684\u534f\u540c\u8fc7\u6ee4\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b", "motivation": "GNN\u6a21\u578b\u5728\u534f\u540c\u8fc7\u6ee4\u4e2d\u4f1a\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u673a\u5236\u4f20\u64ad\u548c\u653e\u5927\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4fee\u6539\u8bad\u7ec3\u76ee\u6807\u4f46\u672a\u80fd\u76f4\u63a5\u5bf9\u6297GNN\u805a\u5408\u8fc7\u7a0b\u4e2d\u7684\u504f\u5dee\u4f20\u64ad", "method": "\u901a\u8fc7\u4f30\u8ba1\u4ea4\u4e92\u7ea7\u522b\u7684\u6d41\u884c\u5ea6\uff0c\u5e76\u4f7f\u7528\u6d41\u884c\u5ea6\u65b9\u5411\u5411\u91cf\u4ece\u8282\u70b9\u8868\u793a\u4e2d\u79fb\u9664\u6d41\u884c\u5ea6\u6210\u5206\uff0c\u4ece\u800c\u51cf\u5c11\u504f\u5dee\u540c\u65f6\u4fdd\u7559\u7528\u6237\u504f\u597d", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728GNN\u534f\u540c\u8fc7\u6ee4\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u6821\u6b63\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "PPD\u65b9\u6cd5\u80fd\u6709\u6548\u7ea0\u6b63GNN\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b"}}
{"id": "2510.13095", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13095", "abs": "https://arxiv.org/abs/2510.13095", "authors": ["Yingchen zhang", "Ruqing zhang", "Jiafeng Guo", "Wenjun Peng", "Sen Li", "Fuyu Lv"], "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for Generative Retrieval", "comment": null, "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large\nlanguage models (LLMs) to autoregressively generate document identifiers\n(docids) relevant to a given query. Prior works have focused on leveraging the\ngenerative capabilities of LLMs to improve GR, while overlooking that their\nreasoning capabilities could likewise help. This raises a key question: Can\nexplicit reasoning benefit GR? To investigate, we first conduct a preliminary\nstudy where an LLM is prompted to generate free-form chain-of-thought (CoT)\nreasoning before performing constrained docid decoding. Although this method\noutperforms standard GR, the generated reasoning tends to be verbose and poorly\naligned with the docid space. These limitations motivate the development of a\nreasoning mechanism better tailored to GR.\n  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented\nframework for GR that converts free-form CoT reasoning into a compact,\nstructured format, and iteratively refines the reasoning during the retrieval\nprocess. R4R augments an existing GR method by leveraging a reasoning-capable\nLLM that has been instruction-tuned for GR. At inference time, R4R first uses\nthe LLM to generate an initial structured reasoning; then the same LLM\nalternates between (i) constrained decoding with the chosen GR method to\nproduce candidate docids and (ii) updating the reasoning based on retrieval\nresults to improve the next round. R4R does not require additional models or\ntraining, and instead a single LLM serves as both the reasoning generator and\nthe retriever. Extensive experiments on Natural Questions, MS MARCO, and a\nreal-world item-search benchmark validate the effectiveness of R4R.", "AI": {"tldr": "\u63d0\u51faR4R\u6846\u67b6\uff0c\u5c06\u81ea\u7531\u5f62\u5f0f\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\uff0c\u5e76\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\uff0c\u4ee5\u589e\u5f3a\u751f\u6210\u5f0f\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u5229\u7528LLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u63a8\u7406\u80fd\u529b\u540c\u6837\u53ef\u4ee5\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002\u521d\u6b65\u7814\u7a76\u53d1\u73b0\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u6539\u5584\u68c0\u7d22\uff0c\u4f46\u5b58\u5728\u5197\u957f\u548c\u4e0e\u6587\u6863\u6807\u8bc6\u7a7a\u95f4\u5bf9\u9f50\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "R4R\u6846\u67b6\uff1a1) \u751f\u6210\u521d\u59cb\u7ed3\u6784\u5316\u63a8\u7406\uff1b2) \u4ea4\u66ff\u8fdb\u884c\u7ea6\u675f\u89e3\u7801\u751f\u6210\u5019\u9009\u6587\u6863\u6807\u8bc6\u548c\u57fa\u4e8e\u68c0\u7d22\u7ed3\u679c\u66f4\u65b0\u63a8\u7406\uff1b\u4f7f\u7528\u5355\u4e2aLLM\u540c\u65f6\u4f5c\u4e3a\u63a8\u7406\u751f\u6210\u5668\u548c\u68c0\u7d22\u5668\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u8bad\u7ec3\u3002", "result": "\u5728Natural Questions\u3001MS MARCO\u548c\u771f\u5b9e\u4e16\u754c\u5546\u54c1\u641c\u7d22\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86R4R\u7684\u6709\u6548\u6027\u3002", "conclusion": "R4R\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u6210\u529f\u5c06LLMs\u7684\u63a8\u7406\u80fd\u529b\u878d\u5165\u751f\u6210\u5f0f\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2510.13193", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13193", "abs": "https://arxiv.org/abs/2510.13193", "authors": ["Yikuan Hu", "Jifeng Zhu", "Lanrui Tang", "Chen Huang"], "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG", "comment": null, "summary": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG.", "AI": {"tldr": "REMINDRAG\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u56fe\u904d\u5386\u548c\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6210\u672c\u6548\u76ca\u7684\u540c\u65f6\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KG-RAG\u7cfb\u7edf\u96be\u4ee5\u5728\u7cfb\u7edf\u6709\u6548\u6027\u548c\u6210\u672c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u6709\u6548\u534f\u540c\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u6216\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u91c7\u7528LLM\u5f15\u5bfc\u7684\u56fe\u904d\u5386\u7b56\u7565\uff0c\u5305\u62ec\u8282\u70b9\u63a2\u7d22\u3001\u8282\u70b9\u5229\u7528\u548c\u8bb0\u5fc6\u91cd\u653e\uff0c\u901a\u8fc7KG\u8fb9\u5d4c\u5165\u8bb0\u5fc6\u904d\u5386\u7ecf\u9a8c\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86REMINDRAG\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548cLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "REMINDRAG\u901a\u8fc7\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\u6210\u529f\u5e73\u8861\u4e86KG-RAG\u7cfb\u7edf\u7684\u6709\u6548\u6027\u548c\u6210\u672c\u6548\u7387\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13217", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13217", "abs": "https://arxiv.org/abs/2510.13217", "authors": ["Nilesh Gupta", "Wei-Cheng Chang", "Ngot Bui", "Cho-Jui Hsieh", "Inderjit S. Dhillon"], "title": "LLM-guided Hierarchical Retrieval", "comment": null, "summary": "Modern IR systems are increasingly tasked with answering complex,\nmulti-faceted queries that require deep reasoning rather than simple keyword or\nsemantic matching. While LLM-based IR has shown great promise, the prevailing\nretrieve-then-rerank paradigm inherits the limitations of embedding-based\nretrieval; parametric generative approaches are difficult to update with new\ninformation; and long-context methods that place the entire corpus in context\nare computationally infeasible for large document collections. To address these\nchallenges, we introduce LATTICE, a hierarchical retrieval framework that\nenables an LLM to reason over and navigate large corpora with logarithmic\nsearch complexity by imposing a semantic tree structure on the corpus. Our\napproach consists of two stages: (1) an offline phase that organizes the corpus\ninto a semantic hierarchy via either a bottom-up agglomerative strategy or a\ntop-down divisive strategy using multi-level summaries and (2) an online\ntraversal phase where a search LLM navigates this tree. A central challenge in\nsuch LLM-guided search is that the model's relevance judgments are noisy,\ncontext-dependent, and unaware of the hierarchy, making cross-branch and\ncross-level comparisons difficult. To overcome this, we propose a traversal\nalgorithm that estimates calibrated latent relevance scores from local LLM\noutputs and aggregates them into a global path relevance metric. Our\ntraining-free framework achieves state-of-the-art zero-shot performance on the\nreasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in\nRecall@100 and 5% in nDCG@10 over the next best zero-shot baseline.\nFurthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains\ncomparable results on BRIGHT subsets that use a static corpus for evaluation.", "AI": {"tldr": "LATTICE\u662f\u4e00\u4e2a\u5206\u5c42\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u6811\u7ed3\u6784\u7ec4\u7ec7\u8bed\u6599\u5e93\uff0c\u4f7fLLM\u80fd\u591f\u4ee5\u5bf9\u6570\u641c\u7d22\u590d\u6742\u5ea6\u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u8fdb\u884c\u63a8\u7406\u548c\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709IR\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u591a\u9762\u67e5\u8be2\u65f6\u7684\u5c40\u9650\u6027\uff1a\u68c0\u7d22-\u91cd\u6392\u5e8f\u8303\u5f0f\u7ee7\u627f\u5d4c\u5165\u68c0\u7d22\u7684\u7f3a\u9677\uff0c\u53c2\u6570\u5316\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u66f4\u65b0\u65b0\u4fe1\u606f\uff0c\u957f\u4e0a\u4e0b\u6587\u65b9\u6cd5\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u81ea\u5e95\u5411\u4e0a\u805a\u5408\u6216\u81ea\u9876\u5411\u4e0b\u5206\u5272\u7b56\u7565\u6784\u5efa\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff1b\u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u641c\u7d22LLM\u5bfc\u822a\u6811\u7ed3\u6784\uff0c\u901a\u8fc7\u6821\u51c6\u6f5c\u5728\u76f8\u5173\u6027\u8bc4\u5206\u7b97\u6cd5\u5904\u7406LLM\u5224\u65ad\u7684\u566a\u58f0\u95ee\u9898\u3002", "result": "\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0cRecall@100\u63d0\u53479%\uff0cnDCG@10\u63d0\u53475%\uff1b\u4e0e\u5fae\u8c03SOTA\u65b9\u6cd5DIVER-v2\u5728\u9759\u6001\u8bed\u6599\u8bc4\u4f30\u5b50\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "LATTICE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u7684\u63a8\u7406\u5bfc\u822a\u95ee\u9898\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u7ed3\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u68c0\u7d22\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8fbe\u5230\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2510.13229", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13229", "abs": "https://arxiv.org/abs/2510.13229", "authors": ["Yi Zhang", "Lili Xie", "Ruihong Qiu", "Jiajun Liu", "Sen Wang"], "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation", "comment": "ICDM 2025 Accepted Paper", "summary": "Recommender systems (RecSys) have become critical tools for enhancing user\nengagement by delivering personalized content across diverse digital platforms.\nRecent advancements in large language models (LLMs) demonstrate significant\npotential for improving RecSys, primarily due to their exceptional\ngeneralization capabilities and sophisticated contextual understanding, which\nfacilitate the generation of flexible and interpretable recommendations.\nHowever, the direct deployment of LLMs as primary recommendation policies\npresents notable challenges, including persistent latency issues stemming from\nfrequent API calls and inherent model limitations such as hallucinations and\nbiases. To address these issues, this paper proposes a novel offline\nreinforcement learning (RL) framework that leverages imitation learning from\nLLM-generated trajectories. Specifically, inverse reinforcement learning is\nemployed to extract robust reward models from LLM demonstrations. This approach\nnegates the need for LLM fine-tuning, thereby substantially reducing\ncomputational overhead. Simultaneously, the RL policy is guided by the\ncumulative rewards derived from these demonstrations, effectively transferring\nthe semantic insights captured by the LLM. Comprehensive experiments conducted\non two benchmark datasets validate the effectiveness of the proposed method,\ndemonstrating superior performance when compared against state-of-the-art\nRL-based and in-context learning baselines. The code can be found at\nhttps://github.com/ArronDZhang/IL-Rec.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u4eceLLM\u751f\u6210\u7684\u8f68\u8ff9\u4e2d\u63d0\u53d6\u5956\u52b1\u6a21\u578b\uff0c\u907f\u514dLLM\u5fae\u8c03\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u76f4\u63a5\u90e8\u7f72\u5b58\u5728\u5ef6\u8fdf\u3001\u5e7b\u89c9\u548c\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u9006\u5f3a\u5316\u5b66\u4e60\u4eceLLM\u6f14\u793a\u4e2d\u63d0\u53d6\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u63a8\u8350\u7b56\u7565\uff0c\u65e0\u9700LLM\u5fae\u8c03\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8f6c\u79fb\u5230\u63a8\u8350\u7b56\u7565\u4e2d\uff0c\u89e3\u51b3\u4e86LLM\u76f4\u63a5\u90e8\u7f72\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2510.13359", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13359", "abs": "https://arxiv.org/abs/2510.13359", "authors": ["Yuki Yada", "Sho Akiyama", "Ryo Watanabe", "Yuta Ueno", "Yusuke Shido", "Andre Rusli"], "title": "Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models", "comment": "Accepted to ACM RecSys 2025 (Spotlight)", "summary": "On large-scale e-commerce platforms with tens of millions of active monthly\nusers, recommending visually similar products is essential for enabling users\nto efficiently discover items that align with their preferences. This study\npresents the application of a vision-language model (VLM) -- which has\ndemonstrated strong performance in image recognition and image-text retrieval\ntasks -- to product recommendations on Mercari, a major consumer-to-consumer\nmarketplace used by more than 20 million monthly users in Japan. Specifically,\nwe fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using\none million product image-title pairs from Mercari collected over a three-month\nperiod, and developed an image encoder for generating item embeddings used in\nthe recommendation system. Our evaluation comprised an offline analysis of\nhistorical interaction logs and an online A/B test in a production environment.\nIn offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared\nwith the baseline. In the online A/B test, the click-through rate improved by\n50% whereas the conversion rate improved by 14% compared with the existing\nmodel. These results demonstrate the effectiveness of VLM-based encoders for\ne-commerce product recommendations and provide practical insights into the\ndevelopment of visual similarity-based recommendation systems.", "AI": {"tldr": "\u5728Mercari\u7535\u5546\u5e73\u53f0\u4e0a\u5e94\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5546\u54c1\u63a8\u8350\uff0c\u901a\u8fc7\u5fae\u8c03SigLIP\u6a21\u578b\u5e76\u4f7f\u7528\u5546\u54c1\u56fe\u50cf-\u6807\u9898\u5bf9\u751f\u6210\u5d4c\u5165\uff0c\u5728\u79bb\u7ebf\u548c\u5728\u7ebf\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u5728\u62e5\u6709\u6570\u5343\u4e07\u6708\u6d3b\u8dc3\u7528\u6237\u7684\u5927\u89c4\u6a21\u7535\u5546\u5e73\u53f0\u4e0a\uff0c\u63a8\u8350\u89c6\u89c9\u76f8\u4f3c\u5546\u54c1\u5bf9\u4e8e\u5e2e\u52a9\u7528\u6237\u9ad8\u6548\u53d1\u73b0\u7b26\u5408\u504f\u597d\u7684\u5546\u54c1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5fae\u8c03\u57fa\u4e8esigmoid\u5bf9\u6bd4\u635f\u5931\u7684SigLIP\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528100\u4e07\u4e2aMercari\u5546\u54c1\u56fe\u50cf-\u6807\u9898\u5bf9\u5f00\u53d1\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u7684\u56fe\u50cf\u7f16\u7801\u5668\u3002", "result": "\u79bb\u7ebf\u5206\u6790\u4e2dnDCG@5\u63d0\u53479.1%\uff1b\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u70b9\u51fb\u7387\u63d0\u534750%\uff0c\u8f6c\u5316\u7387\u63d0\u534714%\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7f16\u7801\u5668\u5728\u7535\u5546\u5546\u54c1\u63a8\u8350\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u76f8\u4f3c\u6027\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2510.13371", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13371", "abs": "https://arxiv.org/abs/2510.13371", "authors": ["Jiin Park", "Misuk Kim"], "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation", "comment": "18 pages", "summary": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations.", "AI": {"tldr": "MADRec\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u65b9\u9762\u9a71\u52a8\u63a8\u8350\u4ee3\u7406\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u63d0\u53d6\u8bc4\u8bba\u4e2d\u7684\u591a\u65b9\u9762\u4fe1\u606f\u6784\u5efa\u7528\u6237\u548c\u7269\u54c1\u753b\u50cf\uff0c\u5b9e\u73b0\u76f4\u63a5\u63a8\u8350\u3001\u5e8f\u5217\u63a8\u8350\u548c\u89e3\u91ca\u751f\u6210\uff0c\u5728\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5c06LLM\u96c6\u6210\u5230\u63a8\u8350\u7cfb\u7edf\u7684\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u7b80\u5355\u6587\u672c\u751f\u6210\u6216\u9759\u6001\u63d0\u793a\u63a8\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u504f\u597d\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u65b9\u9762\u7c7b\u522b\u7684\u603b\u7ed3\u751f\u6210\u7ed3\u6784\u5316\u753b\u50cf\uff0c\u5e94\u7528\u91cd\u6392\u5e8f\u6784\u5efa\u9ad8\u5bc6\u5ea6\u8f93\u5165\uff0c\u5f53\u8f93\u51fa\u4e2d\u7f3a\u5c11\u771f\u5b9e\u7269\u54c1\u65f6\u4f7f\u7528\u81ea\u53cd\u9988\u673a\u5236\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6807\u51c6\u3002", "result": "\u8de8\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMADRec\u5728\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u548cLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eba\u5de5\u8bc4\u4f30\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u751f\u6210\u89e3\u91ca\u7684\u8bf4\u670d\u529b\u3002", "conclusion": "MADRec\u5c55\u793a\u4e86LLM\u4ee3\u7406\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7528\u6237\u504f\u597d\u5e76\u751f\u6210\u6709\u8bf4\u670d\u529b\u7684\u89e3\u91ca\u3002"}}
{"id": "2510.13590", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13590", "abs": "https://arxiv.org/abs/2510.13590", "authors": ["Jiale Han", "Austin Cheung", "Yubai Wei", "Zheng Yu", "Xusheng Wang", "Bing Zhu", "Yi Yang"], "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge", "comment": null, "summary": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86Temporal GraphRAG (TG-RAG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53cc\u5c42\u65f6\u5e8f\u56fe\u6765\u589e\u5f3aRAG\u7cfb\u7edf\u7684\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5e8f\u77e5\u8bc6\u8868\u793a\u548c\u589e\u91cf\u66f4\u65b0\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u5ffd\u7565\u4e86\u77e5\u8bc6\u7684\u65f6\u5e8f\u7279\u6027\uff0c\u96be\u4ee5\u533a\u5206\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u76f8\u540c\u4e8b\u5b9e\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u589e\u91cf\u66f4\u65b0\u673a\u5236\u548c\u65f6\u5e8f\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b\u65f6\u5e8f\u77e5\u8bc6\u56fe\u548c\u5206\u5c42\u65f6\u95f4\u56fe\u7684\u53cc\u5c42\u65f6\u5e8f\u56fe\uff0c\u751f\u6210\u591a\u7c92\u5ea6\u65f6\u5e8f\u6458\u8981\uff0c\u652f\u6301\u589e\u91cf\u66f4\u65b0\u548c\u52a8\u6001\u5b50\u56fe\u68c0\u7d22\u3002", "result": "TG-RAG\u5728\u65f6\u5e8f\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u65f6\u5e8f\u77e5\u8bc6\u548c\u589e\u91cf\u66f4\u65b0\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TG-RAG\u901a\u8fc7\u65f6\u5e8f\u56fe\u8868\u793a\u548c\u5206\u5c42\u65f6\u95f4\u7ed3\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u7684\u65f6\u95f4\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u65f6\u5e8f\u77e5\u8bc6\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13738", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.13738", "abs": "https://arxiv.org/abs/2510.13738", "authors": ["Jingyi Zhou", "Cheng Chen", "Kai Zuo", "Manjie Xu", "Zhendong Fu", "Yibo Chen", "Xu Tang", "Yao Hu"], "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems.", "AI": {"tldr": "\u63d0\u51faHyMiRec\u6df7\u5408\u591a\u5174\u8da3\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u89e3\u51b3LLM\u63a8\u8350\u4e2d\u957f\u5e8f\u5217\u622a\u65ad\u548c\u5174\u8da3\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u65b9\u6cd5\u56e0\u63a8\u7406\u5ef6\u8fdf\u548c\u7279\u5f81\u83b7\u53d6\u9650\u5236\uff0c\u53ea\u80fd\u622a\u65ad\u7528\u6237\u884c\u4e3a\u5e8f\u5217\uff0c\u4e22\u5931\u957f\u671f\u504f\u597d\u4fe1\u53f7\uff1b\u4e14\u4f9d\u8d56\u5355\u9884\u6d4b\u5d4c\u5165\uff0c\u5ffd\u89c6\u7528\u6237\u5174\u8da3\u7684\u591a\u6837\u6027", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u63a8\u8350\u5668\u63d0\u53d6\u957f\u5e8f\u5217\u7684\u7c97\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff0cLLM\u63a8\u8350\u5668\u6355\u83b7\u7ec6\u7c92\u5ea6\u5174\u8da3\u5d4c\u5165\uff1b\u5f15\u5165\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u6b8b\u5dee\u7801\u672c\u538b\u7f29\u7528\u6237\u5386\u53f2\u5d4c\u5165\uff1b\u8bbe\u8ba1\u89e3\u8026\u591a\u5174\u8da3\u5b66\u4e60\u6a21\u5757\u81ea\u9002\u5e94\u5b66\u4e60\u591a\u4e2a\u5174\u8da3\u4fe1\u53f7", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u5728\u771f\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e26\u6765\u6301\u7eed\u6539\u8fdb", "conclusion": "HyMiRec\u901a\u8fc7\u6df7\u5408\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u8350\u4e2d\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u548c\u5174\u8da3\u591a\u6837\u6027\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272"}}

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 12]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant是一个结合LLM和OpenAI函数调用API的金融数据库自然语言查询系统，通过预定义参数化查询而非完全生成SQL，在可靠性、延迟和成本方面优于text-to-SQL方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统text-to-SQL方法在金融数据库查询中存在的可靠性、延迟和成本问题，探索LLM在金融数据查询中的实际应用效果。

Method: 结合LLM和OpenAI函数调用API，使用预定义的参数化查询库，而非完全生成SQL语句。通过三个研究问题评估LLM在金融数据查询中的表现。

Result: LLM单独预测存在非可忽略误差和时间前瞻偏差；股票代码映射在NASDAQ-100中近乎完美，S&P 500中准确率高；FinAI Data Assistant在延迟、成本和可靠性方面优于text-to-SQL基线。

Conclusion: FinAI Data Assistant通过预定义查询库的方法在金融数据查询任务中实现了更好的性能，为实际部署提供了可行方案，同时揭示了LLM在金融数据查询中的局限性。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [2] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: LinkedIn提出了一种基于LLaMA 3大语言模型的检索方法，通过微调双编码器生成高质量的用户和内容嵌入，用于大规模推荐系统的候选检索阶段。


<details>
  <summary>Details</summary>
Motivation: 在LinkedIn Feed等大规模推荐系统中，检索阶段需要从数亿候选内容中快速筛选出少量高质量内容，现有方法在效率和效果上存在挑战。

Method: 使用Meta的LLaMA 3作为双编码器，仅基于文本输入生成用户和内容嵌入；设计了提示模板、大规模微调技术和低延迟在线服务基础设施。

Result: 在线A/B测试显示成员参与度显著提升，特别是对新用户效果更明显，表明高质量的建议内容有助于用户留存。

Conclusion: 这项工作证明了生成式语言模型可以有效适应工业应用中的实时高吞吐量检索需求。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [3] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo是一个端到端的推荐系统框架，通过动态构建用户特定的上下文知识嵌入，实现语义和行为潜在维度的深度融合，在推荐准确性和实际业务效果上均取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态模式提示的LLM推荐方法存在两个主要局限：使用通用模板结构忽略用户偏好多样性，以及语义知识表示与行为特征空间的浅层对齐未能实现全面潜在空间整合。

Method: CoCo采用双机制方法动态构建用户特定的上下文知识嵌入，通过自适应知识融合和矛盾解决模块实现语义和行为潜在维度的深度融合。

Result: 在多个基准数据集和企业级电商平台上的实验表明，CoCo在推荐准确性上比七种先进方法最高提升8.58%，在生产广告系统中的部署实现了1.91%的销售额增长。

Conclusion: CoCo的模块化设计和模型无关架构为需要知识增强推理和个性化适应的下一代推荐系统提供了通用解决方案。

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [4] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 提出大型推理嵌入模型(LREM)，通过将推理过程融入表示学习来解决密集检索中的语义鸿沟问题，显著提升困难查询的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 现有密集检索模型采用直接嵌入方法，语义准确性不足，且倾向于捕捉训练数据中的统计共现模式，导致在词汇差异大的困难查询上性能显著下降。

Method: 采用两阶段训练：第一阶段通过SFT和InfoNCE损失优化LLM在Query-CoT-Item三元组上建立初步推理和嵌入能力；第二阶段通过强化学习进一步优化推理轨迹。

Result: 离线和在线实验验证了LREM的有效性，已于2025年8月在中国最大的电商平台部署。

Conclusion: LREM通过集成推理过程到表示学习中，有效弥合原始查询与目标商品之间的语义鸿沟，显著提升检索准确率。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [5] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: 本文提出了一个基于逻辑回归的幻觉检测方法，通过集成多个检测模型来减少VLM中的幻觉，在KDD Cup 2025的Meta CRAG-MM挑战赛中获得第5名。


<details>
  <summary>Details</summary>
Motivation: 由于错误答案会导致负分，团队策略聚焦于减少VLM内部表示中的幻觉，以提高VQA任务的准确性。

Method: 训练基于逻辑回归的幻觉检测模型，使用隐藏状态和特定注意力头的输出，并采用模型集成方法。

Result: 虽然牺牲了一些正确答案，但显著减少了幻觉，在最终排行榜上进入前五名。

Conclusion: 该方法通过幻觉检测有效提升了VQA系统的性能，证明了减少幻觉对提升模型准确性的重要性。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [6] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出GemiRec框架解决多兴趣推荐中的兴趣崩溃和兴趣演化建模不足问题，通过兴趣量化和兴趣生成实现结构分离和动态学习


<details>
  <summary>Details</summary>
Motivation: 多兴趣推荐在工业检索阶段受到关注，但现有方法存在兴趣崩溃（多个表征同质化）和兴趣演化建模不足（难以捕捉历史行为中不存在的潜在兴趣）两个主要限制

Method: 提出GemiRec框架，包含三个模块：兴趣字典维护模块（IDMM）维护共享量化兴趣字典；多兴趣后验分布模块（MIPDM）使用生成模型捕捉用户未来兴趣分布；多兴趣检索模块（MIRM）使用多个用户兴趣表征进行物品检索

Result: 理论和实证分析以及大量实验证明了该方法的优势和有效性，已于2025年3月部署到生产环境，展示了在工业应用中的实用价值

Conclusion: GemiRec通过兴趣量化和兴趣生成有效解决了多兴趣推荐中的兴趣崩溃和演化建模问题，具有理论优势和实际应用价值

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [7] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: 提出了MR.Rec框架，通过结合记忆和推理来增强LLM在推荐系统中的能力，解决深度个性化和智能推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推荐系统中面临上下文窗口有限和单轮推理的约束，难以捕捉动态用户偏好和进行主动推理。

Method: 开发了基于RAG的记忆检索系统，集成推理增强的记忆检索，并设计了强化学习框架来训练LLM自主学习记忆利用和推理优化的策略。

Result: 在多个指标上显著优于现有最先进基线，验证了其在提供智能和个性化推荐方面的有效性。

Conclusion: 通过动态记忆检索和自适应推理的结合，MR.Rec能够提供更准确、上下文感知和高度个性化的推荐。

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [8] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出CE-CDR框架，通过因果图建模和部分标签因果损失来增强跨域推荐，解决源域任务不一致和因果关系忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 传统跨域推荐存在两个问题：源域任务不一致可能导致负迁移；忽略底层因果关系会限制源域特征的贡献。直接在有因果标签的数据集上训练跨域表示具有挑战性。

Method: 首先将跨域推荐重新表述为因果图，然后启发式构建因果感知数据集，推导理论上无偏的部分标签因果损失来泛化到未见模式，生成增强的跨域表示。

Result: 理论和实证分析证明了CE-CDR的合理性和有效性，可作为模型无关插件通用应用，已在2025年4月部署到生产环境。

Conclusion: CE-CDR是首个探索因果增强跨域推荐的方法，通过因果建模和部分标签损失有效解决了跨域推荐中的关键挑战，具有实际应用价值。

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [9] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 该研究分析了推荐系统中数据修剪（移除交互次数少的用户）对数据集特性和算法性能的影响，发现修剪会显著改变数据集结构，在修剪数据上训练测试会虚高算法性能，但在未修剪测试集上性能优势消失。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究严重依赖数据集，许多数据集都经过修剪处理（如MovieLens），但修剪对数据集特性和算法性能的真实影响尚未充分研究。

Method: 分析5个基准数据集在未修剪和5个修剪级别（5,10,20,50,100）下的结构分布特性，训练测试11种代表性算法，并比较在修剪训练集上训练但在未修剪测试集上评估的性能。

Result: 常用核心修剪可能只保留原始用户的2%；传统算法在修剪数据上训练测试时nDCG@10得分更高，但在未修剪测试集上优势消失；所有算法在未修剪测试集上的性能随修剪级别增加而下降。

Conclusion: 数据修剪会显著改变推荐系统评估结果，在修剪数据上获得的性能提升可能是人为虚高的，建议在未修剪数据上评估算法性能以获得更真实的结果。

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [10] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: RED-Rec是一个面向工业级内容推荐系统的LLM增强分层推荐引擎，通过统一多场景用户兴趣表示和跨场景行为信号融合，在大规模部署中实现更全面的用户建模和个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统通常在孤立场景中优化业务指标，忽视了跨场景行为信号，难以在大规模部署中整合LLM等先进技术，限制了全面捕捉用户兴趣的能力。

Method: 提出RED-Rec框架：采用双塔LLM架构实现细粒度多面表示，通过场景感知的密集混合和查询策略融合多样化行为信号，统一多行为上下文的用户兴趣表示。

Result: 在RedNote平台上对数亿用户进行在线A/B测试，在内容推荐和广告定向任务中均取得显著性能提升，并发布了百万级序列推荐数据集RED-MMU。

Conclusion: 该工作推进了统一用户建模，在大规模UGC平台中实现了更深层次的个性化，促进了更有意义的用户参与。

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [11] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 该论文提出了一个模拟框架来研究推荐系统中的反馈循环如何影响在线零售环境中的用户行为和集体市场动态。研究发现推荐算法在增加个体多样性的同时会减少集体多样性，并导致需求集中在少数热门商品上。


<details>
  <summary>Details</summary>
Motivation: 推荐系统与用户持续交互形成反馈循环，这些循环既影响个体行为也塑造集体市场动态。现有研究缺乏对这些长期动态的系统性分析，特别是在推荐系统周期性重新训练的情况下。

Method: 使用亚马逊电商数据集，构建模拟框架来建模推荐系统反馈循环，分析不同推荐算法如何随时间影响多样性、购买集中度和用户同质化。

Result: 结果显示系统性的权衡：反馈循环增加了个体多样性，但减少了集体多样性，并将需求集中在少数热门商品上。某些推荐系统还会随时间增加用户同质化，使用户购买档案越来越相似。

Conclusion: 研究结果强调了需要在推荐系统设计中平衡个性化与长期多样性，避免反馈循环带来的负面集体影响。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [12] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 提出了mxbai-edge-colbert-v0模型（17M和32M参数），作为小型检索模型的证明概念，在短文本基准上超越ColBERTv2，在长上下文任务中效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 支持从云端大规模检索到本地设备上运行的全尺度检索，为未来实验提供坚实的基础骨干模型。

Method: 通过多次消融研究改进检索和延迟交互模型，并将研究成果提炼到小型模型中。

Result: 在BEIR短文本基准上表现优于ColBERTv2，在长上下文任务中实现前所未有的效率提升。

Conclusion: mxbai-edge-colbert-v0是首个小型证明概念系列版本，为全尺度检索提供了有前景的基础模型。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>

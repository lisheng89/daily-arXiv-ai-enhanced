{"id": "2601.14348", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14348", "abs": "https://arxiv.org/abs/2601.14348", "authors": ["Dominik Stammbach", "Kylie Zhang", "Patty Liu", "Nimra Nadeem", "Lucia Zheng", "Peter Henderson"], "title": "Legal Retrieval for Public Defenders", "comment": null, "summary": "AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.", "AI": {"tldr": "\u5f00\u53d1NJ BriefBank\u68c0\u7d22\u5de5\u5177\uff0c\u5e2e\u52a9\u516c\u8bbe\u8fa9\u62a4\u4eba\u5feb\u901f\u67e5\u627e\u76f8\u5173\u4e0a\u8bc9\u72b6\uff0c\u6539\u8fdb\u6cd5\u5f8b\u68c0\u7d22\u57fa\u51c6", "motivation": "\u516c\u8bbe\u8fa9\u62a4\u673a\u6784\u9762\u4e34\u6848\u4ef6\u91cf\u5927\u3001\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0cAI\u5de5\u5177\u53ef\u80fd\u63d0\u4f9b\u5e2e\u52a9\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4fAI\u5982\u4f55\u6709\u6548\u652f\u6301\u8fa9\u62a4\u4eba\u65e5\u5e38\u5de5\u4f5c\u7684\u8bc1\u636e", "method": "\u4e0e\u7ebd\u6cfd\u897f\u516c\u8bbe\u8fa9\u62a4\u529e\u516c\u5ba4\u5408\u4f5c\u5f00\u53d1NJ BriefBank\u68c0\u7d22\u5de5\u5177\uff0c\u901a\u8fc7\u6dfb\u52a0\u9886\u57df\u77e5\u8bc6\uff08\u6cd5\u5f8b\u63a8\u7406\u67e5\u8be2\u6269\u5c55\u3001\u9886\u57df\u7279\u5b9a\u6570\u636e\u3001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5408\u6210\u793a\u4f8b\uff09\u6539\u8fdb\u68c0\u7d22\u8d28\u91cf", "result": "\u73b0\u6709\u6cd5\u5f8b\u68c0\u7d22\u57fa\u51c6\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u516c\u8bbe\u8fa9\u62a4\u68c0\u7d22\uff0c\u4f46\u6dfb\u52a0\u9886\u57df\u77e5\u8bc6\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\uff1b\u63d0\u4f9b\u4e86\u73b0\u5b9e\u8fa9\u62a4\u4eba\u67e5\u8be2\u5206\u7c7b\u548c\u624b\u52a8\u6807\u6ce8\u7684\u516c\u8bbe\u8fa9\u62a4\u68c0\u7d22\u6570\u636e\u96c6", "conclusion": "\u4e3a\u6784\u5efa\u5b9e\u7528\u53ef\u9760\u7684\u6cd5\u5f8b\u68c0\u7d22AI\u5de5\u5177\u63d0\u4f9b\u4e86\u8d77\u70b9\uff0c\u5e76\u4e3a\u66f4\u73b0\u5b9e\u7684\u6cd5\u5f8b\u68c0\u7d22\u57fa\u51c6\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2601.14460", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14460", "abs": "https://arxiv.org/abs/2601.14460", "authors": ["Weronika \u0141ajewska", "Krisztian Balog"], "title": "Trust Me on This: A User Study of Trustworthiness for RAG Responses", "comment": "This is the author's version of the work. The definitive version is published in: Proceedings of the 48th European Conference on Information Retrieval (ECIR '26), March 29-April 2, 2026, Delft, The Netherlands", "summary": "The integration of generative AI into information access systems often presents users with synthesized answers that lack transparency. This study investigates how different types of explanations can influence user trust in responses from retrieval-augmented generation systems. We conducted a controlled, two-stage user study where participants chose the more trustworthy response from a pair-one objectively higher quality than the other-both with and without one of three explanation types: (1) source attribution, (2) factual grounding, and (3) information coverage. Our results show that while explanations significantly guide users toward selecting higher quality responses, trust is not dictated by objective quality alone: Users' judgments are also heavily influenced by response clarity, actionability, and their own prior knowledge.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u89e3\u91ca\u7c7b\u578b\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u54cd\u5e94\u7684\u4fe1\u4efb\uff0c\u53d1\u73b0\u89e3\u91ca\u80fd\u5f15\u5bfc\u7528\u6237\u9009\u62e9\u66f4\u9ad8\u8d28\u91cf\u56de\u7b54\uff0c\u4f46\u4fe1\u4efb\u8fd8\u53d7\u54cd\u5e94\u6e05\u6670\u5ea6\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u7528\u6237\u5148\u9a8c\u77e5\u8bc6\u5f71\u54cd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u96c6\u6210\u5230\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\u65f6\uff0c\u5e38\u63d0\u4f9b\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u5408\u6210\u7b54\u6848\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u7c7b\u578b\u7684\u89e3\u91ca\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u54cd\u5e94\u7684\u4fe1\u4efb\u3002", "method": "\u8fdb\u884c\u53d7\u63a7\u7684\u4e24\u9636\u6bb5\u7528\u6237\u7814\u7a76\uff0c\u53c2\u4e0e\u8005\u4ece\u4e00\u5bf9\u54cd\u5e94\u4e2d\u9009\u62e9\u66f4\u53ef\u4fe1\u7684\u54cd\u5e94\uff08\u4e00\u4e2a\u5ba2\u89c2\u8d28\u91cf\u66f4\u9ad8\uff09\uff0c\u6bd4\u8f83\u4e09\u79cd\u89e3\u91ca\u7c7b\u578b\uff1a\u6765\u6e90\u5f52\u5c5e\u3001\u4e8b\u5b9e\u57fa\u7840\u3001\u4fe1\u606f\u8986\u76d6\u5ea6\uff0c\u4ee5\u53ca\u65e0\u89e3\u91ca\u60c5\u51b5\u3002", "result": "\u89e3\u91ca\u663e\u8457\u5f15\u5bfc\u7528\u6237\u9009\u62e9\u66f4\u9ad8\u8d28\u91cf\u7684\u54cd\u5e94\uff0c\u4f46\u4fe1\u4efb\u4e0d\u5b8c\u5168\u7531\u5ba2\u89c2\u8d28\u91cf\u51b3\u5b9a\uff0c\u8fd8\u53d7\u54cd\u5e94\u6e05\u6670\u5ea6\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u7528\u6237\u5148\u9a8c\u77e5\u8bc6\u7684\u5f3a\u70c8\u5f71\u54cd\u3002", "conclusion": "\u89e3\u91ca\u8bbe\u8ba1\u5728\u589e\u5f3aAI\u7cfb\u7edf\u900f\u660e\u5ea6\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5e73\u8861\u5ba2\u89c2\u8d28\u91cf\u4e0e\u7528\u6237\u611f\u77e5\u56e0\u7d20\uff08\u5982\u6e05\u6670\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\uff09\uff0c\u624d\u80fd\u6709\u6548\u5efa\u7acb\u7528\u6237\u4fe1\u4efb\u3002"}}
{"id": "2601.14546", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14546", "abs": "https://arxiv.org/abs/2601.14546", "authors": ["Fangzheng Tian", "Debasis Ganguly", "Craig Macdonald"], "title": "Predicting Retrieval Utility and Answer Quality in Retrieval-Augmented Generation", "comment": "18 pages (including reference), 3 figures, 2 table, 61 references; this paper has been accepted by ECIR'26 as a full paper", "summary": "The quality of answers generated by large language models (LLMs) in retrieval-augmented generation (RAG) is largely influenced by the contextual information contained in the retrieved documents. A key challenge for improving RAG is to predict both the utility of retrieved documents -- quantified as the performance gain from using context over generation without context -- and the quality of the final answers in terms of correctness and relevance. In this paper, we define two prediction tasks within RAG. The first is retrieval performance prediction (RPP), which estimates the utility of retrieved documents. The second is generation performance prediction (GPP), which estimates the final answer quality. We hypothesise that in RAG, the topical relevance of retrieved documents correlates with their utility, suggesting that query performance prediction (QPP) approaches can be adapted for RPP and GPP. Beyond these retriever-centric signals, we argue that reader-centric features, such as the LLM's perplexity of the retrieved context conditioned on the input query, can further enhance prediction accuracy for both RPP and GPP. Finally, we propose that features reflecting query-agnostic document quality and readability can also provide useful signals to the predictions. We train linear regression models with the above categories of predictors for both RPP and GPP. Experiments on the Natural Questions (NQ) dataset show that combining predictors from multiple feature categories yields the most accurate estimates of RAG performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e2d\u9884\u6d4b\u68c0\u7d22\u6587\u6863\u6548\u7528\u548c\u6700\u7ec8\u7b54\u6848\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u5668\u4e2d\u5fc3\u7279\u5f81\u3001\u9605\u8bfb\u5668\u4e2d\u5fc3\u7279\u5f81\u548c\u6587\u6863\u8d28\u91cf\u7279\u5f81\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "RAG\u4e2d\u751f\u6210\u7b54\u6848\u7684\u8d28\u91cf\u53d7\u68c0\u7d22\u6587\u6863\u4e0a\u4e0b\u6587\u4fe1\u606f\u5f71\u54cd\u5f88\u5927\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\u6765\u9884\u6d4b\u68c0\u7d22\u6587\u6863\u7684\u6548\u7528\u548c\u6700\u7ec8\u7b54\u6848\u7684\u8d28\u91cf\uff0c\u8fd9\u9650\u5236\u4e86RAG\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u5b9a\u4e49\u4e24\u4e2a\u9884\u6d4b\u4efb\u52a1\uff1a\u68c0\u7d22\u6027\u80fd\u9884\u6d4b(RPP)\u548c\u751f\u6210\u6027\u80fd\u9884\u6d4b(GPP)\u3002\u63d0\u51fa\u4e09\u7c7b\u9884\u6d4b\u7279\u5f81\uff1a1)\u57fa\u4e8e\u67e5\u8be2\u6027\u80fd\u9884\u6d4b(QPP)\u7684\u68c0\u7d22\u5668\u4e2d\u5fc3\u7279\u5f81\uff1b2)\u57fa\u4e8eLLM\u56f0\u60d1\u5ea6\u7684\u9605\u8bfb\u5668\u4e2d\u5fc3\u7279\u5f81\uff1b3)\u67e5\u8be2\u65e0\u5173\u7684\u6587\u6863\u8d28\u91cf\u548c\u53ef\u8bfb\u6027\u7279\u5f81\u3002\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728Natural Questions\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u591a\u4e2a\u7279\u5f81\u7c7b\u522b\u7684\u9884\u6d4b\u5668\u80fd\u591f\u6700\u51c6\u786e\u5730\u4f30\u8ba1RAG\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6574\u5408\u68c0\u7d22\u5668\u4e2d\u5fc3\u3001\u9605\u8bfb\u5668\u4e2d\u5fc3\u548c\u6587\u6863\u8d28\u91cf\u7279\u5f81\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u9884\u6d4bRAG\u4e2d\u7684\u68c0\u7d22\u6587\u6863\u6548\u7528\u548c\u6700\u7ec8\u7b54\u6848\u8d28\u91cf\uff0c\u4e3aRAG\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2601.14697", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14697", "abs": "https://arxiv.org/abs/2601.14697", "authors": ["Shutong Qiao", "Wei Yuan", "Tong Chen", "Xiangyu Zhao", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "When Text-as-Vision Meets Semantic IDs in Generative Recommendation: An Empirical Study", "comment": null, "summary": "Semantic ID learning is a key interface in Generative Recommendation (GR) models, mapping items to discrete identifiers grounded in side information, most commonly via a pretrained text encoder. However, these text encoders are primarily optimized for well-formed natural language. In real-world recommendation data, item descriptions are often symbolic and attribute-centric, containing numerals, units, and abbreviations. These text encoders can break these signals into fragmented tokens, weakening semantic coherence and distorting relationships among attributes. Worse still, when moving to multimodal GR, relying on standard text encoders introduces an additional obstacle: text and image embeddings often exhibit mismatched geometric structures, making cross-modal fusion less effective and less stable.\n  In this paper, we revisit representation design for Semantic ID learning by treating text as a visual signal. We conduct a systematic empirical study of OCR-based text representations, obtained by rendering item descriptions into images and encoding them with vision-based OCR models. Experiments across four datasets and two generative backbones show that OCR-text consistently matches or surpasses standard text embeddings for Semantic ID learning in both unimodal and multimodal settings. Furthermore, we find that OCR-based Semantic IDs remain robust under extreme spatial-resolution compression, indicating strong robustness and efficiency in practical deployments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u6587\u672c\u4f5c\u4e3a\u89c6\u89c9\u4fe1\u53f7\u5904\u7406\uff0c\u4f7f\u7528OCR\u6a21\u578b\u7f16\u7801\u5546\u54c1\u63cf\u8ff0\u56fe\u50cf\uff0c\u5728\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u663e\u8457\u63d0\u5347\u8bed\u4e49ID\u5b66\u4e60\u6548\u679c", "motivation": "\u4f20\u7edf\u6587\u672c\u7f16\u7801\u5668\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u8bed\u8a00\u4f18\u5316\uff0c\u4f46\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5546\u54c1\u63cf\u8ff0\u5e38\u5305\u542b\u6570\u5b57\u3001\u5355\u4f4d\u3001\u7f29\u5199\u7b49\u7b26\u53f7\u5316\u5185\u5bb9\uff0c\u5bfc\u81f4\u8bed\u4e49\u788e\u7247\u5316\u3002\u5728\u591a\u6a21\u6001\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\uff0c\u6587\u672c\u548c\u56fe\u50cf\u5d4c\u5165\u7684\u51e0\u4f55\u7ed3\u6784\u4e0d\u5339\u914d\u8fdb\u4e00\u6b65\u5f71\u54cd\u8de8\u6a21\u6001\u878d\u5408\u6548\u679c\u3002", "method": "\u5c06\u5546\u54c1\u63cf\u8ff0\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u7684OCR\u6a21\u578b\u8fdb\u884c\u7f16\u7801\uff0c\u83b7\u5f97OCR\u6587\u672c\u8868\u793a\u3002\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u79cd\u751f\u6210\u5f0f\u9aa8\u5e72\u7f51\u7edc\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "OCR\u6587\u672c\u8868\u793a\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5747\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u6807\u51c6\u6587\u672c\u5d4c\u5165\u7684\u8bed\u4e49ID\u5b66\u4e60\u6548\u679c\u3002\u5373\u4f7f\u5728\u6781\u7aef\u7a7a\u95f4\u5206\u8fa8\u7387\u538b\u7f29\u4e0b\uff0cOCR\u8bed\u4e49ID\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u5c06\u6587\u672c\u4f5c\u4e3a\u89c6\u89c9\u4fe1\u53f7\u5904\u7406\uff0c\u901a\u8fc7OCR\u6a21\u578b\u7f16\u7801\u5546\u54c1\u63cf\u8ff0\u56fe\u50cf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u6587\u672c\u7f16\u7801\u5668\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u9c81\u68d2\u9ad8\u6548\u7684\u8bed\u4e49ID\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2601.14714", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14714", "abs": "https://arxiv.org/abs/2601.14714", "authors": ["Xinyuan Zhang", "Lina Zhang", "Lisung Chen", "Guangyao Liu", "Shuai Nie", "Jiaming Xu", "Runyu Shi", "Ying Huang", "Guoquan Zhang"], "title": "Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration", "comment": "4 pages, 2 figures, submitted to IEEE ICASSP 2026", "summary": "Multimodal retrieval systems typically employ Vision Language Models (VLMs) that encode images and text independently into vectors within a shared embedding space. Despite incorporating text encoders, VLMs consistently underperform specialized text models on text-only retrieval tasks. Moreover, introducing additional text encoders increases storage, inference overhead, and exacerbates retrieval inefficiencies, especially in multilingual settings. To address these limitations, we propose a multi-task learning framework that unifies the feature representation across images, long and short texts, and intent-rich queries. To our knowledge, this is the first work to jointly optimize multilingual image retrieval, text retrieval, and natural language understanding (NLU) tasks within a single framework. Our approach integrates image and text retrieval with a shared text encoder that is enhanced by NLU features for intent understanding and retrieval accuracy.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u591a\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u4f18\u5316\u591a\u8bed\u8a00\u56fe\u50cf\u68c0\u7d22\u3001\u6587\u672c\u68c0\u7d22\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\uff0c\u4f7f\u7528\u5171\u4eab\u6587\u672c\u7f16\u7801\u5668\u63d0\u5347\u610f\u56fe\u7406\u89e3\u548c\u68c0\u7d22\u51c6\u786e\u6027", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u6dfb\u52a0\u989d\u5916\u6587\u672c\u7f16\u7801\u5668\u4f1a\u589e\u52a0\u5b58\u50a8\u548c\u63a8\u7406\u5f00\u9500\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u68c0\u7d22\u6548\u7387\u95ee\u9898\u66f4\u4e25\u91cd", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u56fe\u50cf\u3001\u957f\u77ed\u6587\u672c\u548c\u610f\u56fe\u4e30\u5bcc\u67e5\u8be2\u7684\u7279\u5f81\u8868\u793a\uff0c\u96c6\u6210\u56fe\u50cf\u548c\u6587\u672c\u68c0\u7d22\uff0c\u4f7f\u7528\u5171\u4eab\u6587\u672c\u7f16\u7801\u5668\u5e76\u901a\u8fc7NLU\u7279\u5f81\u589e\u5f3a", "result": "\u8fd9\u662f\u9996\u4e2a\u5728\u5355\u4e00\u6846\u67b6\u5185\u8054\u5408\u4f18\u5316\u591a\u8bed\u8a00\u56fe\u50cf\u68c0\u7d22\u3001\u6587\u672c\u68c0\u7d22\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u5de5\u4f5c", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u548c\u5171\u4eab\u7f16\u7801\u5668\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2601.14720", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14720", "abs": "https://arxiv.org/abs/2601.14720", "authors": ["Doyun Choi", "Cheonwoo Lee", "Biniyam Aschalew Tolera", "Taewook Ham", "Chanyoung Park", "Jaemin Yoo"], "title": "PULSE: Socially-Aware User Representation Modeling Toward Parameter-Efficient Graph Collaborative Filtering", "comment": "Accepted at WWW 2026, 12pages", "summary": "Graph-based social recommendation (SocialRec) has emerged as a powerful extension of graph collaborative filtering (GCF), which leverages graph neural networks (GNNs) to capture multi-hop collaborative signals from user-item interactions. These methods enrich user representations by incorporating social network information into GCF, thereby integrating additional collaborative signals from social relations. However, existing GCF and graph-based SocialRec approaches face significant challenges: they incur high computational costs and suffer from limited scalability due to the large number of parameters required to assign explicit embeddings to all users and items. In this work, we propose PULSE (Parameter-efficient User representation Learning with Social Knowledge), a framework that addresses this limitation by constructing user representations from socially meaningful signals without creating an explicit learnable embedding for each user. PULSE reduces the parameter size by up to 50% compared to the most lightweight GCF baseline. Beyond parameter efficiency, our method achieves state-of-the-art performance, outperforming 13 GCF and graph-based social recommendation baselines across varying levels of interaction sparsity, from cold-start to highly active users, through a time- and memory-efficient modeling process.", "AI": {"tldr": "\u63d0\u51faPULSE\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u793e\u4ea4\u4fe1\u53f7\u6784\u5efa\u7528\u6237\u8868\u793a\u800c\u975e\u663e\u5f0f\u5b66\u4e60\u5d4c\u5165\uff0c\u51cf\u5c11\u53c2\u6570\u8fbe50%\uff0c\u5728\u7a00\u758f\u4ea4\u4e92\u573a\u666f\u4e0b\u8d85\u8d8a13\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684\u793e\u4ea4\u63a8\u8350\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u9700\u8981\u4e3a\u6240\u6709\u7528\u6237\u548c\u7269\u54c1\u5206\u914d\u663e\u5f0f\u5d4c\u5165\u53c2\u6570\u3002", "method": "\u63d0\u51faPULSE\u6846\u67b6\uff0c\u4ece\u793e\u4ea4\u6709\u610f\u4e49\u7684\u4fe1\u53f7\u6784\u5efa\u7528\u6237\u8868\u793a\uff0c\u800c\u4e0d\u4e3a\u6bcf\u4e2a\u7528\u6237\u521b\u5efa\u663e\u5f0f\u53ef\u5b66\u4e60\u5d4c\u5165\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u7528\u6237\u8868\u793a\u5b66\u4e60\u3002", "result": "\u76f8\u6bd4\u6700\u8f7b\u91cf\u7ea7GCF\u57fa\u7ebf\u51cf\u5c11\u53c2\u6570\u8fbe50%\uff0c\u572813\u4e2aGCF\u548c\u57fa\u4e8e\u56fe\u7684\u793e\u4ea4\u63a8\u8350\u57fa\u7ebf\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5728\u4ece\u51b7\u542f\u52a8\u5230\u9ad8\u6d3b\u8dc3\u7528\u6237\u7684\u4e0d\u540c\u4ea4\u4e92\u7a00\u758f\u5ea6\u573a\u666f\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PULSE\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u7528\u6237\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u56fe\u793e\u4ea4\u63a8\u8350\u65b9\u6cd5\u7684\u8ba1\u7b97\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u3002"}}
{"id": "2601.14949", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.14949", "abs": "https://arxiv.org/abs/2601.14949", "authors": ["Leqi Zheng", "Jiajun Zhang", "Canzhi Chen", "Chaokun Wang", "Hongwei Li", "Yuying Li", "Yaoxin Mao", "Shannan Yan", "Zixin Song", "Zhiyuan Feng", "Zhaolu Kang", "Zirong Chen", "Hang Zhang", "Qiang Liu", "Liang Wang", "Ziyang Liu"], "title": "What Should I Cite? A RAG Benchmark for Academic Citation Prediction", "comment": null, "summary": "With the rapid growth of Web-based academic publications, more and more papers are being published annually, making it increasingly difficult to find relevant prior work. Citation prediction aims to automatically suggest appropriate references, helping scholars navigate the expanding scientific literature. Here we present \\textbf{CiteRAG}, the first comprehensive retrieval-augmented generation (RAG)-integrated benchmark for evaluating large language models on academic citation prediction, featuring a multi-level retrieval strategy, specialized retrievers, and generators. Our benchmark makes four core contributions: (1) We establish two instances of the citation prediction task with different granularity. Task 1 focuses on coarse-grained list-specific citation prediction, while Task 2 targets fine-grained position-specific citation prediction. To enhance these two tasks, we build a dataset containing 7,267 instances for Task 1 and 8,541 instances for Task 2, enabling comprehensive evaluation of both retrieval and generation. (2) We construct a three-level large-scale corpus with 554k papers spanning many major subfields, using an incremental pipeline. (3) We propose a multi-level hybrid RAG approach for citation prediction, fine-tuning embedding models with contrastive learning to capture complex citation relationships, paired with specialized generation models. (4) We conduct extensive experiments across state-of-the-art language models, including closed-source APIs, open-source models, and our fine-tuned generators, demonstrating the effectiveness of our framework. Our open-source toolkit enables reproducible evaluation and focuses on academic literature, providing the first comprehensive evaluation framework for citation prediction and serving as a methodological template for other scientific domains. Our source code and data are released at https://github.com/LQgdwind/CiteRAG.", "AI": {"tldr": "CiteRAG\u662f\u9996\u4e2a\u96c6\u6210\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5b66\u672f\u5f15\u7528\u9884\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u591a\u7ea7\u68c0\u7d22\u7b56\u7565\u3001\u4e13\u7528\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\uff0c\u63d0\u4f9b\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u5b66\u672f\u51fa\u7248\u7269\u5feb\u901f\u589e\u957f\uff0c\u6bcf\u5e74\u53d1\u8868\u7684\u8bba\u6587\u6570\u91cf\u6fc0\u589e\uff0c\u5b66\u8005\u8d8a\u6765\u8d8a\u96be\u627e\u5230\u76f8\u5173\u524d\u671f\u5de5\u4f5c\u3002\u5f15\u7528\u9884\u6d4b\u65e8\u5728\u81ea\u52a8\u63a8\u8350\u5408\u9002\u53c2\u8003\u6587\u732e\uff0c\u5e2e\u52a9\u5b66\u8005\u5728\u6269\u5c55\u7684\u79d1\u5b66\u6587\u732e\u4e2d\u5bfc\u822a\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u6df7\u5408RAG\u65b9\u6cd5\u7528\u4e8e\u5f15\u7528\u9884\u6d4b\uff1a1\uff09\u5efa\u7acb\u4e24\u4e2a\u4e0d\u540c\u7c92\u5ea6\u7684\u5f15\u7528\u9884\u6d4b\u4efb\u52a1\u5b9e\u4f8b\uff1b2\uff09\u6784\u5efa\u5305\u542b554k\u8bba\u6587\u7684\u4e09\u7ea7\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff1b3\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03\u5d4c\u5165\u6a21\u578b\u6355\u6349\u590d\u6742\u5f15\u7528\u5173\u7cfb\uff1b4\uff09\u7ed3\u5408\u4e13\u7528\u751f\u6210\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b7,267\u4e2a\u5b9e\u4f8b\u7684\u4efb\u52a11\u6570\u636e\u96c6\u548c8,541\u4e2a\u5b9e\u4f8b\u7684\u4efb\u52a12\u6570\u636e\u96c6\uff0c\u5728\u5305\u62ec\u95ed\u6e90API\u3001\u5f00\u6e90\u6a21\u578b\u548c\u5fae\u8c03\u751f\u6210\u5668\u5728\u5185\u7684\u6700\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "CiteRAG\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u5f15\u7528\u9884\u6d4b\u8bc4\u4f30\u6846\u67b6\uff0c\u53ef\u4f5c\u4e3a\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u7684\u65b9\u6cd5\u6a21\u677f\uff0c\u5f00\u6e90\u5de5\u5177\u5305\u652f\u6301\u53ef\u91cd\u590d\u8bc4\u4f30\u5e76\u4e13\u6ce8\u4e8e\u5b66\u672f\u6587\u732e\u3002"}}
{"id": "2601.15122", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15122", "abs": "https://arxiv.org/abs/2601.15122", "authors": ["Parviz Ahmadov", "Masoud Mansoury"], "title": "From Insight to Intervention: Interpretable Neuron Steering for Controlling Popularity Bias in Recommender Systems", "comment": null, "summary": "Popularity bias is a pervasive challenge in recommender systems, where a few popular items dominate attention while the majority of less popular items remain underexposed. This imbalance can reduce recommendation quality and lead to unfair item exposure. Although existing mitigation methods address this issue to some extent, they often lack transparency in how they operate. In this paper, we propose a post-hoc approach, PopSteer, that leverages a Sparse Autoencoder (SAE) to both interpret and mitigate popularity bias in recommendation models. The SAE is trained to replicate a trained model's behavior while enabling neuron-level interpretability. By introducing synthetic users with strong preferences for either popular or unpopular items, we identify neurons encoding popularity signals through their activation patterns. We then steer recommendations by adjusting the activations of the most biased neurons. Experiments on three public datasets with a sequential recommendation model demonstrate that PopSteer significantly enhances fairness with minimal impact on accuracy, while providing interpretable insights and fine-grained control over the fairness-accuracy trade-off.", "AI": {"tldr": "PopSteer\uff1a\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u548c\u7f13\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u901a\u8fc7\u8bc6\u522b\u7f16\u7801\u6d41\u884c\u5ea6\u4fe1\u53f7\u7684\u795e\u7ecf\u5143\u5e76\u8c03\u6574\u5176\u6fc0\u6d3b\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u63a7\u5236\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5c11\u6570\u70ed\u95e8\u7269\u54c1\u5360\u636e\u5927\u90e8\u5206\u6ce8\u610f\u529b\uff0c\u800c\u591a\u6570\u975e\u70ed\u95e8\u7269\u54c1\u66dd\u5149\u4e0d\u8db3\u3002\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u89e3\u91ca\u5176\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u63d0\u51faPopSteer\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u590d\u5236\u5df2\u8bad\u7ec3\u63a8\u8350\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5b9e\u73b0\u795e\u7ecf\u5143\u7ea7\u53ef\u89e3\u91ca\u6027\uff1b2\uff09\u521b\u5efa\u504f\u597d\u70ed\u95e8\u6216\u975e\u70ed\u95e8\u7269\u54c1\u7684\u5408\u6210\u7528\u6237\uff0c\u901a\u8fc7\u6fc0\u6d3b\u6a21\u5f0f\u8bc6\u522b\u7f16\u7801\u6d41\u884c\u5ea6\u4fe1\u53f7\u7684\u795e\u7ecf\u5143\uff1b3\uff09\u901a\u8fc7\u8c03\u6574\u6700\u504f\u7f6e\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u6765\u5f15\u5bfc\u63a8\u8350\u7ed3\u679c\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u987a\u5e8f\u63a8\u8350\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cPopSteer\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\uff0c\u540c\u65f6\u5bf9\u51c6\u786e\u6027\u5f71\u54cd\u6700\u5c0f\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u548c\u7ec6\u7c92\u5ea6\u7684\u516c\u5e73\u6027-\u51c6\u786e\u6027\u6743\u8861\u63a7\u5236\u3002", "conclusion": "PopSteer\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u516c\u5e73\u6027\uff0c\u5e76\u5141\u8bb8\u7ec6\u7c92\u5ea6\u7684\u516c\u5e73\u6027-\u51c6\u786e\u6027\u6743\u8861\u63a7\u5236\u3002"}}
{"id": "2601.15205", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15205", "abs": "https://arxiv.org/abs/2601.15205", "authors": ["Sangeet Sharma"], "title": "Beyond the Geometric Curse: High-Dimensional N-Gram Hashing for Dense Retrieval", "comment": "11 page long, 5 figure. Yes, am undergrad in pharmacy and love computer work", "summary": "Why do even the most powerful 7B-parameter embedding models struggle with simple retrieval tasks that the decades old BM25 handles with ease? Recent theory suggests that this happens because of a dimensionality bottleneck. This occurs when we force infinite linguistic nuances into small, fixed-length learned vectors. We developed NUMEN to break this bottleneck by removing the learning process entirely. Instead of training heavy layers to map text to a constrained space, NUMEN uses deterministic character hashing to project language directly onto high-dimensional vectors. This approach requires no training, supports an unlimited vocabulary, and allows the geometric capacity scale as needed. On the LIMIT benchmark, NUMEN achieves 93.90 % Recall@100 at 32,768 dimensions. This makes it the first dense retrieval model to officially surpass the sparse BM25 baseline 93.6 %. Our findings show that the real problem in dense retrieval isn't the architecture, but the embedding layer itself. The solution isn't necessarily smarter training, but simply providing more room to breathe.", "AI": {"tldr": "NUMEN\u4f7f\u7528\u786e\u5b9a\u6027\u5b57\u7b26\u54c8\u5e0c\u5c06\u6587\u672c\u76f4\u63a5\u6295\u5f71\u5230\u9ad8\u7ef4\u5411\u91cf\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u7a81\u7834\u4e86\u5bc6\u96c6\u68c0\u7d22\u4e2d\u7684\u7ef4\u5ea6\u74f6\u9888\uff0c\u9996\u6b21\u5728LIMIT\u57fa\u51c6\u4e0a\u8d85\u8d8aBM25\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5f3a\u5927\u76847B\u53c2\u6570\u5d4c\u5165\u6a21\u578b\u5728\u7b80\u5355\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u51e0\u5341\u5e74\u524d\u7684BM25\uff0c\u7406\u8bba\u8868\u660e\u8fd9\u662f\u7531\u4e8e\u7ef4\u5ea6\u74f6\u9888\u95ee\u9898\u2014\u2014\u5c06\u65e0\u9650\u7684\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u538b\u7f29\u5230\u56fa\u5b9a\u957f\u5ea6\u7684\u5c0f\u5411\u91cf\u4e2d\u3002", "method": "NUMEN\u91c7\u7528\u786e\u5b9a\u6027\u5b57\u7b26\u54c8\u5e0c\u65b9\u6cd5\uff0c\u5b8c\u5168\u79fb\u9664\u5b66\u4e60\u8fc7\u7a0b\uff0c\u76f4\u63a5\u5c06\u8bed\u8a00\u6295\u5f71\u5230\u9ad8\u7ef4\u5411\u91cf\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u652f\u6301\u65e0\u9650\u8bcd\u6c47\u8868\uff0c\u51e0\u4f55\u5bb9\u91cf\u53ef\u6309\u9700\u6269\u5c55\u3002", "result": "\u5728LIMIT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNUMEN\u572832,768\u7ef4\u5ea6\u4e0b\u8fbe\u523093.90%\u7684Recall@100\uff0c\u9996\u6b21\u4f7f\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u6b63\u5f0f\u8d85\u8d8a\u7a00\u758fBM25\u57fa\u7ebf\uff0893.6%\uff09\u3002", "conclusion": "\u5bc6\u96c6\u68c0\u7d22\u7684\u771f\u6b63\u95ee\u9898\u4e0d\u5728\u4e8e\u67b6\u6784\uff0c\u800c\u5728\u4e8e\u5d4c\u5165\u5c42\u672c\u8eab\u3002\u89e3\u51b3\u65b9\u6848\u4e0d\u4e00\u5b9a\u662f\u66f4\u667a\u80fd\u7684\u8bad\u7ec3\uff0c\u800c\u662f\u4e3a\u8868\u793a\u63d0\u4f9b\u66f4\u591a\u7a7a\u95f4\u3002"}}

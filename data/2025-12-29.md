<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Selective LLM-Guided Regularization for Enhancing Recommendation Models](https://arxiv.org/abs/2512.21526)
*Shanglin Yang,Zhan Shi*

Main category: cs.IR

TL;DR: 提出选择性LLM引导正则化框架，仅在LLM可靠时激活其成对排序监督，提升推荐系统在冷启动和长尾场景的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将LLM作为独立推荐器（成本高、有偏见、不可靠），要么进行全局知识蒸馏（强制下游模型模仿LLM预测，即使指导不准确）。LLM在重排序和挑战性场景表现优异，但并非在所有上下文都一致可靠。

Method: 选择性LLM引导正则化框架：1）使用可训练的门控机制（基于用户历史长度、物品流行度和模型不确定性）预测LLM何时可靠；2）仅在LLM可靠时激活基于LLM的成对排序监督；3）所有LLM评分离线完成，不增加推理成本

Result: 在多个数据集上的实验表明，该选择性策略持续提升整体准确性，在冷启动和长尾场景带来显著增益，优于全局蒸馏基线方法

Conclusion: 选择性激活LLM监督比全局蒸馏更有效，能够智能利用LLM的优势（特别是在挑战性场景），同时避免其不可靠时的负面影响，实现了计算高效的知识迁移

Abstract: Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.

</details>


### [2] [CEMG: Collaborative-Enhanced Multimodal Generative Recommendation](https://arxiv.org/abs/2512.21543)
*Yuzhen Lin,Hongyi Chen,Xuanjing Chen,Shaowen Wang,Ivonne Xu,Dongming Jiang*

Main category: cs.IR

TL;DR: CEMG框架通过协作信号引导的多模态融合、统一模态标记化和端到端生成推荐，解决了生成推荐中协作信号浅层整合和多模态特征解耦融合的问题。


<details>
  <summary>Details</summary>
Motivation: 生成推荐模型面临两大挑战：1) 协作信号的浅层整合，2) 多模态特征的解耦融合。这些限制阻碍了创建真正全面的物品表示。

Method: 提出CEMG框架，包含：1) 多模态融合层，在协作信号引导下动态整合视觉和文本特征；2) 统一模态标记化阶段，使用残差量化VAE将融合表示转换为离散语义代码；3) 端到端生成推荐阶段，微调大语言模型自回归生成物品代码。

Result: 大量实验表明，CEMG显著优于最先进的基线方法。

Conclusion: CEMG通过协作增强的多模态融合和生成建模，有效解决了生成推荐中的关键挑战，实现了更全面的物品表示和更好的推荐性能。

Abstract: Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.

</details>


### [3] [LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model](https://arxiv.org/abs/2512.21595)
*Yinfu Feng,Yanjing Wu,Rong Xiao,Xiaoyi Zen*

Main category: cs.IR

TL;DR: LLM-I2I：利用大语言模型解决I2I推荐系统中数据稀疏和噪声问题的数据中心化框架，通过生成合成交互和过滤噪声数据提升推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有I2I推荐系统面临两个方向的挑战：模型中心化方法计算成本高且部署复杂，数据中心化方法受限于数据稀疏和噪声问题。需要一种既能保持成本效益又能解决数据质量问题的方案

Method: 提出LLM-I2I框架：1）基于LLM的生成器为长尾物品合成用户-物品交互，缓解数据稀疏；2）基于LLM的判别器过滤真实和合成数据中的噪声交互；3）融合精炼后的数据训练I2I模型

Result: 在行业（AEDS）和学术（ARD）数据集上评估，LLM-I2I持续提升推荐准确率，特别是对长尾物品。在大型跨境电商平台部署后，召回数提升6.02%，商品交易总额提升1.22%

Conclusion: LLM-I2I展示了在不修改模型架构的情况下，利用LLM增强数据中心化推荐系统的潜力，为I2I推荐系统提供了高效的数据质量提升方案

Abstract: Item-to-Item (I2I) recommendation models are widely used in real-world systems due to their scalability, real-time capabilities, and high recommendation quality. Research to enhance I2I performance focuses on two directions: 1) model-centric approaches, which adopt deeper architectures but risk increased computational costs and deployment complexity, and 2) data-centric methods, which refine training data without altering models, offering cost-effectiveness but struggling with data sparsity and noise. To address these challenges, we propose LLM-I2I, a data-centric framework leveraging Large Language Models (LLMs) to mitigate data quality issues. LLM-I2I includes (1) an LLM-based generator that synthesizes user-item interactions for long-tail items, alleviating data sparsity, and (2) an LLM-based discriminator that filters noisy interactions from real and synthetic data. The refined data is then fused to train I2I models. Evaluated on industry (AEDS) and academic (ARD) datasets, LLM-I2I consistently improves recommendation accuracy, particularly for long-tail items. Deployed on a large-scale cross-border e-commerce platform, it boosts recall number (RN) by 6.02% and gross merchandise value (GMV) by 1.22% over existing I2I models. This work highlights the potential of LLMs in enhancing data-centric recommendation systems without modifying model architectures.

</details>


### [4] [KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering](https://arxiv.org/abs/2512.21799)
*Hung-Nghiep Tran,Atsuhiro Takasu*

Main category: cs.IR

TL;DR: KG20C和KG20C-QA是两个用于学术数据问答研究的精选数据集，前者是基于微软学术图谱构建的高质量学术知识图谱，后者是基于KG20C构建的问答基准数据集。


<details>
  <summary>Details</summary>
Motivation: 为学术领域的问答、推理和知识驱动应用研究提供可重用、可扩展的资源，解决现有学术数据集中缺乏高质量、结构化的知识图谱和标准化问答基准的问题。

Method: 通过有针对性的会议选择、基于质量的过滤和模式定义，从微软学术图谱构建KG20C知识图谱；然后定义一组问答模板，将图谱三元组转换为自然语言问答对，构建KG20C-QA数据集。

Result: 创建了两个高质量数据集：KG20C（学术知识图谱）和KG20C-QA（问答基准），提供了标准知识图谱嵌入方法在KG20C-QA上的基准测试结果，并分析了不同关系类型的性能表现。

Conclusion: 通过正式发布这些经过充分文档记录的数据集，为研究社区贡献了可重用的资源，支持未来在学术领域的问答、推理和知识驱动应用研究。

Abstract: In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.

</details>


### [5] [Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion](https://arxiv.org/abs/2512.21863)
*Huatuan Sun,Yunshan Ma,Changguang Wu,Yanxin Zhang,Pengfei Wang,Xiaoyu Du*

Main category: cs.IR

TL;DR: 本文系统评估了冻结大型视频语言模型在微视频推荐中的应用，提出了双特征融合框架，通过融合多层中间表示与ID嵌入实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前冻结大型视频语言模型在微视频推荐中缺乏系统性评估，通常被用作固定的黑盒特征提取器，没有系统比较不同的表示策略。

Method: 提出双特征融合框架，系统研究两个关键设计维度：1)与ID嵌入的集成策略（替换vs融合）；2)特征提取范式（LVLM生成的标题vs中间解码器隐藏状态）。

Result: 发现三个关键原则：1)中间隐藏状态始终优于基于标题的表示；2)ID嵌入捕获不可替代的协同信号，融合严格优于替换；3)中间解码器特征在不同层间有效性差异显著。DFF框架在两个真实世界微视频推荐基准上达到SOTA性能。

Conclusion: DFF提供了一个轻量级、即插即用的方法，自适应地融合冻结LVLMs的多层表示与项目ID嵌入，为将现成大型视觉语言模型集成到微视频推荐系统提供了原则性方法。

Abstract: Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.

</details>

{"id": "2511.08941", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.08941", "abs": "https://arxiv.org/abs/2511.08941", "authors": ["Chenhao Wang", "Shanshan Feng", "Lisi Chen", "Fan Li", "Shuo Shang"], "title": "Efficient Model-Agnostic Continual Learning for Next POI Recommendation", "comment": "This paper was accepted by ICDE2026", "summary": "Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.", "AI": {"tldr": "GIRAM\u662f\u4e00\u4e2a\u7528\u4e8e\u6301\u7eed\u4e0b\u4e00\u4e2a\u5174\u8da3\u70b9\u63a8\u8350\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6301\u7eed\u5174\u8da3\u548c\u8fd1\u671f\u5174\u8da3\uff0c\u5728\u52a8\u6001\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u884c\u4e3a\u7684\u540c\u65f6\u4fdd\u6301\u5386\u53f2\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u548c\u65f6\u95f4\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u4e0b\u4e00\u4e2a\u5174\u8da3\u70b9\u63a8\u8350\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u548c\u56fa\u5b9a\u6a21\u578b\uff0c\u65e0\u6cd5\u9002\u5e94\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7528\u6237\u884c\u4e3a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u66f4\u65b0\u3001\u6355\u6349\u7528\u6237\u5174\u8da3\u53d8\u5316\u540c\u65f6\u4fdd\u7559\u5386\u53f2\u77e5\u8bc6\u7684\u52a8\u6001\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u63d0\u51faGIRAM\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u5174\u8da3\u8bb0\u5fc6\u5e93\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u952e\u7f16\u7801\u6a21\u5757\u3001\u57fa\u4e8e\u751f\u6210\u5f0f\u952e\u7684\u68c0\u7d22\u6a21\u5757\u3001\u81ea\u9002\u5e94\u5174\u8da3\u66f4\u65b0\u4e0e\u878d\u5408\u6a21\u5757\u3002\u8be5\u6846\u67b6\u53ef\u4e0e\u73b0\u6709\u63a8\u8350\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGIRAM\u5728\u66f4\u65b0\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GIRAM\u4e3a\u89e3\u51b3\u6301\u7eed\u4e0b\u4e00\u4e2aPOI\u63a8\u8350\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u53d8\u5316\u7684\u7528\u6237\u5174\u8da3\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2511.09250", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.09250", "abs": "https://arxiv.org/abs/2511.09250", "authors": ["Jiyuan Wang", "Li Zhang", "Haipeng Lin", "Qile Liu", "Gan Huang", "Ziyu Li", "Zhen Liang", "Xia Wu"], "title": "NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning", "comment": null, "summary": "Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality- level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.", "AI": {"tldr": "NeuroCLIP\u662f\u4e00\u4e2a\u9488\u5bf9EEG-\u56fe\u50cf\u5bf9\u6bd4\u5b66\u4e60\u7684\u63d0\u793a\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u89c6\u89c9\u5d4c\u5165\u3001\u89c6\u89c9\u63d0\u793a\u4ee4\u724c\u548c\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u5230\u56fe\u50cf\u68c0\u7d22\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06CLIP\u89c6\u4e3a\u9759\u6001\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5ffd\u89c6\u4e86\u5176\u5bf9\u795e\u7ecf\u8868\u5f81\u7684\u9002\u5e94\u6027\u4ee5\u53caEEG-\u56fe\u50cf\u5bf9\u9f50\u4e2d\u56fa\u6709\u7684\u751f\u7406-\u7b26\u53f7\u9e3f\u6c9f\u3002", "method": "1. \u53cc\u6d41\u89c6\u89c9\u5d4c\u5165\u7ba1\u9053\u7ed3\u5408\u52a8\u6001\u8fc7\u6ee4\u548c\u4ee4\u724c\u7ea7\u878d\u5408\u751f\u6210\u5b9e\u4f8b\u7ea7\u81ea\u9002\u5e94\u63d0\u793a\uff1b2. \u9996\u6b21\u5728EEG-\u56fe\u50cf\u5bf9\u9f50\u4e2d\u5f15\u5165\u89c6\u89c9\u63d0\u793a\u4ee4\u724c\uff1b3. \u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u79d1\u5b66\u539f\u7406\u7684\u6539\u8fdb\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5728THINGS-EEG2\u6570\u636e\u96c6\u4e0a\uff0cNeuroCLIP\u5728\u96f6\u6837\u672c\u56fe\u50cf\u68c0\u7d22\u4e2d\u8fbe\u523063.2%\u7684Top-1\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u534712.3%\uff0c\u5e76\u5728\u8de8\u88ab\u8bd5\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u751f\u7406\u611f\u77e5\u7684\u63d0\u793a\u8c03\u4f18\u5728\u8fde\u63a5\u8111\u4fe1\u53f7\u548c\u89c6\u89c9\u8bed\u4e49\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.09329", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.09329", "abs": "https://arxiv.org/abs/2511.09329", "authors": ["Andreas Konstantin Kruff", "Christin Katharina Kreutz", "Timo Breuer", "Philipp Schaer", "Krisztian Balog"], "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction", "comment": null, "summary": "Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR com- munity. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simu- lation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query pre- dictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.", "AI": {"tldr": "Sim4IA-Bench\u662f\u9996\u4e2aIR\u793e\u533a\u7684\u7528\u6237\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b160\u4e2a\u771f\u5b9e\u641c\u7d22\u4f1a\u8bdd\u548c\u6a21\u62df\u5668\u8fd0\u884c\u6570\u636e\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0b\u4e00\u67e5\u8be2\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u6210\u719f\u7684\u5ea6\u91cf\u548c\u57fa\u51c6\uff0c\u9a8c\u8bc1\u7528\u6237\u6a21\u62df\u5668\u662f\u5426\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b160\u4e2a\u771f\u5b9e\u641c\u7d22\u4f1a\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d70\u4e2a\u4f1a\u8bdd\u5305\u542b\u6700\u591a62\u4e2a\u6a21\u62df\u5668\u8fd0\u884c\uff0c\u5206\u4e3a\u4efb\u52a1A\u548c\u4efb\u52a1B\u6765\u9884\u6d4b\u7528\u6237\u7684\u4e0b\u4e00\u4e2a\u641c\u7d22\u67e5\u8be2\u6216\u8bdd\u8bed\u3002", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5c06\u771f\u5b9e\u641c\u7d22\u4f1a\u8bdd\u4e0e\u6a21\u62df\u7684\u4e0b\u4e00\u67e5\u8be2\u9884\u6d4b\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u5ea6\u91cf\u65b9\u6cd5\u3002", "conclusion": "Sim4IA-Bench\u4e3a\u8bc4\u4f30\u7528\u6237\u6a21\u62df\u65b9\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u5e76\u63a8\u52a8\u4e86\u4fe1\u606f\u8bbf\u95ee\u4e2d\u73b0\u5b9e\u548c\u53ef\u89e3\u91ca\u7528\u6237\u6a21\u62df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 11]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management](https://arxiv.org/abs/2510.15087)
*Kai Yin,Xiangjue Dong,Chengkai Liu,Allen Lin,Lingfeng Shi,Ali Mostafavi,James Caverlee*

Main category: cs.IR

TL;DR: DMRetriever是首个专门为灾害管理设计的密集检索模型系列，通过三阶段训练框架实现SOTA性能，参数效率极高。


<details>
  <summary>Details</summary>
Motivation: 现有通用检索模型无法有效处理灾害管理场景中的多样化搜索意图，导致性能不稳定不可靠。

Method: 使用双向注意力适应、无监督对比预训练和难度感知渐进指令微调的三阶段框架，结合高质量数据精炼流水线。

Result: 在所有六种搜索意图和每个模型规模上都达到SOTA性能，596M模型超过基线13.3倍大模型，33M模型仅用基线7.6%参数就超越基线。

Conclusion: DMRetriever为灾害管理提供了首个专业化检索解决方案，在性能和参数效率方面均表现卓越。

Abstract: Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

</details>


### [2] [MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation](https://arxiv.org/abs/2510.15286)
*Xianyang Qi,Yuan Tian,Zhaoyu Hu,Zhirui Kuai,Chang Liu,Hongxiang Lin,Lei Wang*

Main category: cs.IR

TL;DR: MTmixAtt是一个统一的混合专家架构，通过自动特征聚类和多混合注意力机制，解决了传统推荐系统手动特征工程和场景特定架构的限制，在工业推荐任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖手动特征工程和场景特定架构，阻碍了跨场景迁移和大规模部署。需要统一的架构来处理异构特征并实现跨场景建模。

Method: 提出MTmixAtt架构，包含AutoToken模块自动聚类异构特征为语义连贯的token，以及MTmixAttBlock模块通过可学习的混合矩阵、共享密集专家和场景感知稀疏专家实现高效token交互。

Result: 在美团TRec数据集上超越多个SOTA基线模型。在可比参数量下获得更优的CTR和CTCVR指标，MTmixAtt-1B版本实现进一步单调增益。在线A/B测试显示首页场景支付PV提升3.62%，实际支付GTV提升2.54%。

Conclusion: MTmixAtt为跨场景建模任意异构特征提供了统一且可扩展的解决方案，显著改善了用户体验和商业成果。

Abstract: Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

</details>


### [3] [GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework](https://arxiv.org/abs/2510.15299)
*Yijia Sun,Shanshan Huang,Zhiyuan Guan,Qiang Luo,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GRank是一个无需结构化索引的检索新范式，将目标感知学习与用户中心检索无缝统一，在召回率和延迟方面显著优于现有方法，已在生产环境中部署。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统检索阶段存在两个主要问题：(1)双塔架构表达能力有限，无法捕捉细粒度用户-物品交互；(2)结构化索引方法难以融入动态用户偏好，且构建维护成本高昂。

Method: GRank包含三个关键创新：(1)目标感知生成器通过GPU加速MIPS进行个性化候选生成；(2)轻量级排序器在小子集上执行细粒度推理；(3)端到端多任务学习框架确保生成和排序目标语义一致性。

Result: 在两个公共基准和十亿级生产语料上的实验表明，GRank将Recall@500提升超过30%，P99 QPS达到最先进树和基于图检索器的1.7倍。

Conclusion: GRank已在生产环境中成功部署，服务4亿活跃用户，在线A/B测试确认核心参与度指标显著提升，总应用使用时间在主应用和轻量版分别增加0.160%和0.165%。

Abstract: Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

</details>


### [4] [Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable ID-based Models](https://arxiv.org/abs/2510.15308)
*Srijan Saket,Ikuhiro Ihara,Vaibhav Sharma,Danish Kalim*

Main category: cs.IR

TL;DR: 提出一种自动确定ID特征最佳嵌入尺寸的方法，通过维度掩码层修剪嵌入向量，显著减小模型大小同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统和社交媒体平台中的大规模ID特征需要消耗大量内存的嵌入表，导致模型庞大难以部署和维护。

Method: 定义自定义Keras维度掩码层，放置在嵌入查找之后，通过仅允许前N个维度通过来修剪嵌入向量。

Result: 在公共数据集和真实生产数据的A/B测试中，有效嵌入维度减少40-50%，内存效率显著提升。

Conclusion: 该方法为处理大量ID特征的平台提供了可扩展解决方案，优化了资源使用和模型性能。

Abstract: In modern recommendation systems and social media platforms like Meta,
TikTok, and Instagram, large-scale ID-based features often require embedding
tables that consume significant memory. Managing these embedding sizes can be
challenging, leading to bulky models that are harder to deploy and maintain. In
this paper, we introduce a method to automatically determine the optimal
embedding size for ID features, significantly reducing the model size while
maintaining performance.
  Our approach involves defining a custom Keras layer called the dimension mask
layer, which sits directly after the embedding lookup. This layer trims the
embedding vector by allowing only the first N dimensions to pass through. By
doing this, we can reduce the input feature dimension by more than half with
minimal or no loss in model performance metrics. This reduction helps cut down
the memory footprint of the model and lowers the risk of overfitting due to
multicollinearity.
  Through offline experiments on public datasets and an online A/B test on a
real production dataset, we demonstrate that using a dimension mask layer can
shrink the effective embedding dimension by 40-50\%, leading to substantial
improvements in memory efficiency. This method provides a scalable solution for
platforms dealing with a high volume of ID features, optimizing both resource
usage and model performance.

</details>


### [5] [Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs](https://arxiv.org/abs/2510.15428)
*Sho Okazaki,Kohei Kaminishi,Takuma Fujiu,Yusheng Wang,Jun Ota*

Main category: cs.IR

TL;DR: 提出了一种结合制造领域概念化和图神经网络推理的过程感知框架，通过本体引导的LLM提取将FMEA工作表转换为统一知识图，使用RGCN进行链接预测，显著提高了FMEA知识在异构生产线间的可重用性。


<details>
  <summary>Details</summary>
Motivation: 解决自动化制造生产线中故障原因识别的挑战，包括系统复杂性、频繁重新配置以及现有FMEA知识有限的可重用性，特别是克服自然语言变异性、术语不一致和流程差异对FMEA知识跨线重用的阻碍。

Method: 1. 通过本体引导的LLM提取将多个生产线的FMEA工作表转换为统一知识图；2. 使用具有过程感知评分函数的RGCN学习既尊重语义关系又考虑顺序流程的嵌入；3. 采用链接预测来推断和排序与目标生产线流程一致的候选故障原因。

Result: 在汽车压力传感器装配线的案例研究中，该方法在故障原因识别方面表现最佳（F1@20 = 0.523），优于最先进的检索增强生成基线（0.267）和RGCN方法（0.400）。消融研究证实了LLM驱动的领域概念化和过程感知学习的贡献。

Conclusion: 该框架显著提高了FMEA知识在异构生产线间的可转移性，有助于操作人员更可靠地诊断故障，并为智能制造中未来领域自适应LLM应用铺平了道路。

Abstract: Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

</details>


### [6] [Enhance Large Language Models as Recommendation Systems with Collaborative Filtering](https://arxiv.org/abs/2510.15647)
*Zhisheng Yang,Xiaofei Xu,Ke Deng,Li Li*

Main category: cs.IR

TL;DR: 提出Critic-LLM-RS方法，通过训练独立的协同过滤模型Critic为LLMs提供反馈，在不微调LLMs的情况下提升推荐质量


<details>
  <summary>Details</summary>
Motivation: 现有非微调策略的LLM推荐方法缺乏特定任务的企业知识，且未有效整合协同过滤这一成功推荐技术

Method: 训练独立的协同过滤模型Critic，学习用户与物品的交互关系，为LLMs提供反馈来优化推荐结果

Result: 在真实数据集上的广泛实验验证了Critic-LLM-RS的有效性

Conclusion: Critic-LLM-RS成功填补了非微调LLM推荐方法中协同过滤整合的空白，显著提升了推荐质量

Abstract: As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

</details>


### [7] [SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15682)
*Ines Besrour,Jingbo He,Tobias Schreieder,Michael Färber*

Main category: cs.IR

TL;DR: SQuAI是一个可扩展且可信赖的多智能体检索增强生成框架，用于科学问答，基于230万篇arXiv论文，通过四个协作智能体分解复杂问题、检索证据并提供可验证的引用。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在科学领域中的局限性，处理复杂开放域问题需要准确答案、明确的引用声明以及跨数百万科学文档的检索。

Method: 使用四个协作智能体：分解复杂问题为子问题，通过混合稀疏-稠密检索获取目标证据，自适应过滤文档以提高上下文相关性，集成内联引用并提供源文档支持句子。

Result: 相比强RAG基线，在忠实度、答案相关性和上下文相关性方面提升了高达+0.088（12%），并发布了包含1000个科学问答证据三元组的基准数据集。

Conclusion: SQuAI通过透明推理、可验证引用和领域级可扩展性，展示了多智能体RAG如何实现更可信赖的科学问答。

Abstract: We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

</details>


### [8] [Mixture of Experts Approaches in Dense Retrieval Tasks](https://arxiv.org/abs/2510.15683)
*Effrosyni Sokli,Pranav Kasela,Georgios Peikos,Gabriella Pasi*

Main category: cs.IR

TL;DR: 提出SB-MoE方法，在DRM的最终Transformer层后添加单个MoE块，相比传统每层都加MoE的方法更高效，在轻量级模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决密集检索模型泛化能力差的问题，同时避免传统MoE方法参数过多的问题。

Method: 在DRM的最终Transformer层后引入单个MoE块，使用两种评估设置：领域内微调和零样本泛化评估。

Result: SB-MoE在轻量级模型上表现突出，在参数较多的模型上需要更多训练样本才能提升性能。

Conclusion: SB-MoE是高效且有效的DRM增强方法，特别适合轻量级模型，能显著提升检索性能。

Abstract: Dense Retrieval Models (DRMs) are a prominent development in Information
Retrieval (IR). A key challenge with these neural Transformer-based models is
that they often struggle to generalize beyond the specific tasks and domains
they were trained on. To address this challenge, prior research in IR
incorporated the Mixture-of-Experts (MoE) framework within each Transformer
layer of a DRM, which, though effective, substantially increased the number of
additional parameters. In this paper, we propose a more efficient design, which
introduces a single MoE block (SB-MoE) after the final Transformer layer. To
assess the retrieval effectiveness of SB-MoE, we perform an empirical
evaluation across three IR tasks. Our experiments involve two evaluation
setups, aiming to assess both in-domain effectiveness and the model's zero-shot
generalizability. In the first setup, we fine-tune SB-MoE with four different
underlying DRMs on seven IR benchmarks and evaluate them on their respective
test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform
zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform
further experiments to analyze the model's dependency on its hyperparameters
(i.e., the number of employed and activated experts) and investigate how this
variation affects SB-MoE's performance. The obtained results show that SB-MoE
is particularly effective for DRMs with lightweight base models, such as
TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning
across benchmarks. For DRMs with more parameters, such as BERT-Base and
Contriever, our model requires a larger number of training samples to achieve
improved retrieval performance. Our code is available online at:
https://github.com/FaySokli/SB-MoE.

</details>


### [9] [GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery](https://arxiv.org/abs/2510.15706)
*Italo Luis da Silva,Hanqi Yan,Lin Gui,Yulan He*

Main category: cs.IR

TL;DR: GraphMind是一个交互式网络工具，帮助用户评估科学论文的新颖性，通过整合外部API和LLMs支持论文的注释、提取、检索和分类。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助科学文献分析方法透明度有限，缺乏通过信息检索模块实现结果可追溯性的机制。

Method: 开发GraphMind工具，集成arXiv和Semantic Scholar等外部API与LLMs，支持用户注释论文关键元素，通过多种关系探索相关论文，并提供可验证的上下文洞察。

Result: GraphMind提供了一个易于使用的交互式网络工具，使用户能够捕获科学论文的主要结构，从多个角度探索相关想法，并通过提供可验证的上下文洞察来评估新颖性。

Conclusion: GraphMind通过结合外部API和LLMs，为用户提供了科学论文核心贡献及其与现有工作联系的丰富结构化视图，解决了现有方法在透明度和可追溯性方面的不足。

Abstract: Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

</details>


### [10] [The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation](https://arxiv.org/abs/2510.15722)
*Da Li,Zecheng Fang,Qiang Yan,Wei Huang,Xuanpu Luo*

Main category: cs.IR

TL;DR: 本文介绍了在CCIR CUP 2025中提出的法律知识检索与生成方法，利用大语言模型和信息检索系统基于法律条文回答用户问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成在自然语言处理领域取得显著进展，但在法律领域的应用仍处于探索阶段，需要专门的方法来处理法律知识检索与生成。

Method: 结合大语言模型和信息检索系统，从可靠来源检索法律条文，并基于检索内容生成相关且上下文适当的回答。

Result: 该方法能够基于法律条文为用户问题提供相关回答，展示了在法律领域的应用潜力。

Conclusion: 提出的法律知识检索与生成方法为RAG技术在法律领域的应用提供了有效解决方案，具有进一步研究和发展的价值。

Abstract: Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

</details>


### [11] [FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens](https://arxiv.org/abs/2510.15729)
*Chao Wang,Yixin Song,Jinhui Ye,Chuan Qin,Dazhong Shen,Lingfeng Liu,Xiang Wang,Yanyong Zhang*

Main category: cs.IR

TL;DR: FACE是一个可解释的框架，将协同过滤嵌入映射到预训练LLM的token中，通过解耦投影和量化自编码器实现语义对齐，提升推荐性能而不需要微调LLM。


<details>
  <summary>Details</summary>
Motivation: 解决LLM难以解释CF方法产生的潜在非语义嵌入的问题，这限制了推荐效果和进一步应用。

Method: 提出FACE框架：1）解耦投影模块分解CF嵌入为概念特定向量；2）量化自编码器将连续嵌入转换为LLM token；3）对比对齐目标确保token与文本信号对齐。

Result: 在三个真实世界推荐数据集上的实证结果显示基准模型性能提升，可解释性研究证实了描述符的可解释性。

Conclusion: 模型无关的FACE框架无需微调LLM即可实现语义对齐，通过利用其预训练能力增强推荐性能。

Abstract: Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

</details>

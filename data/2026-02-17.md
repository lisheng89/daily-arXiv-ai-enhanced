<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 17]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [LiveNewsBench: Evaluating LLM Web Search Capabilities with Freshly Curated News](https://arxiv.org/abs/2602.13543)
*Yunfan Zhang,Kathleen McKeown,Smaranda Muresan*

Main category: cs.IR

TL;DR: LiveNewsBench是一个评估LLM代理式网页搜索能力的基准测试，通过从近期新闻自动生成问题-答案对，确保问题需要超出模型训练数据的信息，并支持多跳搜索和推理。


<details>
  <summary>Details</summary>
Motivation: 当前评估具有代理式网页搜索能力的LLM系统面临挑战，需要能够区分模型内部知识和实际搜索能力的评估方法，同时解决相关训练数据稀缺的问题。

Method: 开发自动化的数据收集和问题生成流程，从近期新闻文章生成新鲜的问题-答案对，包含需要多跳搜索、页面访问和推理的困难问题，并在测试集中包含人工验证样本。

Result: 创建了LiveNewsBench基准测试，支持定期更新，构建了大规模代理式网页搜索训练数据集，评估了包括商业和开源LLM以及基于LLM的网页搜索API在内的广泛系统。

Conclusion: LiveNewsBench提供了一个严谨且可定期更新的基准测试，能够有效评估LLM的代理式网页搜索能力，解决了该领域评估和训练数据稀缺的问题，相关资源已公开。

Abstract: Large Language Models (LLMs) with agentic web search capabilities show strong potential for tasks requiring real-time information access and complex fact retrieval, yet evaluating such systems remains challenging. We introduce \bench, a rigorous and regularly updated benchmark designed to assess the agentic web search abilities of LLMs. \bench automatically generates fresh question-answer pairs from recent news articles, ensuring that questions require information beyond an LLM's training data and enabling clear separation between internal knowledge and search capability. The benchmark features intentionally difficult questions requiring multi-hop search queries, page visits, and reasoning, making it well-suited for evaluating agentic search behavior. Our automated data curation and question generation pipeline enables frequent benchmark updates and supports construction of a large-scale training dataset for agentic web search models, addressing the scarcity of such data in the research community. To ensure reliable evaluation, we include a subset of human-verified samples in the test set. We evaluate a broad range of systems using \bench, including commercial and open-weight LLMs as well as LLM-based web search APIs. The leaderboard, datasets, and code are publicly available at livenewsbench.com.

</details>


### [2] [Unleash the Potential of Long Semantic IDs for Generative Recommendation](https://arxiv.org/abs/2602.13573)
*Ming Xia,Zhiqin Zhou,Guoxin Ma,Dongmin Huang*

Main category: cs.IR

TL;DR: ACERec通过解耦细粒度标记化与高效序列建模的粒度差距，使用注意力标记合并器和意图标记，在保持语义表达力的同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的语义ID生成推荐方法面临表达力与计算效率的权衡：基于残差量化的方法限制语义ID长度以保证序列建模可行性，而基于优化乘积量化的方法通过简单聚合压缩长语义ID，但会丢失细粒度语义信息。

Method: 提出ACERec框架：1) 使用注意力标记合并器将长表达性语义标记蒸馏为紧凑潜在表示；2) 引入专门的意图标记作为动态预测锚点；3) 通过双粒度目标指导学习过程，协调细粒度标记预测与全局项目级语义对齐。

Result: 在六个真实世界基准测试上，ACERec始终优于最先进的基线方法，NDCG@10平均提升14.40%，有效调和了语义表达力与计算效率。

Conclusion: ACERec成功解决了语义ID生成推荐中表达力与效率的权衡问题，通过解耦粒度差距、注意力标记合并和双粒度学习，实现了更好的推荐性能。

Abstract: Semantic ID-based generative recommendation represents items as sequences of discrete tokens, but it inherently faces a trade-off between representational expressiveness and computational efficiency. Residual Quantization (RQ)-based approaches restrict semantic IDs to be short to enable tractable sequential modeling, while Optimized Product Quantization (OPQ)-based methods compress long semantic IDs through naive rigid aggregation, inevitably discarding fine-grained semantic information. To resolve this dilemma, we propose ACERec, a novel framework that decouples the granularity gap between fine-grained tokenization and efficient sequential modeling. It employs an Attentive Token Merger to distill long expressive semantic tokens into compact latents and introduces a dedicated Intent Token serving as a dynamic prediction anchor. To capture cohesive user intents, we guide the learning process via a dual-granularity objective, harmonizing fine-grained token prediction with global item-level semantic alignment. Extensive experiments on six real-world benchmarks demonstrate that ACERec consistently outperforms state-of-the-art baselines, achieving an average improvement of 14.40\% in NDCG@10, effectively reconciling semantic expressiveness and computational efficiency.

</details>


### [3] [Climber-Pilot: A Non-Myopic Generative Recommendation Model Towards Better Instruction-Following](https://arxiv.org/abs/2602.13581)
*Da Guo,Shijia Wang,Qiang Xiao,Yintao Ren,Weisheng Li,Songpei Xu,Ming Yue,Bin Huang,Guanlin Wu,Chuanjiang Luo*

Main category: cs.IR

TL;DR: Climber-Pilot是一个统一的生成式检索框架，通过时间感知多项目预测缓解生成式检索的短视问题，并通过条件引导稀疏注意力实现灵活的指令跟随检索，在网易云音乐平台显著提升业务指标。


<details>
  <summary>Details</summary>
Motivation: 生成式检索在推荐系统中展现出优势，但在大规模工业场景中存在两个主要问题：1) 由于单步推理和严格延迟限制，模型倾向于将多样用户意图坍缩为局部最优预测，无法捕捉长期和多项目消费模式；2) 实际检索系统需要遵循明确的检索指令（如类别级控制和策略约束），现有条件或后处理过滤方法往往损害相关性或效率。

Method: 提出Climber-Pilot统一框架：1) 时间感知多项目预测(TAMIP)：通过时间感知掩码将长期多项目前瞻性蒸馏到模型参数中，缓解局部最优预测同时保持高效单步推理；2) 条件引导稀疏注意力(CGSA)：通过稀疏注意力将业务约束直接融入生成过程，无需额外推理步骤。

Result: 在网易云音乐平台进行大量离线实验和在线A/B测试，Climber-Pilot显著优于最先进的基线方法，核心业务指标提升4.24%。

Conclusion: Climber-Pilot成功解决了生成式检索在大规模工业场景中的短视问题和指令跟随挑战，通过TAMIP和CGSA的创新设计，在保持高效推理的同时提升了检索质量和灵活性。

Abstract: Generative retrieval has emerged as a promising paradigm in recommender systems, offering superior sequence modeling capabilities over traditional dual-tower architectures. However, in large-scale industrial scenarios, such models often suffer from inherent myopia: due to single-step inference and strict latency constraints, they tend to collapse diverse user intents into locally optimal predictions, failing to capture long-horizon and multi-item consumption patterns. Moreover, real-world retrieval systems must follow explicit retrieval instructions, such as category-level control and policy constraints. Incorporating such instruction-following behavior into generative retrieval remains challenging, as existing conditioning or post-hoc filtering approaches often compromise relevance or efficiency. In this work, we present Climber-Pilot, a unified generative retrieval framework to address both limitations. First, we introduce Time-Aware Multi-Item Prediction (TAMIP), a novel training paradigm designed to mitigate inherent myopia in generative retrieval. By distilling long-horizon, multi-item foresight into model parameters through time-aware masking, TAMIP alleviates locally optimal predictions while preserving efficient single-step inference. Second, to support flexible instruction-following retrieval, we propose Condition-Guided Sparse Attention (CGSA), which incorporates business constraints directly into the generative process via sparse attention, without introducing additional inference steps. Extensive offline experiments and online A/B testing at NetEase Cloud Music, one of the largest music streaming platforms, demonstrate that Climber-Pilot significantly outperforms state-of-the-art baselines, achieving a 4.24\% lift of the core business metric.

</details>


### [4] [GEMs: Breaking the Long-Sequence Barrier in Generative Recommendation with a Multi-Stream Decoder](https://arxiv.org/abs/2602.13631)
*Yu Zhou,Chengcheng Guo,Kuo Cai,Ji Liu,Qiang Luo,Ruiming Tang,Han Li,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: GEMs提出多流解码器框架解决生成式推荐中长序列处理难题，通过将用户行为分为近期、中期和生命周期三个时间流，采用定制化推理方案，实现高效处理超10万交互序列的终身兴趣建模。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐虽然具备强大的序列推理能力，但在处理极长用户行为序列时面临两大挑战：1）高计算成本迫使实际序列长度受限，无法捕捉用户终身兴趣；2）注意力机制固有的"近期偏好"进一步削弱了从长期历史中学习的能力。

Method: GEMs将用户行为划分为三个时间流：近期流、中期流和生命周期流，并为每个流设计定制化推理方案：近期流使用单阶段实时提取器处理即时动态；中期流采用轻量级索引器进行跨注意力计算以平衡精度和成本；生命周期流使用两阶段离线-在线压缩模块进行终身建模。这些流通过参数无关的融合策略整合，实现整体兴趣表示。

Result: 在大规模工业数据集上的实验表明，GEMs在推荐准确性上显著优于最先进方法。GEMs是首个成功部署在高并发工业环境中的终身生成式推荐框架，在处理超过10万交互的用户序列时实现了优越的推理效率。

Conclusion: GEMs通过多流解码器框架成功突破了生成式推荐中的长序列处理瓶颈，实现了对用户终身兴趣的高效建模，为工业级大规模推荐系统提供了可行的解决方案。

Abstract: While generative recommendations (GR) possess strong sequential reasoning capabilities, they face significant challenges when processing extremely long user behavior sequences: the high computational cost forces practical sequence lengths to be limited, preventing models from capturing users' lifelong interests; meanwhile, the inherent "recency bias" of attention mechanisms further weakens learning from long-term history. To overcome this bottleneck, we propose GEMs (Generative rEcommendation with a Multi-stream decoder), a novel and unified framework designed to break the long-sequence barrier by capturing users' lifelong interaction sequences through a multi-stream perspective. Specifically, GEMs partitions user behaviors into three temporal streams$\unicode{x2014}$Recent, Mid-term, and Lifecycle$\unicode{x2014}$and employs tailored inference schemes for each: a one-stage real-time extractor for immediate dynamics, a lightweight indexer for cross attention to balance accuracy and cost for mid-term sequences, and a two-stage offline-online compression module for lifelong modeling. These streams are integrated via a parameter-free fusion strategy to enable holistic interest representation. Extensive experiments on large-scale industrial datasets demonstrate that GEMs significantly outperforms state-of-the-art methods in recommendation accuracy. Notably, GEMs is the first lifelong GR framework successfully deployed in a high-concurrency industrial environment, achieving superior inference efficiency while processing user sequences of over 100,000 interactions.

</details>


### [5] [PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers](https://arxiv.org/abs/2602.13647)
*Rui Yu,Tianyi Wang,Ruixia Liu,Yinglong Wang*

Main category: cs.IR

TL;DR: PT-RAG是一个针对学术论文问答的检索增强生成框架，通过保留论文原生层次结构来降低检索熵，提高证据分配的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理学术论文时，通常将论文扁平化为非结构化块，破坏了原生层次结构。这导致检索在无序空间中进行，产生碎片化上下文，在有限token预算下将token分配给非证据区域，增加了下游语言模型的推理负担。

Method: PT-RAG将学术论文的原生层次结构作为低熵检索先验。首先继承原生层次构建结构保真的PaperTree索引，防止源头的熵增加；然后设计路径引导的检索机制，将查询语义对齐到相关章节，在固定token预算下选择高相关性的根到叶路径，产生紧凑、连贯、低熵的检索上下文。

Result: 在三个学术问答基准测试中，PT-RAG相比强基线实现了更低的章节熵和证据对齐交叉熵，表明减少了上下文碎片化，更精确地将token分配给证据区域。这些结构优势直接转化为更高的答案质量。

Conclusion: PT-RAG通过保留学术论文的原生层次结构，避免了破坏性预处理导致的熵增加，为后续检索提供了原生低熵结构基础，在固定token预算下实现了更准确的证据分配和更好的问答性能。

Abstract: Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.

</details>


### [6] [Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search](https://arxiv.org/abs/2602.13704)
*Lei Chen,Chen Ju,Xu Chen,Zhicheng Wang,Yuheng Jiao,Hongfeng Zhan,Zhaoyang Li,Shihao Xu,Zhixiang Zhao,Tong Jia,Jinsong Lan,Xiaoyong Zhu,Bo Zheng*

Main category: cs.IR

TL;DR: Pailitao-VL是一个面向工业搜索的多模态检索系统，通过两种范式转变解决现有方案的三大挑战：从对比学习转向绝对ID识别任务，以及从点式评估转向列表式比较校准策略，实现了高精度实时检索。


<details>
  <summary>Details</summary>
Motivation: 解决当前最先进解决方案中的三个关键挑战：检索粒度不足、对环境噪声的脆弱性，以及效率与性能之间的巨大差距。这些挑战限制了多模态检索系统在工业环境中的实际应用。

Method: 1. 嵌入范式转变：从传统对比学习转向绝对ID识别任务，将实例锚定到由数十亿语义原型定义的全局一致潜在空间。2. 生成式重排序器演进：从孤立点式评估转向比较校准列表式策略，结合基于分块的比较推理和校准的绝对相关性评分。

Result: 在阿里巴巴电商平台的离线基准测试和在线A/B测试中，Pailitao-VL实现了最先进的性能，并产生了显著的商业影响。系统克服了现有嵌入解决方案的随机性和粒度瓶颈，同时避免了传统重排序方法的高延迟问题。

Conclusion: 这项工作展示了在要求苛刻的大规模生产环境中部署先进MLLM（多模态大语言模型）检索架构的稳健且可扩展的路径，为工业级多模态检索系统提供了有效的解决方案。

Abstract: In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.

</details>


### [7] [DMESR: Dual-view MLLM-based Enhancing Framework for Multimodal Sequential Recommendation](https://arxiv.org/abs/2602.13715)
*Mingyao Huang,Qidong Liu,Wenxuan Yang,Moranxin Wang,Yuqi Sun,Haiping Zhu,Feng Tian,Yan Chen*

Main category: cs.IR

TL;DR: DMESR框架通过对比学习对齐多模态表示，并利用交叉注意力融合MLLM粗粒度语义与原始文本细粒度语义，提升序列推荐性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLM增强的推荐方法存在两个关键限制：1）多模态表示对齐效果不佳，导致跨模态语义信息利用不足；2）过度依赖MLLM生成内容，忽略了物品原始文本数据中的细粒度语义线索

Method: 提出DMESR框架：1）使用对比学习机制对齐MLLM生成的跨模态语义表示；2）引入交叉注意力融合模块，将MLLM获取的粗粒度语义知识与原始文本的细粒度语义相结合；3）将融合后的表示无缝集成到下游序列推荐模型中

Result: 在三个真实世界数据集和三种流行序列推荐架构上的广泛实验表明，该方法具有优越的有效性和泛化能力

Conclusion: DMESR框架通过有效对齐多模态表示并融合粗粒度与细粒度语义，显著提升了多模态序列推荐的性能，解决了现有MLLM增强方法的关键局限性

Abstract: Sequential Recommender Systems (SRS) aim to predict users' next interaction based on their historical behaviors, while still facing the challenge of data sparsity. With the rapid advancement of Multimodal Large Language Models (MLLMs), leveraging their multimodal understanding capabilities to enrich item semantic representation has emerged as an effective enhancement strategy for SRS. However, existing MLLM-enhanced recommendation methods still suffer from two key limitations. First, they struggle to effectively align multimodal representations, leading to suboptimal utilization of semantic information across modalities. Second, they often overly rely on MLLM-generated content while overlooking the fine-grained semantic cues contained in the original textual data of items. To address these issues, we propose a Dual-view MLLM-based Enhancing framework for multimodal Sequential Recommendation (DMESR). For the misalignment issue, we employ a contrastive learning mechanism to align the cross-modal semantic representations generated by MLLMs. For the loss of fine-grained semantics, we introduce a cross-attention fusion module that integrates the coarse-grained semantic knowledge obtained from MLLMs with the fine-grained original textual semantics. Finally, these two fused representations can be seamlessly integrated into the downstream sequential recommendation models. Extensive experiments conducted on three real-world datasets and three popular sequential recommendation architectures demonstrate the superior effectiveness and generalizability of our proposed approach.

</details>


### [8] [A Tale of Two Graphs: Separating Knowledge Exploration from Outline Structure for Open-Ended Deep Research](https://arxiv.org/abs/2602.13830)
*Zhuofan Shi,Ming Ma,Zekun Yao,Fangkai Yang,Jue Zhang,Dongge Han,Victor Rühle,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.IR

TL;DR: DualGraph记忆架构通过分离知识存储与写作过程，使用知识图和提纲图协同演化，解决了开放深度研究中知识积累与探索效率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放深度研究(OEDR)代理主要采用线性"搜索-生成"或提纲中心规划方法，前者在证据增长时出现"迷失中间"问题，后者仅依赖提纲隐含推断知识缺口，导致关系识别和针对性探索的监督较弱。

Method: 提出DualGraph记忆架构，分离代理所知与写作过程，维护两个协同演化的图：提纲图(OG)和知识图(KG)。知识图存储细粒度知识单元（核心实体、概念及其关系），通过分析知识图拓扑和提纲图结构信号，生成针对性搜索查询。

Result: 在DeepResearch Bench、DeepResearchGym和DeepConsult三个基准上，DualGraph在报告深度、广度和事实基础方面持续优于最先进基线；在DeepResearch Bench上使用GPT-5达到53.08 RACE分数。消融研究证实了双图设计的核心作用。

Conclusion: DualGraph架构通过分离知识存储与写作过程，使用协同演化的知识图和提纲图，实现了更高效、全面的迭代知识驱动探索和精炼，显著提升了开放深度研究代理的性能。

Abstract: Open-Ended Deep Research (OEDR) pushes LLM agents beyond short-form QA toward long-horizon workflows that iteratively search, connect, and synthesize evidence into structured reports. However, existing OEDR agents largely follow either linear ``search-then-generate'' accumulation or outline-centric planning. The former suffers from lost-in-the-middle failures as evidence grows, while the latter relies on the LLM to implicitly infer knowledge gaps from the outline alone, providing weak supervision for identifying missing relations and triggering targeted exploration. We present DualGraph memory, an architecture that separates what the agent knows from how it writes. DualGraph maintains two co-evolving graphs: an Outline Graph (OG), and a Knowledge Graph (KG), a semantic memory that stores fine-grained knowledge units, including core entities, concepts, and their relations. By analyzing the KG topology together with structural signals from the OG, DualGraph generates targeted search queries, enabling more efficient and comprehensive iterative knowledge-driven exploration and refinement. Across DeepResearch Bench, DeepResearchGym, and DeepConsult, DualGraph consistently outperforms state-of-the-art baselines in report depth, breadth, and factual grounding; for example, it reaches a 53.08 RACE score on DeepResearch Bench with GPT-5. Moreover, ablation studies confirm the central role of the dual-graph design.

</details>


### [9] [DAIAN: Deep Adaptive Intent-Aware Network for CTR Prediction in Trigger-Induced Recommendation](https://arxiv.org/abs/2602.13971)
*Zhihao Lv,Longtao Zhang,Ailong He,Shuzhi Cao,Shuguang Han,Jufeng Chen*

Main category: cs.IR

TL;DR: 提出DAIAN模型解决触发推荐中的意图短视问题，通过个性化意图表示和混合增强器动态适应用户意图偏好。


<details>
  <summary>Details</summary>
Motivation: 现有触发推荐方法存在意图短视问题，过度强调触发项作用，且基于ID的协同行为稀疏性限制了性能，需要更精准的意图感知推荐。

Method: DAIAN模型：1) 通过分析用户点击与触发项相关性提取个性化意图表示；2) 检索相关历史行为挖掘多样化意图；3) 使用ID和语义信息的混合增强器强化相似性；4) 基于不同意图进行自适应选择。

Result: 在公开数据集和工业电商数据集上的实验证明了DAIAN的有效性，能够更好地适应用户意图偏好。

Conclusion: DAIAN通过动态适应用户意图偏好，解决了触发推荐中的意图短视问题，结合ID和语义信息增强了推荐性能。

Abstract: Recommendation systems are essential for personalizing e-commerce shopping experiences. Among these, Trigger-Induced Recommendation (TIR) has emerged as a key scenario, which utilizes a trigger item (explicitly represents a user's instantaneous interest), enabling precise, real-time recommendations. Although several trigger-based techniques have been proposed, most of them struggle to address the intent myopia issue, that is, a recommendation system overemphasizes the role of trigger items and narrowly focuses on suggesting commodities that are highly relevant to trigger items. Meanwhile, existing methods rely on collaborative behavior patterns between trigger and recommended items to identify the user's preferences, yet the sparsity of ID-based interaction restricts their effectiveness. To this end, we propose the Deep Adaptive Intent-Aware Network (DAIAN) that dynamically adapts to users' intent preferences. In general, we first extract the users' personalized intent representations by analyzing the correlation between a user's click and the trigger item, and accordingly retrieve the user's related historical behaviors to mine the user's diverse intent. Besides, sparse collaborative behaviors constrain the performance in capturing items associated with user intent. Hence, we reinforce similarity by leveraging a hybrid enhancer with ID and semantic information, followed by adaptive selection based on varying intents. Experimental results on public datasets and our industrial e-commerce datasets demonstrate the effectiveness of DAIAN.

</details>


### [10] [MixFormer: Co-Scaling Up Dense and Sequence in Industrial Recommenders](https://arxiv.org/abs/2602.14110)
*Xu Huang,Hao Zhang,Zhifang Fan,Yunwen Huang,Zhuoxing Wei,Zheng Chai,Jinan Ni,Yuchao Zheng,Qiwei Chen*

Main category: cs.IR

TL;DR: MixFormer：为推荐系统设计的统一Transformer架构，将序列建模和特征交互集成在单一主干中，解决了现有分离设计的协同缩放问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的推荐模型存在结构碎片化问题，序列建模和特征交互作为独立模块实现，导致在有限计算预算下模型容量必须在密集特征交互和序列建模之间进行次优分配，存在协同缩放挑战。

Method: 提出MixFormer统一架构，在单一主干中联合建模序列行为和特征交互；采用统一参数化实现密集容量和序列长度的有效协同缩放；引入用户-物品解耦策略进行效率优化，减少冗余计算和推理延迟。

Result: 在大规模工业数据集上的实验表明MixFormer在准确性和效率方面表现优越；在抖音和抖音轻量版两个生产推荐系统上的大规模在线A/B测试显示，用户参与度指标（包括活跃天数和应用内使用时长）持续改善。

Conclusion: MixFormer通过统一的Transformer架构解决了推荐系统中序列建模和特征交互的协同缩放问题，在保持工业实用性的同时实现了更好的性能和效率，为推荐系统的规模化提供了有效解决方案。

Abstract: As industrial recommender systems enter a scaling-driven regime, Transformer architectures have become increasingly attractive for scaling models towards larger capacity and longer sequence. However, existing Transformer-based recommendation models remain structurally fragmented, where sequence modeling and feature interaction are implemented as separate modules with independent parameterization. Such designs introduce a fundamental co-scaling challenge, as model capacity must be suboptimally allocated between dense feature interaction and sequence modeling under a limited computational budget. In this work, we propose MixFormer, a unified Transformer-style architecture tailored for recommender systems, which jointly models sequential behaviors and feature interactions within a single backbone. Through a unified parameterization, MixFormer enables effective co-scaling across both dense capacity and sequence length, mitigating the trade-off observed in decoupled designs. Moreover, the integrated architecture facilitates deep interaction between sequential and non-sequential representations, allowing high-order feature semantics to directly inform sequence aggregation and enhancing overall expressiveness. To ensure industrial practicality, we further introduce a user-item decoupling strategy for efficiency optimizations that significantly reduce redundant computation and inference latency. Extensive experiments on large-scale industrial datasets demonstrate that MixFormer consistently exhibits superior accuracy and efficiency. Furthermore, large-scale online A/B tests on two production recommender systems, Douyin and Douyin Lite, show consistent improvements in user engagement metrics, including active days and in-app usage duration.

</details>


### [11] [High Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace](https://arxiv.org/abs/2602.14358)
*Dillon Davis,Huiji Gao,Thomas Legrand,Juan Manuel Caicedo Carvajal,Malay Haldar,Kedar Bellare,Moutupsi Paul,Soumyadip Banerjee,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: Airbnb重构搜索系统，从基于深度贝叶斯bandit的矩形检索区域预测，转向使用全球2500万个均匀网格单元进行高精度检索


<details>
  <summary>Details</summary>
Motivation: Airbnb搜索面临独特的位置检索挑战：需要在资源密集型排序模型之前，高效筛选出用户可能实际预订的房源。现有基于深度贝叶斯bandit的矩形区域检索方法需要改进，以更精确地匹配用户的地理位置需求。

Method: 将全球划分为2500万个均匀网格单元，重新架构搜索系统，从最可能被预订的高精度矩形地图单元子集中进行检索，取代原有的矩形边界区域预测方法。

Result: 论文展示了新架构的方法论、挑战和影响，通过更精确的地理位置过滤提高了检索效率，为后续排序模型提供了更相关的候选房源集合。

Conclusion: 采用全球均匀网格单元的高精度检索架构能够更有效地处理Airbnb搜索中的位置检索挑战，提升房源筛选的准确性和系统效率。

Abstract: Airbnb search must balance a worldwide, highly varied supply of homes with guests whose location, amenity, style, and price expectations differ widely. Meeting those expectations hinges on an efficient retrieval stage that surfaces only the listings a guest might realistically book, before resource intensive ranking models are applied to determine the best results. Unlike many recommendation engines, our system faces a distinctive challenge, location retrieval, that sits upstream of ranking and determines which geographic areas are queried in order to filter inventory to a candidate set. The preexisting approach employs a deep bayesian bandit based system to predict a rectangular retrieval bounds area that can be used for filtering. The purpose of this paper is to demonstrate the methodology, challenges, and impact of rearchitecting search to retrieve from the subset of most bookable high precision rectangular map cells defined by dividing the world into 25M uniform cells.

</details>


### [12] [Behavioral Feature Boosting via Substitute Relationships for E-commerce Search](https://arxiv.org/abs/2602.14502)
*Chaosheng Dong,Michinari Momma,Yijia Wang,Yan Gao,Yi Sun*

Main category: cs.IR

TL;DR: 提出BFS方法，通过聚合替代产品的行为特征来缓解电商平台新品冷启动问题，提升搜索相关性和产品发现


<details>
  <summary>Details</summary>
Motivation: 电商平台新品面临冷启动问题：交互数据有限导致搜索可见性降低和相关性排序受损，影响新品竞争力

Method: BFS（行为特征增强）方法：识别满足相似用户需求的替代产品，聚合其点击、加购、购买、评分等行为信号，为新品提供"热启动"数据支持

Result: 在大型电商平台的离线和在线实验中，BFS显著提升了冷启动产品的搜索相关性和产品发现效果，方法可扩展且实用

Conclusion: BFS方法有效缓解了电商新品冷启动问题，已投入生产环境自2025年起服务客户，改善了用户体验并增加了新品曝光

Abstract: On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.

</details>


### [13] [Adaptive Autoguidance for Item-Side Fairness in Diffusion Recommender Systems](https://arxiv.org/abs/2602.14706)
*Zihan Li,Gustavo Escobedo,Marta Moscati,Oleg Lesota,Markus Schedl*

Main category: cs.IR

TL;DR: A2G-DiffRec是一种扩散推荐系统，通过自适应自动引导机制减少流行度偏差，在保持推荐准确性的同时提升项目侧公平性。


<details>
  <summary>Details</summary>
Motivation: 扩散推荐系统虽然准确性高，但存在流行度偏差问题，导致不同流行度的项目曝光不均。需要一种方法在保持准确性的同时提升公平性。

Method: 提出A2G-DiffRec，采用自适应自动引导机制：主模型由一个训练较弱的自身版本引导。通过自适应权重学习平衡主模型和弱模型的输出，使用流行度正则化监督训练以促进不同流行度项目的均衡曝光。

Result: 在MovieLens-1M、Foursquare-Tokyo和Music4All-Onion数据集上的实验表明，A2G-DiffRec能有效提升项目侧公平性，相比现有引导扩散推荐器和其他非扩散基线，仅以微小的准确性下降为代价。

Conclusion: A2G-DiffRec通过自适应自动引导机制成功解决了扩散推荐系统的流行度偏差问题，在保持推荐准确性的同时显著提升了项目侧公平性。

Abstract: Diffusion recommender systems achieve strong recommendation accuracy but often suffer from popularity bias, resulting in unequal item exposure. To address this shortcoming, we introduce A2G-DiffRec, a diffusion recommender that incorporates adaptive autoguidance, where the main model is guided by a less-trained version of itself. Instead of using a fixed guidance weight, A2G-DiffRec learns to adaptively weigh the outputs of the main and weak models during training, supervised by a popularity regularization that promotes balanced exposure across items with different popularity levels. Experimental results on the MovieLens-1M, Foursquare-Tokyo, and Music4All-Onion datasets show that A2G-DiffRec is effective in enhancing item-side fairness at a marginal cost of accuracy reduction compared to existing guided diffusion recommenders and other non-diffusion baselines.

</details>


### [14] [Orcheo: A Modular Full-Stack Platform for Conversational Search](https://arxiv.org/abs/2602.14710)
*Shaojie Jiang,Svitlana Vakulenko,Maarten de Rijke*

Main category: cs.IR

TL;DR: Orcheo是一个开源对话搜索平台，提供模块化架构、生产就绪基础设施和50+现成组件，旨在解决对话搜索研究中缺乏统一框架和难以部署端到端原型的问题。


<details>
  <summary>Details</summary>
Motivation: 对话搜索研究面临两大障碍：缺乏统一的框架来高效共享研究成果，以及难以部署用于用户评估的端到端原型。这阻碍了社区贡献的分享和实际系统的验证。

Method: Orcheo采用模块化架构，通过单文件节点模块促进组件重用；提供生产就绪的基础设施，包括双执行模式、安全凭证管理和执行遥测；内置AI编码支持降低学习曲线；提供包含50+现成组件的入门套件。

Result: Orcheo作为一个开源平台发布，通过案例研究验证了其模块性和易用性。平台已在GitHub上以MIT许可证开源，地址为https://github.com/ShaojieJiang/orcheo。

Conclusion: Orcheo成功填补了对话搜索研究中的框架空白，通过提供统一的平台促进了组件共享、研究可复现性，并降低了从原型到生产系统的部署难度。

Abstract: Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.

</details>


### [15] [Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs](https://arxiv.org/abs/2602.14784)
*Christos Koutsiaris*

Main category: cs.IR

TL;DR: IDC使用预测的用户查询意图指导文档分块，通过LLM生成意图和动态规划算法寻找最优分块边界，在多个QA数据集上显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统文档分块方法（如固定长度或基于连贯性的分块）忽略了用户意图，导致分块可能分割答案或包含无关噪声，影响信息检索效果。

Method: 提出意图驱动的动态分块（IDC）：1）使用大语言模型预测文档可能的用户意图/查询；2）采用动态规划算法寻找全局最优的分块边界，避免贪婪算法的缺陷。

Result: 在六个不同的问答数据集（新闻、维基百科、学术论文、技术文档）上评估，IDC在五个数据集上优于传统分块策略，top-1检索准确率提升5%-67%，在第六个数据集上与最佳基线持平。同时产生比基线方法少40-60%的分块，达到93-100%的答案覆盖率。

Conclusion: 将文档结构与预期的信息需求对齐能显著提升检索性能，特别是对于长且异构的文档。IDC展示了意图感知分块的有效性。

Abstract: Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.

</details>


### [16] [Beyond Retractions: Forensic Scientometrics Techniques to Identify Research Misconduct, Citation Leakage, and Funding Anomalies](https://arxiv.org/abs/2602.14793)
*Leslie D. McIntosh,Alexandra Sinclair,Simon Linacre*

Main category: cs.IR

TL;DR: 对虚构的Pharmakon神经科学研究网络进行法证科学计量学案例分析，该网络在2019-2022年间通过学术出版渠道运作


<details>
  <summary>Details</summary>
Motivation: 研究虚构研究网络如何嵌入合法学术出版渠道，揭示学术出版系统中的漏洞和潜在风险

Method: 采用法证科学计量学方法，对虚构的Pharmakon神经科学研究网络进行案例研究

Result: 揭示了虚构研究网络如何在2019-2022年间成功嵌入学术出版系统，暴露了出版渠道的脆弱性

Conclusion: 学术出版系统存在被虚构研究网络利用的漏洞，需要加强审查机制和验证流程

Abstract: This paper presents a forensic scientometric case study of the Pharmakon Neuroscience Research Network, a fabricated research collective that operated primarily between 2019 and 2022 while embedding itself within legitimate scholarly publishing channels.

</details>


### [17] [DRAMA: Domain Retrieval using Adaptive Module Allocation](https://arxiv.org/abs/2602.14960)
*Pranav Kasela,Marco Braga,Ophir Frieder,Nazli Goharian,Gabriella Pasi,Raffaele Perego*

Main category: cs.IR

TL;DR: DRAMA是一个参数和能耗高效的神经检索框架，通过动态门控机制选择特定领域适配器，在保持检索效果的同时显著减少计算资源和参数需求。


<details>
  <summary>Details</summary>
Motivation: 神经检索模型虽然效果优秀，但在大规模部署中存在巨大的计算和能耗成本，特别是在多领域场景下，训练和维护领域特定模型效率低下，统一的跨领域泛化模型难以实现。

Method: 提出DRAMA框架，集成领域特定适配器模块和动态门控机制，为每个查询选择最相关的领域知识。新领域可以通过轻量级适配器训练高效添加，避免完整模型重训练。

Result: 在多个Web检索基准测试中，DRAMA达到与领域特定模型相当的检索效果，同时仅使用其一小部分的参数和计算资源。

Conclusion: 能量感知的模型设计可以显著提高神经信息检索的可扩展性和可持续性，DRAMA框架为大规模部署提供了高效的解决方案。

Abstract: Neural models are increasingly used in Web-scale Information Retrieval (IR). However, relying on these models introduces substantial computational and energy requirements, leading to increasing attention toward their environmental cost and the sustainability of large-scale deployments. While neural IR models deliver high retrieval effectiveness, their scalability is constrained in multi-domain scenarios, where training and maintaining domain-specific models is inefficient and achieving robust cross-domain generalisation within a unified model remains difficult. This paper introduces DRAMA (Domain Retrieval using Adaptive Module Allocation), an energy- and parameter-efficient framework designed to reduce the environmental footprint of neural retrieval. DRAMA integrates domain-specific adapter modules with a dynamic gating mechanism that selects the most relevant domain knowledge for each query. New domains can be added efficiently through lightweight adapter training, avoiding full model retraining. We evaluate DRAMA on multiple Web retrieval benchmarks covering different domains. Our extensive evaluation shows that DRAMA achieves comparable effectiveness to domain-specific models while using only a fraction of their parameters and computational resources. These findings show that energy-aware model design can significantly improve scalability and sustainability in neural IR.

</details>

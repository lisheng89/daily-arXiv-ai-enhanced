{"id": "2601.22498", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.22498", "abs": "https://arxiv.org/abs/2601.22498", "authors": ["Wei Yang", "Rui Zhong", "Yiqun Chen", "Shixuan Li", "Heng Ping", "Chi Lu", "Peng Jiang"], "title": "FITMM: Adaptive Frequency-Aware Multimodal Recommendation via Information-Theoretic Representation Learning", "comment": null, "summary": "Multimodal recommendation aims to enhance user preference modeling by leveraging rich item content such as images and text. Yet dominant systems fuse modalities in the spatial domain, obscuring the frequency structure of signals and amplifying misalignment and redundancy. We adopt a spectral information-theoretic view and show that, under an orthogonal transform that approximately block-diagonalizes bandwise covariances, the Gaussian Information Bottleneck objective decouples across frequency bands, providing a principled basis for separate-then-fuse paradigm. Building on this foundation, we propose FITMM, a Frequency-aware Information-Theoretic framework for multimodal recommendation. FITMM constructs graph-enhanced item representations, performs modality-wise spectral decomposition to obtain orthogonal bands, and forms lightweight within-band multimodal components. A residual, task-adaptive gate aggregates bands into the final representation. To control redundancy and improve generalization, we regularize training with a frequency-domain IB term that allocates capacity across bands (Wiener-like shrinkage with shut-off of weak bands). We further introduce a cross-modal spectral consistency loss that aligns modalities within each band. The model is jointly optimized with the standard recommendation loss. Extensive experiments on three real-world datasets demonstrate that FITMM consistently and significantly outperforms advanced baselines.", "AI": {"tldr": "FITMM\u662f\u4e00\u4e2a\u57fa\u4e8e\u9891\u7387\u611f\u77e5\u4fe1\u606f\u8bba\u7684\u591a\u6a21\u6001\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u5206\u89e3\u5c06\u6a21\u6001\u5206\u79bb\u5230\u4e0d\u540c\u9891\u5e26\uff0c\u51cf\u5c11\u5197\u4f59\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u5728\u7a7a\u95f4\u57df\u878d\u5408\u6a21\u6001\uff0c\u8fd9\u4f1a\u6a21\u7cca\u4fe1\u53f7\u7684\u9891\u7387\u7ed3\u6784\uff0c\u653e\u5927\u6a21\u6001\u95f4\u7684\u9519\u4f4d\u548c\u5197\u4f59\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u591a\u6a21\u6001\u4fe1\u53f7\u3002", "method": "\u63d0\u51faFITMM\u6846\u67b6\uff1a1)\u6784\u5efa\u56fe\u589e\u5f3a\u7684\u7269\u54c1\u8868\u793a\uff1b2)\u8fdb\u884c\u6a21\u6001\u7ea7\u8c31\u5206\u89e3\u83b7\u5f97\u6b63\u4ea4\u9891\u5e26\uff1b3)\u6784\u5efa\u8f7b\u91cf\u7ea7\u5e26\u5185\u591a\u6a21\u6001\u7ec4\u4ef6\uff1b4)\u4f7f\u7528\u6b8b\u5dee\u4efb\u52a1\u81ea\u9002\u5e94\u95e8\u805a\u5408\u9891\u5e26\uff1b5)\u5f15\u5165\u9891\u7387\u57df\u4fe1\u606f\u74f6\u9888\u6b63\u5219\u5316\u63a7\u5236\u5197\u4f59\uff1b6)\u6dfb\u52a0\u8de8\u6a21\u6001\u8c31\u4e00\u81f4\u6027\u635f\u5931\u5bf9\u9f50\u9891\u5e26\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFITMM\u59cb\u7ec8\u4e14\u663e\u8457\u4f18\u4e8e\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9891\u7387\u611f\u77e5\u4fe1\u606f\u8bba\u89c6\u89d2\u5904\u7406\u591a\u6a21\u6001\u63a8\u8350\u95ee\u9898\uff0cFITMM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u79bb\u540e\u878d\u5408\u7684\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u9519\u4f4d\u548c\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2601.22543", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.22543", "abs": "https://arxiv.org/abs/2601.22543", "authors": ["Ruiqi Zheng", "Jinli Cao", "Jiao Yin", "Hongzhi Yin"], "title": "SCaLRec: Semantic Calibration for LLM-enabled Cloud-Device Sequential Recommendation", "comment": null, "summary": "Cloud-device collaborative recommendation partitions computation across the cloud and user devices: the cloud provides semantic user modeling, while the device leverages recent interactions and cloud semantic signals for privacy-preserving, responsive reranking. With large language models (LLMs) on the cloud, semantic user representations can improve sequential recommendation by capturing high-level intent. However, regenerating such representations via cloud LLM inference for every request is often infeasible at real-world scale. As a result, on-device reranking commonly reuses a cached cloud semantic user embedding across requests. We empirically identify a cloud semantic staleness effect: reused embeddings become less aligned with the user's latest interactions, leading to measurable ranking degradation.\n  Most existing LLM-enabled cloud-device recommenders are typically designed around on-demand cloud semantics, either by assuming low-latency cloud LLM access or by regenerating semantic embeddings per request. When per-request regeneration is infeasible and cached semantics must be reused, two technical challenges arise: (1) deciding when cached cloud semantics remain useful for on-device reranking, and (2) maintaining ranking quality when the cloud LLM cannot be invoked and only cached semantics are available. To address this gap, we introduce the Semantic Calibration for LLM-enabled Cloud-Device Recommendation (SCaLRec). First, it estimates the reliability of cached semantics under the user's latest interactions. Second, an on-device semantic calibration module is proposed to adjusts the cached semantic embedding on-device using up-to-date interaction evidence, without per-request cloud LLM involvement. Experiments on real-world datasets show that SCaLRec consistently improves recommendation performance over strong baselines under cloud semantic staleness.", "AI": {"tldr": "SCaLRec\u901a\u8fc7\u8bbe\u5907\u7aef\u8bed\u4e49\u6821\u51c6\u6a21\u5757\uff0c\u5728\u65e0\u6cd5\u5b9e\u65f6\u8c03\u7528\u4e91\u7aefLLM\u65f6\uff0c\u8c03\u6574\u7f13\u5b58\u7684\u8bed\u4e49\u5d4c\u5165\u4ee5\u9002\u5e94\u7528\u6237\u6700\u65b0\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e91\u7aef\u8bed\u4e49\u9648\u65e7\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u4e91-\u8bbe\u5907\u534f\u540c\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u4e91\u7aef\u8bed\u4e49\u9648\u65e7\u5316\u95ee\u9898\uff1a\u7531\u4e8e\u65e0\u6cd5\u4e3a\u6bcf\u4e2a\u8bf7\u6c42\u91cd\u65b0\u751f\u6210LLM\u8bed\u4e49\u5d4c\u5165\uff0c\u91cd\u7528\u7f13\u5b58\u7684\u5d4c\u5165\u4f1a\u5bfc\u81f4\u4e0e\u7528\u6237\u6700\u65b0\u4ea4\u4e92\u4e0d\u4e00\u81f4\uff0c\u9020\u6210\u63a8\u8350\u8d28\u91cf\u4e0b\u964d", "method": "\u63d0\u51faSCaLRec\u6846\u67b6\uff1a1) \u8bc4\u4f30\u7f13\u5b58\u8bed\u4e49\u5728\u7528\u6237\u6700\u65b0\u4ea4\u4e92\u4e0b\u7684\u53ef\u9760\u6027\uff1b2) \u8bbe\u5907\u7aef\u8bed\u4e49\u6821\u51c6\u6a21\u5757\uff0c\u5229\u7528\u6700\u65b0\u4ea4\u4e92\u8bc1\u636e\u8c03\u6574\u7f13\u5b58\u8bed\u4e49\u5d4c\u5165\uff0c\u65e0\u9700\u6bcf\u6b21\u8bf7\u6c42\u8c03\u7528\u4e91\u7aefLLM", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCaLRec\u5728\u4e91\u7aef\u8bed\u4e49\u9648\u65e7\u5316\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u8350\u6027\u80fd", "conclusion": "SCaLRec\u6709\u6548\u89e3\u51b3\u4e86\u4e91-\u8bbe\u5907\u534f\u540c\u63a8\u8350\u4e2d\u7f13\u5b58\u8bed\u4e49\u9648\u65e7\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8bbe\u5907\u7aef\u6821\u51c6\u673a\u5236\u5728\u4e0d\u589e\u52a0\u4e91\u7aef\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u63a8\u8350\u8d28\u91cf"}}
{"id": "2601.22547", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.22547", "abs": "https://arxiv.org/abs/2601.22547", "authors": ["Shilong Zhao", "Qinggang Yang", "Zhiyi Yin", "Xiaoshi Wang", "Zhenxing Chen", "Du Su", "Xueqi Cheng"], "title": "PersonaAct: Simulating Short-Video Users with Personalized Agents for Counterfactual Filter Bubble Auditing", "comment": null, "summary": "Short-video platforms rely on personalized recommendation, raising concerns about filter bubbles that narrow content exposure. Auditing such phenomena at scale is challenging because real user studies are costly and privacy-sensitive, and existing simulators fail to reproduce realistic behaviors due to their reliance on textual signals and weak personalization. We propose PersonaAct, a framework for simulating short-video users with persona-conditioned multimodal agents trained on real behavioral traces for auditing filter bubbles in breadth and depth. PersonaAct synthesizes interpretable personas through automated interviews combining behavioral analysis with structured questioning, then trains agents on multimodal observations using supervised fine-tuning and reinforcement learning. We deploy trained agents for filter bubble auditing and evaluate bubble breadth via content diversity and bubble depth via escape potential. The evaluation demonstrates substantial improvements in fidelity over generic LLM baselines, enabling realistic behavior reproduction. Results reveal significant content narrowing over interaction. However, we find that Bilibili demonstrates the strongest escape potential. We release the first open multimodal short-video dataset and code to support reproducible auditing of recommender systems.", "AI": {"tldr": "\u63d0\u51faPersonaAct\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u884c\u4e3a\u8f68\u8ff9\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u6a21\u62df\u77ed\u89c6\u9891\u7528\u6237\uff0c\u7528\u4e8e\u5ba1\u8ba1\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8fc7\u6ee4\u6c14\u6ce1\u73b0\u8c61\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4f9d\u8d56\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u5f15\u53d1\u8fc7\u6ee4\u6c14\u6ce1\u62c5\u5fe7\uff0c\u4f46\u5927\u89c4\u6a21\u5ba1\u8ba1\u9762\u4e34\u6311\u6218\uff1a\u771f\u5b9e\u7528\u6237\u7814\u7a76\u6210\u672c\u9ad8\u4e14\u6d89\u53ca\u9690\u79c1\uff0c\u73b0\u6709\u6a21\u62df\u5668\u4f9d\u8d56\u6587\u672c\u4fe1\u53f7\u4e14\u4e2a\u6027\u5316\u5f31\uff0c\u65e0\u6cd5\u590d\u73b0\u771f\u5b9e\u884c\u4e3a\u3002", "method": "1) \u901a\u8fc7\u7ed3\u5408\u884c\u4e3a\u5206\u6790\u548c\u7ed3\u6784\u5316\u63d0\u95ee\u7684\u81ea\u52a8\u5316\u8bbf\u8c08\u5408\u6210\u53ef\u89e3\u91ca\u7684\u7528\u6237\u753b\u50cf\uff1b2) \u5728\u591a\u6a21\u6001\u89c2\u5bdf\u4e0a\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\uff1b3) \u90e8\u7f72\u667a\u80fd\u4f53\u8fdb\u884c\u8fc7\u6ee4\u6c14\u6ce1\u5ba1\u8ba1\uff0c\u4ece\u5185\u5bb9\u591a\u6837\u6027\uff08\u5e7f\u5ea6\uff09\u548c\u9003\u79bb\u6f5c\u529b\uff08\u6df1\u5ea6\uff09\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "result": "1) \u76f8\u6bd4\u901a\u7528LLM\u57fa\u7ebf\uff0cPersonaAct\u5728\u4fdd\u771f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u80fd\u590d\u73b0\u771f\u5b9e\u884c\u4e3a\uff1b2) \u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u5185\u5bb9\u7a84\u5316\u73b0\u8c61\u660e\u663e\uff1b3) Bilibili\u5e73\u53f0\u5c55\u73b0\u51fa\u6700\u5f3a\u7684\u9003\u79bb\u6f5c\u529b\u3002", "conclusion": "PersonaAct\u6846\u67b6\u80fd\u6709\u6548\u6a21\u62df\u77ed\u89c6\u9891\u7528\u6237\u884c\u4e3a\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u5ba1\u8ba1\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5f00\u6e90\u591a\u6a21\u6001\u77ed\u89c6\u9891\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7684\u63a8\u8350\u7cfb\u7edf\u5ba1\u8ba1\u7814\u7a76\u3002"}}
{"id": "2601.22694", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.22694", "abs": "https://arxiv.org/abs/2601.22694", "authors": ["Zhen Zhao", "Tong Zhang", "Jie Xu", "Qingliang Cai", "Qile Zhang", "Leyuan Yang", "Daorui Xiao", "Xiaojia Chang"], "title": "Farewell to Item IDs: Unlocking the Scaling Potential of Large Ranking Models via Semantic Tokens", "comment": null, "summary": "Recent studies on scaling up ranking models have achieved substantial improvement for recommendation systems and search engines. However, most large-scale ranking systems rely on item IDs, where each item is treated as an independent categorical symbol and mapped to a learned embedding. As items rapidly appear and disappear, these embeddings become difficult to train and maintain. This instability impedes effective learning of neural network parameters and limits the scalability of ranking models. In this paper, we show that semantic tokens possess greater scaling potential compared to item IDs. Our proposed framework TRM improves the token generation and application pipeline, leading to 33% reduction in sparse storage while achieving 0.85% AUC increase. Extensive experiments further show that TRM could consistently outperform state-of-the-art models when model capacity scales. Finally, TRM has been successfully deployed on large-scale personalized search engines, yielding 0.26% and 0.75% improvement on user active days and change query ratio respectively through A/B test.", "AI": {"tldr": "TRM\u6846\u67b6\u7528\u8bed\u4e49token\u66ff\u4ee3item ID\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6392\u5e8f\u7cfb\u7edf\u4e2ditem ID\u5d4c\u5165\u8bad\u7ec3\u548c\u7ef4\u62a4\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u5b58\u50a8\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd", "motivation": "\u4f20\u7edf\u5927\u89c4\u6a21\u6392\u5e8f\u7cfb\u7edf\u4f9d\u8d56item ID\uff0c\u5c06\u6bcf\u4e2aitem\u89c6\u4e3a\u72ec\u7acb\u5206\u7c7b\u7b26\u53f7\u5e76\u6620\u5c04\u5230\u5b66\u4e60\u5d4c\u5165\u3002\u968f\u7740item\u5feb\u901f\u51fa\u73b0\u548c\u6d88\u5931\uff0c\u8fd9\u4e9b\u5d4c\u5165\u96be\u4ee5\u8bad\u7ec3\u548c\u7ef4\u62a4\uff0c\u963b\u788d\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u7684\u6709\u6548\u5b66\u4e60\u5e76\u9650\u5236\u6392\u5e8f\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51faTRM\u6846\u67b6\uff0c\u7528\u8bed\u4e49token\u66ff\u4ee3item ID\uff0c\u6539\u8fdbtoken\u751f\u6210\u548c\u5e94\u7528\u6d41\u7a0b\u3002\u8bed\u4e49token\u76f8\u6bd4item ID\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6f5c\u529b", "result": "TRM\u5b9e\u73b0\u7a00\u758f\u5b58\u50a8\u51cf\u5c1133%\uff0cAUC\u63d0\u53470.85%\u3002\u5728\u6a21\u578b\u5bb9\u91cf\u6269\u5c55\u65f6\u6301\u7eed\u4f18\u4e8eSOTA\u6a21\u578b\u3002\u5728\u5927\u89c4\u6a21\u4e2a\u6027\u5316\u641c\u7d22\u5f15\u64ce\u90e8\u7f72\u4e2d\uff0cA/B\u6d4b\u8bd5\u663e\u793a\u7528\u6237\u6d3b\u8dc3\u5929\u6570\u63d0\u53470.26%\uff0c\u67e5\u8be2\u53d8\u5316\u7387\u63d0\u53470.75%", "conclusion": "\u8bed\u4e49token\u6bd4item ID\u5177\u6709\u66f4\u5927\u7684\u6269\u5c55\u6f5c\u529b\uff0cTRM\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u6392\u5e8f\u7cfb\u7edf\u4e2ditem ID\u5d4c\u5165\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2601.22783", "categories": ["cs.IR", "cs.CV", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.22783", "abs": "https://arxiv.org/abs/2601.22783", "authors": ["Ilyass Moummad", "Marius Miron", "David Robinson", "Kawtar Zaher", "Herv\u00e9 Go\u00ebau", "Olivier Pietquin", "Pierre Bonnet", "Emmanuel Chemla", "Matthieu Geist", "Alexis Joly"], "title": "Compact Hypercube Embeddings for Fast Text-based Wildlife Observation Retrieval", "comment": null, "summary": "Large-scale biodiversity monitoring platforms increasingly rely on multimodal wildlife observations. While recent foundation models enable rich semantic representations across vision, audio, and language, retrieving relevant observations from massive archives remains challenging due to the computational cost of high-dimensional similarity search. In this work, we introduce compact hypercube embeddings for fast text-based wildlife observation retrieval, a framework that enables efficient text-based search over large-scale wildlife image and audio databases using compact binary representations. Building on the cross-view code alignment hashing framework, we extend lightweight hashing beyond a single-modality setup to align natural language descriptions with visual or acoustic observations in a shared Hamming space. Our approach leverages pretrained wildlife foundation models, including BioCLIP and BioLingual, and adapts them efficiently for hashing using parameter-efficient fine-tuning. We evaluate our method on large-scale benchmarks, including iNaturalist2024 for text-to-image retrieval and iNatSounds2024 for text-to-audio retrieval, as well as multiple soundscape datasets to assess robustness under domain shift. Results show that retrieval using discrete hypercube embeddings achieves competitive, and in several cases superior, performance compared to continuous embeddings, while drastically reducing memory and search cost. Moreover, we observe that the hashing objective consistently improves the underlying encoder representations, leading to stronger retrieval and zero-shot generalization. These results demonstrate that binary, language-based retrieval enables scalable and efficient search over large wildlife archives for biodiversity monitoring systems.", "AI": {"tldr": "\u63d0\u51fa\u7d27\u51d1\u8d85\u7acb\u65b9\u4f53\u5d4c\u5165\u7528\u4e8e\u5feb\u901f\u6587\u672c\u9a71\u52a8\u7684\u91ce\u751f\u52a8\u7269\u89c2\u6d4b\u68c0\u7d22\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u8868\u793a\u5b9e\u73b0\u5927\u89c4\u6a21\u56fe\u50cf\u548c\u97f3\u9891\u6570\u636e\u5e93\u7684\u9ad8\u6548\u641c\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u641c\u7d22\u6210\u672c\u3002", "motivation": "\u5927\u89c4\u6a21\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u5e73\u53f0\u4f9d\u8d56\u591a\u6a21\u6001\u91ce\u751f\u52a8\u7269\u89c2\u6d4b\uff0c\u4f46\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u7ef4\u76f8\u4f3c\u6027\u641c\u7d22\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u4ece\u6d77\u91cf\u6863\u6848\u4e2d\u68c0\u7d22\u76f8\u5173\u89c2\u6d4b\u3002", "method": "\u57fa\u4e8e\u8de8\u89c6\u56fe\u4ee3\u7801\u5bf9\u9f50\u54c8\u5e0c\u6846\u67b6\uff0c\u5c06\u8f7b\u91cf\u7ea7\u54c8\u5e0c\u6269\u5c55\u5230\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u5728\u5171\u4eab\u6c49\u660e\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e0e\u89c6\u89c9/\u542c\u89c9\u89c2\u6d4b\u3002\u5229\u7528\u9884\u8bad\u7ec3\u7684\u91ce\u751f\u52a8\u7269\u57fa\u7840\u6a21\u578b\uff08BioCLIP\u548cBioLingual\uff09\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u9002\u5e94\u54c8\u5e0c\u4efb\u52a1\u3002", "result": "\u5728iNaturalist2024\uff08\u6587\u672c\u5230\u56fe\u50cf\uff09\u548ciNatSounds2024\uff08\u6587\u672c\u5230\u97f3\u9891\uff09\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u79bb\u6563\u8d85\u7acb\u65b9\u4f53\u5d4c\u5165\u76f8\u6bd4\u8fde\u7eed\u5d4c\u5165\u5b9e\u73b0\u7ade\u4e89\u6027\u751a\u81f3\u66f4\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u548c\u641c\u7d22\u6210\u672c\u3002\u54c8\u5e0c\u76ee\u6807\u6301\u7eed\u6539\u8fdb\u5e95\u5c42\u7f16\u7801\u5668\u8868\u793a\uff0c\u589e\u5f3a\u68c0\u7d22\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4e8c\u8fdb\u5236\u3001\u57fa\u4e8e\u8bed\u8a00\u7684\u68c0\u7d22\u65b9\u6cd5\u4e3a\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u91ce\u751f\u52a8\u7269\u6863\u6848\u641c\u7d22\u65b9\u6848\u3002"}}
{"id": "2601.22925", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22925", "abs": "https://arxiv.org/abs/2601.22925", "authors": ["Weiqin Yang", "Bohao Wang", "Zhenxiang Xu", "Jiawei Chen", "Shengjia Zhang", "Jingbang Chen", "Canghong Jin", "Can Wang"], "title": "BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models", "comment": null, "summary": "Recent years have witnessed a rapid surge in research leveraging Large Language Models (LLMs) for recommendation. These methods typically employ supervised fine-tuning (SFT) to adapt LLMs to recommendation scenarios, and utilize beam search during inference to efficiently retrieve $B$ top-ranked recommended items. However, we identify a critical training-inference inconsistency: while SFT optimizes the overall probability of positive items, it does not guarantee that such items will be retrieved by beam search even if they possess high overall probabilities. Due to the greedy pruning mechanism, beam search can prematurely discard a positive item once its prefix probability is insufficient.\n  To address this inconsistency, we propose BEAR (Beam-SEarch-Aware Regularization), a novel fine-tuning objective that explicitly accounts for beam search behavior during training. Rather than directly simulating beam search for each instance during training, which is computationally prohibitive, BEAR enforces a relaxed necessary condition: each token in a positive item must rank within the top-$B$ candidate tokens at each decoding step. This objective effectively mitigates the risk of incorrect pruning while incurring negligible computational overhead compared to standard SFT. Extensive experiments across four real-world datasets demonstrate that BEAR significantly outperforms strong baselines. Code will be released upon acceptance.", "AI": {"tldr": "BEAR\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u76ee\u6807\uff0c\u901a\u8fc7\u8003\u8651beam search\u7684\u884c\u4e3a\u6765\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2dLLM\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u6b63\u6837\u672c\u5728beam search\u4e2d\u4e0d\u4f1a\u88ab\u8fc7\u65e9\u526a\u679d\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff1a\u76d1\u7763\u5fae\u8c03\u4f18\u5316\u6b63\u6837\u672c\u7684\u6574\u4f53\u6982\u7387\uff0c\u4f46beam search\u7684\u8d2a\u5fc3\u526a\u679d\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u9ad8\u6574\u4f53\u6982\u7387\u7684\u6b63\u6837\u672c\u56e0\u524d\u7f00\u6982\u7387\u4e0d\u8db3\u800c\u88ab\u8fc7\u65e9\u4e22\u5f03\u3002", "method": "\u63d0\u51faBEAR\uff08Beam-SEarch-Aware Regularization\uff09\u5fae\u8c03\u76ee\u6807\uff0c\u5f3a\u5236\u6b63\u6837\u672c\u7684\u6bcf\u4e2atoken\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\u6392\u540d\u90fd\u5728\u524dB\u4e2a\u5019\u9009token\u5185\uff0c\u8fd9\u662f\u4e00\u4e2a\u5bbd\u677e\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u907f\u514dbeam search\u4e2d\u7684\u9519\u8bef\u526a\u679d\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBEAR\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u76f8\u6bd4\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u53ea\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "BEAR\u901a\u8fc7\u663e\u5f0f\u8003\u8651beam search\u884c\u4e3a\u6765\u89e3\u51b3\u8bad\u7ec3-\u63a8\u7406\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2601.23085", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.23085", "abs": "https://arxiv.org/abs/2601.23085", "authors": ["Mohanna Hoveyda", "Jelle Piepenbrock", "Arjen P de Vries", "Maarten de Rijke", "Faegheh Hasibi"], "title": "OrLog: Resolving Complex Queries with LLMs and Probabilistic Reasoning", "comment": "Accepted to ECIR 2026", "summary": "Resolving complex information needs that come with multiple constraints should consider enforcing the logical operators encoded in the query (i.e., conjunction, disjunction, negation) on the candidate answer set. Current retrieval systems either ignore these constraints in neural embeddings or approximate them in a generative reasoning process that can be inconsistent and unreliable. Although well-suited to structured reasoning, existing neuro-symbolic approaches remain confined to formal logic or mathematics problems as they often assume unambiguous queries and access to complete evidence, conditions rarely met in information retrieval. To bridge this gap, we introduce OrLog, a neuro-symbolic retrieval framework that decouples predicate-level plausibility estimation from logical reasoning: a large language model (LLM) provides plausibility scores for atomic predicates in one decoding-free forward pass, from which a probabilistic reasoning engine derives the posterior probability of query satisfaction. We evaluate OrLog across multiple backbone LLMs, varying levels of access to external knowledge, and a range of logical constraints, and compare it against base retrievers and LLM-as-reasoner methods. Provided with entity descriptions, OrLog can significantly boost top-rank precision compared to LLM reasoning with larger gains on disjunctive queries. OrLog is also more efficient, cutting mean tokens by $\\sim$90\\% per query-entity pair. These results demonstrate that generation-free predicate plausibility estimation combined with probabilistic reasoning enables constraint-aware retrieval that outperforms monolithic reasoning while using far fewer tokens.", "AI": {"tldr": "OrLog\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u68c0\u7d22\u6846\u67b6\uff0c\u5c06\u8c13\u8bcd\u7ea7\u53ef\u80fd\u6027\u4f30\u8ba1\u4e0e\u903b\u8f91\u63a8\u7406\u89e3\u8026\uff0c\u4f7f\u7528LLM\u63d0\u4f9b\u539f\u5b50\u8c13\u8bcd\u7684\u53ef\u80fd\u6027\u5206\u6570\uff0c\u7136\u540e\u901a\u8fc7\u6982\u7387\u63a8\u7406\u5f15\u64ce\u8ba1\u7b97\u67e5\u8be2\u6ee1\u8db3\u7684\u540e\u9a8c\u6982\u7387\uff0c\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u68c0\u7d22\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u7cfb\u7edf\u8981\u4e48\u5ffd\u7565\u67e5\u8be2\u4e2d\u7684\u903b\u8f91\u7ea6\u675f\uff0c\u8981\u4e48\u5728\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fd1\u4f3c\u5904\u7406\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u548c\u4e0d\u53ef\u9760\u3002\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u867d\u7136\u9002\u5408\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u4f46\u5c40\u9650\u4e8e\u5f62\u5f0f\u903b\u8f91\u6216\u6570\u5b66\u95ee\u9898\uff0c\u5047\u8bbe\u67e5\u8be2\u660e\u786e\u4e14\u8bc1\u636e\u5b8c\u6574\uff0c\u8fd9\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5f88\u5c11\u6ee1\u8db3\u3002", "method": "OrLog\u6846\u67b6\u5c06\u8c13\u8bcd\u7ea7\u53ef\u80fd\u6027\u4f30\u8ba1\u4e0e\u903b\u8f91\u63a8\u7406\u89e3\u8026\uff1a1) LLM\u5728\u4e00\u6b21\u65e0\u89e3\u7801\u524d\u5411\u4f20\u9012\u4e2d\u4e3a\u539f\u5b50\u8c13\u8bcd\u63d0\u4f9b\u53ef\u80fd\u6027\u5206\u6570\uff1b2) \u6982\u7387\u63a8\u7406\u5f15\u64ce\u6839\u636e\u8fd9\u4e9b\u5206\u6570\u63a8\u5bfc\u67e5\u8be2\u6ee1\u8db3\u7684\u540e\u9a8c\u6982\u7387\u3002", "result": "\u5728\u591a\u4e2a\u9aa8\u5e72LLM\u3001\u4e0d\u540c\u5916\u90e8\u77e5\u8bc6\u8bbf\u95ee\u7ea7\u522b\u548c\u5404\u79cd\u903b\u8f91\u7ea6\u675f\u4e0b\u8bc4\u4f30\uff0cOrLog\u663e\u8457\u63d0\u5347\u4e86top-rank\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6790\u53d6\u67e5\u8be2\u4e0a\u8868\u73b0\u66f4\u597d\u3002\u6548\u7387\u65b9\u9762\uff0c\u6bcf\u4e2a\u67e5\u8be2-\u5b9e\u4f53\u5bf9\u7684\u5e73\u5747token\u6570\u51cf\u5c11\u4e86\u7ea690%\u3002", "conclusion": "\u65e0\u751f\u6210\u7684\u8c13\u8bcd\u53ef\u80fd\u6027\u4f30\u8ba1\u7ed3\u5408\u6982\u7387\u63a8\u7406\uff0c\u80fd\u591f\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u68c0\u7d22\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6574\u4f53\u63a8\u7406\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684token\uff0c\u4e3a\u590d\u6742\u4fe1\u606f\u9700\u6c42\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}

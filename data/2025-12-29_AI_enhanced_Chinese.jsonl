{"id": "2512.21526", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21526", "abs": "https://arxiv.org/abs/2512.21526", "authors": ["Shanglin Yang", "Zhan Shi"], "title": "Selective LLM-Guided Regularization for Enhancing Recommendation Models", "comment": null, "summary": "Large language models provide rich semantic priors and strong reasoning capabilities, making them promising auxiliary signals for recommendation. However, prevailing approaches either deploy LLMs as standalone recommender or apply global knowledge distillation, both of which suffer from inherent drawbacks. Standalone LLM recommender are costly, biased, and unreliable across large regions of the user item space, while global distillation forces the downstream model to imitate LLM predictions even when such guidance is inaccurate. Meanwhile, recent studies show that LLMs excel particularly in re-ranking and challenging scenarios, rather than uniformly across all contexts.We introduce Selective LLM Guided Regularization, a model-agnostic and computation efficient framework that activates LLM based pairwise ranking supervision only when a trainable gating mechanism informing by user history length, item popularity, and model uncertainty predicts the LLM to be reliable. All LLM scoring is performed offline, transferring knowledge without increasing inference cost. Experiments across multiple datasets show that this selective strategy consistently improves overall accuracy and yields substantial gains in cold start and long tail regimes, outperforming global distillation baselines.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027LLM\u5f15\u5bfc\u6b63\u5219\u5316\u6846\u67b6\uff0c\u4ec5\u5728LLM\u53ef\u9760\u65f6\u6fc0\u6d3b\u5176\u6210\u5bf9\u6392\u5e8f\u76d1\u7763\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u5728\u51b7\u542f\u52a8\u548c\u957f\u5c3e\u573a\u666f\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06LLM\u4f5c\u4e3a\u72ec\u7acb\u63a8\u8350\u5668\uff08\u6210\u672c\u9ad8\u3001\u6709\u504f\u89c1\u3001\u4e0d\u53ef\u9760\uff09\uff0c\u8981\u4e48\u8fdb\u884c\u5168\u5c40\u77e5\u8bc6\u84b8\u998f\uff08\u5f3a\u5236\u4e0b\u6e38\u6a21\u578b\u6a21\u4effLLM\u9884\u6d4b\uff0c\u5373\u4f7f\u6307\u5bfc\u4e0d\u51c6\u786e\uff09\u3002LLM\u5728\u91cd\u6392\u5e8f\u548c\u6311\u6218\u6027\u573a\u666f\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e76\u975e\u5728\u6240\u6709\u4e0a\u4e0b\u6587\u90fd\u4e00\u81f4\u53ef\u9760\u3002", "method": "\u9009\u62e9\u6027LLM\u5f15\u5bfc\u6b63\u5219\u5316\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u95e8\u63a7\u673a\u5236\uff08\u57fa\u4e8e\u7528\u6237\u5386\u53f2\u957f\u5ea6\u3001\u7269\u54c1\u6d41\u884c\u5ea6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff09\u9884\u6d4bLLM\u4f55\u65f6\u53ef\u9760\uff1b2\uff09\u4ec5\u5728LLM\u53ef\u9760\u65f6\u6fc0\u6d3b\u57fa\u4e8eLLM\u7684\u6210\u5bf9\u6392\u5e8f\u76d1\u7763\uff1b3\uff09\u6240\u6709LLM\u8bc4\u5206\u79bb\u7ebf\u5b8c\u6210\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u9009\u62e9\u6027\u7b56\u7565\u6301\u7eed\u63d0\u5347\u6574\u4f53\u51c6\u786e\u6027\uff0c\u5728\u51b7\u542f\u52a8\u548c\u957f\u5c3e\u573a\u666f\u5e26\u6765\u663e\u8457\u589e\u76ca\uff0c\u4f18\u4e8e\u5168\u5c40\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u9009\u62e9\u6027\u6fc0\u6d3bLLM\u76d1\u7763\u6bd4\u5168\u5c40\u84b8\u998f\u66f4\u6709\u6548\uff0c\u80fd\u591f\u667a\u80fd\u5229\u7528LLM\u7684\u4f18\u52bf\uff08\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\uff09\uff0c\u540c\u65f6\u907f\u514d\u5176\u4e0d\u53ef\u9760\u65f6\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb"}}
{"id": "2512.21543", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.21543", "abs": "https://arxiv.org/abs/2512.21543", "authors": ["Yuzhen Lin", "Hongyi Chen", "Xuanjing Chen", "Shaowen Wang", "Ivonne Xu", "Dongming Jiang"], "title": "CEMG: Collaborative-Enhanced Multimodal Generative Recommendation", "comment": null, "summary": "Generative recommendation models often struggle with two key challenges: (1) the superficial integration of collaborative signals, and (2) the decoupled fusion of multimodal features. These limitations hinder the creation of a truly holistic item representation. To overcome this, we propose CEMG, a novel Collaborative-Enhaned Multimodal Generative Recommendation framework. Our approach features a Multimodal Fusion Layer that dynamically integrates visual and textual features under the guidance of collaborative signals. Subsequently, a Unified Modality Tokenization stage employs a Residual Quantization VAE (RQ-VAE) to convert this fused representation into discrete semantic codes. Finally, in the End-to-End Generative Recommendation stage, a large language model is fine-tuned to autoregressively generate these item codes. Extensive experiments demonstrate that CEMG significantly outperforms state-of-the-art baselines.", "AI": {"tldr": "CEMG\u6846\u67b6\u901a\u8fc7\u534f\u4f5c\u4fe1\u53f7\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u3001\u7edf\u4e00\u6a21\u6001\u6807\u8bb0\u5316\u548c\u7aef\u5230\u7aef\u751f\u6210\u63a8\u8350\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u63a8\u8350\u4e2d\u534f\u4f5c\u4fe1\u53f7\u6d45\u5c42\u6574\u5408\u548c\u591a\u6a21\u6001\u7279\u5f81\u89e3\u8026\u878d\u5408\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u63a8\u8350\u6a21\u578b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u534f\u4f5c\u4fe1\u53f7\u7684\u6d45\u5c42\u6574\u5408\uff0c2) \u591a\u6a21\u6001\u7279\u5f81\u7684\u89e3\u8026\u878d\u5408\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u521b\u5efa\u771f\u6b63\u5168\u9762\u7684\u7269\u54c1\u8868\u793a\u3002", "method": "\u63d0\u51faCEMG\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u591a\u6a21\u6001\u878d\u5408\u5c42\uff0c\u5728\u534f\u4f5c\u4fe1\u53f7\u5f15\u5bfc\u4e0b\u52a8\u6001\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff1b2) \u7edf\u4e00\u6a21\u6001\u6807\u8bb0\u5316\u9636\u6bb5\uff0c\u4f7f\u7528\u6b8b\u5dee\u91cf\u5316VAE\u5c06\u878d\u5408\u8868\u793a\u8f6c\u6362\u4e3a\u79bb\u6563\u8bed\u4e49\u4ee3\u7801\uff1b3) \u7aef\u5230\u7aef\u751f\u6210\u63a8\u8350\u9636\u6bb5\uff0c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u56de\u5f52\u751f\u6210\u7269\u54c1\u4ee3\u7801\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCEMG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CEMG\u901a\u8fc7\u534f\u4f5c\u589e\u5f3a\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u751f\u6210\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u63a8\u8350\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u7269\u54c1\u8868\u793a\u548c\u66f4\u597d\u7684\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2512.21595", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21595", "abs": "https://arxiv.org/abs/2512.21595", "authors": ["Yinfu Feng", "Yanjing Wu", "Rong Xiao", "Xiaoyi Zen"], "title": "LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model", "comment": null, "summary": "Item-to-Item (I2I) recommendation models are widely used in real-world systems due to their scalability, real-time capabilities, and high recommendation quality. Research to enhance I2I performance focuses on two directions: 1) model-centric approaches, which adopt deeper architectures but risk increased computational costs and deployment complexity, and 2) data-centric methods, which refine training data without altering models, offering cost-effectiveness but struggling with data sparsity and noise. To address these challenges, we propose LLM-I2I, a data-centric framework leveraging Large Language Models (LLMs) to mitigate data quality issues. LLM-I2I includes (1) an LLM-based generator that synthesizes user-item interactions for long-tail items, alleviating data sparsity, and (2) an LLM-based discriminator that filters noisy interactions from real and synthetic data. The refined data is then fused to train I2I models. Evaluated on industry (AEDS) and academic (ARD) datasets, LLM-I2I consistently improves recommendation accuracy, particularly for long-tail items. Deployed on a large-scale cross-border e-commerce platform, it boosts recall number (RN) by 6.02% and gross merchandise value (GMV) by 1.22% over existing I2I models. This work highlights the potential of LLMs in enhancing data-centric recommendation systems without modifying model architectures.", "AI": {"tldr": "LLM-I2I\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3I2I\u63a8\u8350\u7cfb\u7edf\u4e2d\u6570\u636e\u7a00\u758f\u548c\u566a\u58f0\u95ee\u9898\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u4ea4\u4e92\u548c\u8fc7\u6ee4\u566a\u58f0\u6570\u636e\u63d0\u5347\u63a8\u8350\u6548\u679c", "motivation": "\u73b0\u6709I2I\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u65b9\u5411\u7684\u6311\u6218\uff1a\u6a21\u578b\u4e2d\u5fc3\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u90e8\u7f72\u590d\u6742\uff0c\u6570\u636e\u4e2d\u5fc3\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u758f\u548c\u566a\u58f0\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6210\u672c\u6548\u76ca\u53c8\u80fd\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u65b9\u6848", "method": "\u63d0\u51faLLM-I2I\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eLLM\u7684\u751f\u6210\u5668\u4e3a\u957f\u5c3e\u7269\u54c1\u5408\u6210\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\uff0c\u7f13\u89e3\u6570\u636e\u7a00\u758f\uff1b2\uff09\u57fa\u4e8eLLM\u7684\u5224\u522b\u5668\u8fc7\u6ee4\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u4e2d\u7684\u566a\u58f0\u4ea4\u4e92\uff1b3\uff09\u878d\u5408\u7cbe\u70bc\u540e\u7684\u6570\u636e\u8bad\u7ec3I2I\u6a21\u578b", "result": "\u5728\u884c\u4e1a\uff08AEDS\uff09\u548c\u5b66\u672f\uff08ARD\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLLM-I2I\u6301\u7eed\u63d0\u5347\u63a8\u8350\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5bf9\u957f\u5c3e\u7269\u54c1\u3002\u5728\u5927\u578b\u8de8\u5883\u7535\u5546\u5e73\u53f0\u90e8\u7f72\u540e\uff0c\u53ec\u56de\u6570\u63d0\u53476.02%\uff0c\u5546\u54c1\u4ea4\u6613\u603b\u989d\u63d0\u53471.22%", "conclusion": "LLM-I2I\u5c55\u793a\u4e86\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528LLM\u589e\u5f3a\u6570\u636e\u4e2d\u5fc3\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u4e3aI2I\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u8d28\u91cf\u63d0\u5347\u65b9\u6848"}}
{"id": "2512.21799", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.21799", "abs": "https://arxiv.org/abs/2512.21799", "authors": ["Hung-Nghiep Tran", "Atsuhiro Takasu"], "title": "KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering", "comment": "Extracted and extended from the first author's PhD thesis titled \"Multi-Relational Embedding for Knowledge Graph Representation and Analysis\"", "summary": "In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.", "AI": {"tldr": "KG20C\u548cKG20C-QA\u662f\u4e24\u4e2a\u7528\u4e8e\u5b66\u672f\u6570\u636e\u95ee\u7b54\u7814\u7a76\u7684\u7cbe\u9009\u6570\u636e\u96c6\uff0c\u524d\u8005\u662f\u57fa\u4e8e\u5fae\u8f6f\u5b66\u672f\u56fe\u8c31\u6784\u5efa\u7684\u9ad8\u8d28\u91cf\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\uff0c\u540e\u8005\u662f\u57fa\u4e8eKG20C\u6784\u5efa\u7684\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u5b66\u672f\u9886\u57df\u7684\u95ee\u7b54\u3001\u63a8\u7406\u548c\u77e5\u8bc6\u9a71\u52a8\u5e94\u7528\u7814\u7a76\u63d0\u4f9b\u53ef\u91cd\u7528\u3001\u53ef\u6269\u5c55\u7684\u8d44\u6e90\uff0c\u89e3\u51b3\u73b0\u6709\u5b66\u672f\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u548c\u6807\u51c6\u5316\u95ee\u7b54\u57fa\u51c6\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u4f1a\u8bae\u9009\u62e9\u3001\u57fa\u4e8e\u8d28\u91cf\u7684\u8fc7\u6ee4\u548c\u6a21\u5f0f\u5b9a\u4e49\uff0c\u4ece\u5fae\u8f6f\u5b66\u672f\u56fe\u8c31\u6784\u5efaKG20C\u77e5\u8bc6\u56fe\u8c31\uff1b\u7136\u540e\u5b9a\u4e49\u4e00\u7ec4\u95ee\u7b54\u6a21\u677f\uff0c\u5c06\u56fe\u8c31\u4e09\u5143\u7ec4\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u5bf9\uff0c\u6784\u5efaKG20C-QA\u6570\u636e\u96c6\u3002", "result": "\u521b\u5efa\u4e86\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1aKG20C\uff08\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\uff09\u548cKG20C-QA\uff08\u95ee\u7b54\u57fa\u51c6\uff09\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u5728KG20C-QA\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u5173\u7cfb\u7c7b\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u6b63\u5f0f\u53d1\u5e03\u8fd9\u4e9b\u7ecf\u8fc7\u5145\u5206\u6587\u6863\u8bb0\u5f55\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u8d21\u732e\u4e86\u53ef\u91cd\u7528\u7684\u8d44\u6e90\uff0c\u652f\u6301\u672a\u6765\u5728\u5b66\u672f\u9886\u57df\u7684\u95ee\u7b54\u3001\u63a8\u7406\u548c\u77e5\u8bc6\u9a71\u52a8\u5e94\u7528\u7814\u7a76\u3002"}}
{"id": "2512.21863", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.21863", "abs": "https://arxiv.org/abs/2512.21863", "authors": ["Huatuan Sun", "Yunshan Ma", "Changguang Wu", "Yanxin Zhang", "Pengfei Wang", "Xiaoyu Du"], "title": "Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion", "comment": "10 pages, 4 figures", "summary": "Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u51bb\u7ed3\u5927\u578b\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u89c6\u9891\u63a8\u8350\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u53cc\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u5c42\u4e2d\u95f4\u8868\u793a\u4e0eID\u5d4c\u5165\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u51bb\u7ed3\u5927\u578b\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u89c6\u9891\u63a8\u8350\u4e2d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u901a\u5e38\u88ab\u7528\u4f5c\u56fa\u5b9a\u7684\u9ed1\u76d2\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u6ca1\u6709\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u7684\u8868\u793a\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u53cc\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u7cfb\u7edf\u7814\u7a76\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\u7ef4\u5ea6\uff1a1)\u4e0eID\u5d4c\u5165\u7684\u96c6\u6210\u7b56\u7565\uff08\u66ff\u6362vs\u878d\u5408\uff09\uff1b2)\u7279\u5f81\u63d0\u53d6\u8303\u5f0f\uff08LVLM\u751f\u6210\u7684\u6807\u9898vs\u4e2d\u95f4\u89e3\u7801\u5668\u9690\u85cf\u72b6\u6001\uff09\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u539f\u5219\uff1a1)\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u6807\u9898\u7684\u8868\u793a\uff1b2)ID\u5d4c\u5165\u6355\u83b7\u4e0d\u53ef\u66ff\u4ee3\u7684\u534f\u540c\u4fe1\u53f7\uff0c\u878d\u5408\u4e25\u683c\u4f18\u4e8e\u66ff\u6362\uff1b3)\u4e2d\u95f4\u89e3\u7801\u5668\u7279\u5f81\u5728\u4e0d\u540c\u5c42\u95f4\u6709\u6548\u6027\u5dee\u5f02\u663e\u8457\u3002DFF\u6846\u67b6\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u5fae\u89c6\u9891\u63a8\u8350\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DFF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u5730\u878d\u5408\u51bb\u7ed3LVLMs\u7684\u591a\u5c42\u8868\u793a\u4e0e\u9879\u76eeID\u5d4c\u5165\uff0c\u4e3a\u5c06\u73b0\u6210\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u5fae\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002"}}

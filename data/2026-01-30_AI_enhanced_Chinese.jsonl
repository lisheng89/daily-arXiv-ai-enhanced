{"id": "2601.21105", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.21105", "abs": "https://arxiv.org/abs/2601.21105", "authors": ["Joyce Zhou", "Weijie Zhou", "Doug Turnbull", "Thorsten Joachims"], "title": "SteerEval: A Framework for Evaluating Steerability with Natural Language Profiles for Recommendation", "comment": "10 pages, 2 figures, 8 tables. Pre-print", "summary": "Natural-language user profiles have recently attracted attention not only for improved interpretability, but also for their potential to make recommender systems more steerable. By enabling direct editing, natural-language profiles allow users to explicitly articulate preferences that may be difficult to infer from past behavior. However, it remains unclear whether current natural-language-based recommendation methods can follow such steering commands. While existing steerability evaluations have shown some success for well-recognized item attributes (e.g., movie genres), we argue that these benchmarks fail to capture the richer forms of user control that motivate steerable recommendations. To address this gap, we introduce SteerEval, an evaluation framework designed to measure more nuanced and diverse forms of steerability by using interventions that range from genres to content-warning for movies. We assess the steerability of a family of pretrained natural-language recommenders, examine the potential and limitations of steering on relatively niche topics, and compare how different profile and recommendation interventions impact steering effectiveness. Finally, we offer practical design suggestions informed by our findings and discuss future steps in steerable recommender design.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SteerEval\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u81ea\u7136\u8bed\u8a00\u63a8\u8350\u7cfb\u7edf\u5bf9\u7528\u6237\u5f15\u5bfc\u6307\u4ee4\u7684\u54cd\u5e94\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u5c5e\u6027\u7684\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u66f4\u4e30\u5bcc\u7684\u5f15\u5bfc\u5f62\u5f0f\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u7528\u6237\u6863\u6848\u867d\u7136\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5f15\u5bfc\u6027\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u660e\u663e\u5c5e\u6027\uff08\u5982\u7535\u5f71\u7c7b\u578b\uff09\uff0c\u672a\u80fd\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u7528\u6237\u63a7\u5236\u5f62\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u63a8\u8350\u7cfb\u7edf\u771f\u6b63\u5b9e\u73b0\u7528\u6237\u5f15\u5bfc\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86SteerEval\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u4ece\u7c7b\u578b\u5230\u5185\u5bb9\u8b66\u544a\u7b49\u591a\u79cd\u5e72\u9884\u63aa\u65bd\u6765\u6d4b\u91cf\u66f4\u7ec6\u81f4\u591a\u6837\u7684\u5f15\u5bfc\u80fd\u529b\uff0c\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u9884\u8bad\u7ec3\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u8350\u5668\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6863\u6848\u548c\u63a8\u8350\u5e72\u9884\u5bf9\u5f15\u5bfc\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u8bc4\u4f30\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u8350\u5668\u7684\u5f15\u5bfc\u80fd\u529b\uff0c\u5206\u6790\u4e86\u5728\u76f8\u5bf9\u5c0f\u4f17\u4e3b\u9898\u4e0a\u5f15\u5bfc\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5e72\u9884\u63aa\u65bd\u5bf9\u5f15\u5bfc\u6548\u679c\u7684\u5f71\u54cd\uff0c\u4e3a\u53ef\u5f15\u5bfc\u63a8\u8350\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5e76\u8ba8\u8bba\u4e86\u53ef\u5f15\u5bfc\u63a8\u8350\u5668\u8bbe\u8ba1\u7684\u672a\u6765\u65b9\u5411\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u771f\u6b63\u5b9e\u73b0\u7528\u6237\u5bf9\u63a8\u8350\u7cfb\u7edf\u7684\u63a7\u5236\u3002"}}
{"id": "2601.21162", "categories": ["cs.IR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.21162", "abs": "https://arxiv.org/abs/2601.21162", "authors": ["Jiate Liu", "Zebin Chen", "Shaobo Qiao", "Mingchen Ju", "Danting Zhang", "Bocheng Han", "Shuyue Yu", "Xin Shu", "Jingling Wu", "Dong Wen", "Xin Cao", "Guanfeng Liu", "Zhengyi Yang"], "title": "A2RAG: Adaptive Agentic Graph Retrieval for Cost-Aware and Reliable Reasoning", "comment": null, "summary": "Graph Retrieval-Augmented Generation (Graph-RAG) enhances multihop question answering by organizing corpora into knowledge graphs and routing evidence through relational structure. However, practical deployments face two persistent bottlenecks: (i) mixed-difficulty workloads where one-size-fits-all retrieval either wastes cost on easy queries or fails on hard multihop cases, and (ii) extraction loss, where graph abstraction omits fine-grained qualifiers that remain only in source text. We present A2RAG, an adaptive-and-agentic GraphRAG framework for cost-aware and reliable reasoning. A2RAG couples an adaptive controller that verifies evidence sufficiency and triggers targeted refinement only when necessary, with an agentic retriever that progressively escalates retrieval effort and maps graph signals back to provenance text to remain robust under extraction loss and incomplete graphs. Experiments on HotpotQA and 2WikiMultiHopQA demonstrate that A2RAG achieves +9.9/+11.8 absolute gains in Recall@2, while cutting token consumption and end-to-end latency by about 50% relative to iterative multihop baselines.", "AI": {"tldr": "A2RAG\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u548c\u667a\u80fd\u7684\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7b56\u7565\u548c\u7ed3\u5408\u56fe\u7ed3\u6784\u4e0e\u6587\u672c\u8bc1\u636e\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u591a\u8df3\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edfGraph-RAG\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a1) \u6df7\u5408\u96be\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u56fa\u5b9a\u68c0\u7d22\u7b56\u7565\u8981\u4e48\u5728\u7b80\u5355\u67e5\u8be2\u4e0a\u6d6a\u8d39\u6210\u672c\uff0c\u8981\u4e48\u65e0\u6cd5\u5904\u7406\u56f0\u96be\u7684\u591a\u8df3\u95ee\u9898\uff1b2) \u63d0\u53d6\u635f\u5931\u95ee\u9898\uff0c\u56fe\u62bd\u8c61\u4f1a\u4e22\u5931\u6e90\u6587\u672c\u4e2d\u7684\u7ec6\u7c92\u5ea6\u9650\u5b9a\u4fe1\u606f\u3002", "method": "A2RAG\u7ed3\u5408\u81ea\u9002\u5e94\u63a7\u5236\u5668\u548c\u667a\u80fd\u68c0\u7d22\u5668\uff1a\u63a7\u5236\u5668\u9a8c\u8bc1\u8bc1\u636e\u5145\u5206\u6027\u5e76\u4ec5\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u9488\u5bf9\u6027\u7ec6\u5316\uff1b\u68c0\u7d22\u5668\u9010\u6b65\u63d0\u5347\u68c0\u7d22\u529b\u5ea6\uff0c\u5e76\u5c06\u56fe\u4fe1\u53f7\u6620\u5c04\u56de\u539f\u59cb\u6587\u672c\uff0c\u4ee5\u5e94\u5bf9\u63d0\u53d6\u635f\u5931\u548c\u4e0d\u5b8c\u6574\u56fe\u7ed3\u6784\u3002", "result": "\u5728HotpotQA\u548c2WikiMultiHopQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cA2RAG\u5728Recall@2\u6307\u6807\u4e0a\u5206\u522b\u83b7\u5f97+9.9\u548c+11.8\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u540c\u65f6\u5c06\u4ee4\u724c\u6d88\u8017\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u7ea650%\u3002", "conclusion": "A2RAG\u901a\u8fc7\u81ea\u9002\u5e94\u63a7\u5236\u548c\u667a\u80fd\u68c0\u7d22\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86Graph-RAG\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6210\u672c\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u8df3\u95ee\u7b54\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21611", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21611", "abs": "https://arxiv.org/abs/2601.21611", "authors": ["Baopu Qiu", "Hao Chen", "Yuanrong Wu", "Changtong Zan", "Chao Wei", "Weiru Zhang", "Xiaoyi Zeng"], "title": "Thinking Broad, Acting Fast: Latent Reasoning Distillation from Multi-Perspective Chain-of-Thought for E-Commerce Relevance", "comment": "12 pages, 6 figures, Accepted by WWW2026 industry track", "summary": "Effective relevance modeling is crucial for e-commerce search, as it aligns search results with user intent and enhances customer experience. Recent work has leveraged large language models (LLMs) to address the limitations of traditional relevance models, especially for long-tail and ambiguous queries. By incorporating Chain-of-Thought (CoT) reasoning, these approaches improve both accuracy and interpretability through multi-step reasoning. However, two key limitations remain: (1) most existing approaches rely on single-perspective CoT reasoning, which fails to capture the multifaceted nature of e-commerce relevance (e.g., user intent vs. attribute-level matching vs. business-specific rules); and (2) although CoT-enhanced LLM's offer rich reasoning capabilities, their high inference latency necessitates knowledge distillation for real-time deployment, yet current distillation methods discard the CoT rationale structure at inference, using it as a transient auxiliary signal and forfeiting its reasoning utility. To address these challenges, we propose a novel framework that better exploits CoT semantics throughout the optimization pipeline. Specifically, the teacher model leverages Multi-Perspective CoT (MPCoT) to generate diverse rationales and combines Supervised Fine-Tuning (SFT) with Direct Preference Optimization (DPO) to construct a more robust reasoner. For distillation, we introduce Latent Reasoning Knowledge Distillation (LRKD), which endows a student model with a lightweight inference-time latent reasoning extractor, allowing efficient and low-latency internalization of the LLM's sophisticated reasoning capabilities. Evaluated in offline experiments and online A/B tests on an e-commerce search advertising platform serving tens of millions of users daily, our method delivers significant offline gains, showing clear benefits in both commercial performance and user experience.", "AI": {"tldr": "\u63d0\u51faMPCoT+LRKD\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u6f5c\u5728\u63a8\u7406\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u5347\u7535\u5546\u641c\u7d22\u76f8\u5173\u6027\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027", "motivation": "\u73b0\u6709LLM\u76f8\u5173\u6027\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u5355\u89c6\u89d2\u601d\u7ef4\u94fe\u63a8\u7406\u65e0\u6cd5\u6355\u6349\u7535\u5546\u76f8\u5173\u6027\u7684\u591a\u7ef4\u5ea6\u7279\u6027\uff1b2) \u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e22\u5f03\u4e86\u63a8\u7406\u7ed3\u6784\uff0c\u53ea\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\uff0c\u4e27\u5931\u4e86\u63a8\u7406\u6548\u7528", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u601d\u7ef4\u94fe(MPCoT)\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\uff0c\u7ed3\u5408SFT\u548cDPO\u6784\u5efa\u66f4\u5f3a\u63a8\u7406\u5668\uff1b\u5f15\u5165\u6f5c\u5728\u63a8\u7406\u77e5\u8bc6\u84b8\u998f(LRKD)\uff0c\u4e3a\u5b66\u751f\u6a21\u578b\u914d\u5907\u8f7b\u91cf\u7ea7\u63a8\u7406\u63d0\u53d6\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u63a8\u7406", "result": "\u5728\u670d\u52a1\u6570\u5343\u4e07\u7528\u6237\u7684\u7535\u5546\u641c\u7d22\u5e7f\u544a\u5e73\u53f0\u4e0a\u8fdb\u884c\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\uff0c\u65b9\u6cd5\u5728\u5546\u4e1a\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u5747\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf", "conclusion": "MPCoT+LRKD\u6846\u67b6\u901a\u8fc7\u5145\u5206\u5229\u7528\u601d\u7ef4\u94fe\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86\u7535\u5546\u641c\u7d22\u76f8\u5173\u6027\u5efa\u6a21\u4e2d\u7684\u591a\u7ef4\u5ea6\u63a8\u7406\u548c\u5b9e\u65f6\u90e8\u7f72\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u53cc\u91cd\u63d0\u5347"}}
{"id": "2601.21759", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21759", "abs": "https://arxiv.org/abs/2601.21759", "authors": ["Meet Doshi", "Vishwajeet Kumar", "Yulong Li", "Jaydeep Sen"], "title": "Influence Guided Sampling for Domain Adaptation of Text Retrievers", "comment": null, "summary": "General-purpose open-domain dense retrieval systems are usually trained with a large, eclectic mix of corpora and search tasks. How should these diverse corpora and tasks be sampled for training? Conventional approaches sample them uniformly, proportional to their instance population sizes, or depend on human-level expert supervision. It is well known that the training data sampling strategy can greatly impact model performance. However, how to find the optimal strategy has not been adequately studied in the context of embedding models. We propose Inf-DDS, a novel reinforcement learning driven sampling framework that adaptively reweighs training datasets guided by influence-based reward signals and is much more lightweight with respect to GPU consumption. Our technique iteratively refines the sampling policy, prioritizing datasets that maximize model performance on a target development set. We evaluate the efficacy of our sampling strategy on a wide range of text retrieval tasks, demonstrating strong improvements in retrieval performance and better adaptation compared to existing gradient-based sampling methods, while also being 1.5x to 4x cheaper in GPU compute. Our sampling strategy achieves a 5.03 absolute NDCG@10 improvement while training a multilingual bge-m3 model and an absolute NDCG@10 improvement of 0.94 while training all-MiniLM-L6-v2, even when starting from expert-assigned weights on a large pool of training datasets.", "AI": {"tldr": "Inf-DDS\uff1a\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u8bad\u7ec3\u6570\u636e\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5f71\u54cd\u529b\u5956\u52b1\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\u6570\u636e\u96c6\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6a21\u578b\u6027\u80fd\u5e76\u964d\u4f4eGPU\u8ba1\u7b97\u6210\u672c", "motivation": "\u901a\u7528\u5f00\u653e\u57df\u7a20\u5bc6\u68c0\u7d22\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u591a\u6837\u5316\u7684\u8bed\u6599\u5e93\u548c\u641c\u7d22\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u5982\u4f55\u6700\u4f18\u5730\u91c7\u6837\u8fd9\u4e9b\u8bad\u7ec3\u6570\u636e\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u4f20\u7edf\u7684\u5747\u5300\u91c7\u6837\u3001\u6309\u5b9e\u4f8b\u6570\u91cf\u6bd4\u4f8b\u91c7\u6837\u6216\u4f9d\u8d56\u4e13\u5bb6\u76d1\u7763\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u8bad\u7ec3\u6570\u636e\u91c7\u6837\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u91cd\u5927\u5f71\u54cd\u3002", "method": "\u63d0\u51faInf-DDS\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f71\u54cd\u529b\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\uff0c\u8fed\u4ee3\u4f18\u5316\u91c7\u6837\u7b56\u7565\uff0c\u4f18\u5148\u9009\u62e9\u80fd\u6700\u5927\u5316\u76ee\u6807\u5f00\u53d1\u96c6\u6027\u80fd\u7684\u6570\u636e\u96c6\u3002\u76f8\u6bd4\u73b0\u6709\u68af\u5ea6\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5GPU\u6d88\u8017\u66f4\u8f7b\u91cf\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cInf-DDS\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff1a\u8bad\u7ec3\u591a\u8bed\u8a00bge-m3\u6a21\u578b\u65f6NDCG@10\u7edd\u5bf9\u63d0\u53475.03\uff0c\u8bad\u7ec3all-MiniLM-L6-v2\u65f6NDCG@10\u7edd\u5bf9\u63d0\u53470.94\u3002\u540c\u65f6GPU\u8ba1\u7b97\u6210\u672c\u964d\u4f4e1.5-4\u500d\uff0c\u5373\u4f7f\u4ece\u4e13\u5bb6\u5206\u914d\u7684\u6743\u91cd\u5f00\u59cb\u8bad\u7ec3\u4e5f\u80fd\u83b7\u5f97\u6539\u8fdb\u3002", "conclusion": "Inf-DDS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u6570\u636e\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b\u7684\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5927\u89c4\u6a21\u6df7\u5408\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.21770", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21770", "abs": "https://arxiv.org/abs/2601.21770", "authors": ["Kun Zhang", "Jingming Zhang", "Wei Cheng", "Yansong Cheng", "Jiaqi Zhang", "Hao Lu", "Xu Zhang", "Haixiang Gan", "Jiangxia Cao", "Tenglong Wang", "Ximing Zhang", "Boyang Xia", "Kuo Cai", "Shiyao Wang", "Hongjian Dou", "Jinkai Yu", "Mingxing Wen", "Qiang Luo", "Dongxu Liang", "Chenyi Lei", "Jun Wang", "Runan Liu", "Zhaojie Liu", "Ruiming Tang", "Tingting Gao", "Shaoguo Liu", "Yuqing Ding", "Hui Kong", "Han Li", "Guorui Zhou", "Wenwu Ou", "Kun Gai"], "title": "OneMall: One Model, More Scenarios -- End-to-End Generative Recommender Family at Kuaishou E-Commerce", "comment": "Work in progress", "summary": "In the wave of generative recommendation, we present OneMall, an end-to-end generative recommendation framework tailored for e-commerce services at Kuaishou. Our OneMall systematically unifies the e-commerce's multiple item distribution scenarios, such as Product-card, short-video and live-streaming. Specifically, it comprises three key components, aligning the entire model training pipeline to the LLM's pre-training/post-training: (1) E-commerce Semantic Tokenizer: we provide a tokenizer solution that captures both real-world semantics and business-specific item relations across different scenarios; (2) Transformer-based Architecture: we largely utilize Transformer as our model backbone, e.g., employing Query-Former for long sequence compression, Cross-Attention for multi-behavior sequence fusion, and Sparse MoE for scalable auto-regressive generation; (3) Reinforcement Learning Pipeline: we further connect retrieval and ranking models via RL, enabling the ranking model to serve as a reward signal for end-to-end policy retrieval model optimization. Extensive experiments demonstrate that OneMall achieves consistent improvements across all e-commerce scenarios: +13.01\\% GMV in product-card, +15.32\\% Orders in Short-Video, and +2.78\\% Orders in Live-Streaming. OneMall has been deployed, serving over 400 million daily active users at Kuaishou.", "AI": {"tldr": "\u5feb\u624b\u7535\u5546OneMall\uff1a\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u7edf\u4e00\u591a\u573a\u666f\uff08\u5546\u54c1\u5361\u7247\u3001\u77ed\u89c6\u9891\u3001\u76f4\u64ad\uff09\u63a8\u8350\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u8bcd\u5668\u3001Transformer\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u5404\u573a\u666f\u4e1a\u52a1\u6307\u6807\u3002", "motivation": "\u7535\u5546\u5e73\u53f0\u5b58\u5728\u591a\u79cd\u5546\u54c1\u5206\u53d1\u573a\u666f\uff08\u5546\u54c1\u5361\u7247\u3001\u77ed\u89c6\u9891\u3001\u76f4\u64ad\uff09\uff0c\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u7edf\u4e00\u5904\u7406\u3002\u9700\u8981\u6784\u5efa\u7aef\u5230\u7aef\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\u6765\u7edf\u4e00\u8fd9\u4e9b\u573a\u666f\uff0c\u63d0\u5347\u63a8\u8350\u6548\u679c\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "1. \u7535\u5546\u8bed\u4e49\u5206\u8bcd\u5668\uff1a\u6355\u83b7\u771f\u5b9e\u4e16\u754c\u8bed\u4e49\u548c\u8de8\u573a\u666f\u4e1a\u52a1\u7279\u5b9a\u5546\u54c1\u5173\u7cfb\uff1b2. Transformer\u67b6\u6784\uff1a\u4f7f\u7528Query-Former\u538b\u7f29\u957f\u5e8f\u5217\u3001Cross-Attention\u878d\u5408\u591a\u884c\u4e3a\u5e8f\u5217\u3001Sparse MoE\u5b9e\u73b0\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u751f\u6210\uff1b3. \u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\uff1a\u8fde\u63a5\u68c0\u7d22\u548c\u6392\u5e8f\u6a21\u578b\uff0c\u7528\u6392\u5e8f\u6a21\u578b\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u7aef\u5230\u7aef\u7b56\u7565\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u5728\u6240\u6709\u7535\u5546\u573a\u666f\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1a\u5546\u54c1\u5361\u7247GMV\u63d0\u534713.01%\uff0c\u77ed\u89c6\u9891\u8ba2\u5355\u63d0\u534715.32%\uff0c\u76f4\u64ad\u8ba2\u5355\u63d0\u53472.78%\u3002\u5df2\u5728\u5feb\u624b\u90e8\u7f72\uff0c\u670d\u52a1\u8d85\u8fc74\u4ebf\u65e5\u6d3b\u7528\u6237\u3002", "conclusion": "OneMall\u6210\u529f\u6784\u5efa\u4e86\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u573a\u666f\u7535\u5546\u63a8\u8350\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e1a\u52a1\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u5728\u7535\u5546\u9886\u57df\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.21805", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21805", "abs": "https://arxiv.org/abs/2601.21805", "authors": ["Yuhan Zhao", "Weixin Chen", "Li Chen", "Weike Pan"], "title": "The Double-Edged Sword of Knowledge Transfer: Diagnosing and Curing Fairness Pathologies in Cross-Domain Recommendation", "comment": "Accepted by WWW'26", "summary": "Cross-domain recommendation (CDR) offers an effective strategy for improving recommendation quality in a target domain by leveraging auxiliary signals from source domains. Nonetheless, emerging evidence shows that CDR can inadvertently heighten group-level unfairness. In this work, we conduct a comprehensive theoretical and empirical analysis to uncover why these fairness issues arise. Specifically, we identify two key challenges: (i) Cross-Domain Disparity Transfer, wherein existing group-level disparities in the source domain are systematically propagated to the target domain; and (ii) Unfairness from Cross-Domain Information Gain, where the benefits derived from cross-domain knowledge are unevenly allocated among distinct groups. To address these two challenges, we propose a Cross-Domain Fairness Augmentation (CDFA) framework composed of two key components. Firstly, it mitigates cross-domain disparity transfer by adaptively integrating unlabeled data to equilibrate the informativeness of training signals across groups. Secondly, it redistributes cross-domain information gains via an information-theoretic approach to ensure equitable benefit allocation across groups. Extensive experiments on multiple datasets and baselines demonstrate that our framework significantly reduces unfairness in CDR without sacrificing overall recommendation performance, while even enhancing it.", "AI": {"tldr": "\u8de8\u57df\u63a8\u8350\u4f1a\u52a0\u5267\u7fa4\u4f53\u4e0d\u516c\u5e73\u6027\uff0c\u672c\u6587\u63d0\u51faCDFA\u6846\u67b6\u89e3\u51b3\u8de8\u57df\u5dee\u5f02\u8f6c\u79fb\u548c\u4fe1\u606f\u589e\u76ca\u4e0d\u516c\u5e73\u5206\u914d\u95ee\u9898", "motivation": "\u8de8\u57df\u63a8\u8350\u867d\u7136\u80fd\u63d0\u5347\u63a8\u8350\u8d28\u91cf\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u4f1a\u65e0\u610f\u4e2d\u52a0\u5267\u7fa4\u4f53\u5c42\u9762\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u8de8\u57df\u516c\u5e73\u589e\u5f3a\uff08CDFA\uff09\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u672a\u6807\u8bb0\u6570\u636e\u5e73\u8861\u8de8\u7ec4\u8bad\u7ec3\u4fe1\u53f7\u4fe1\u606f\u91cf\uff0c\u7f13\u89e3\u8de8\u57df\u5dee\u5f02\u8f6c\u79fb\uff1b2\uff09\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u91cd\u65b0\u5206\u914d\u8de8\u57df\u4fe1\u606f\u589e\u76ca\uff0c\u786e\u4fdd\u516c\u5e73\u5229\u76ca\u5206\u914d", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDFA\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u8de8\u57df\u63a8\u8350\u7684\u4e0d\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6574\u4f53\u63a8\u8350\u6027\u80fd\uff0c\u751a\u81f3\u6709\u6240\u63d0\u5347", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u8de8\u57df\u63a8\u8350\u4e2d\u4e0d\u516c\u5e73\u6027\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5b9e\u73b0\u516c\u5e73\u7684\u8de8\u57df\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840"}}
{"id": "2601.21853", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21853", "abs": "https://arxiv.org/abs/2601.21853", "authors": ["Elias J\u00e4\u00e4saari", "Ville Hyv\u00f6nen", "Teemu Roos"], "title": "LEMUR: Learned Multi-Vector Retrieval", "comment": "17 pages", "summary": "Multi-vector representations generated by late interaction models, such as ColBERT, enable superior retrieval quality compared to single-vector representations in information retrieval applications. In multi-vector retrieval systems, both queries and documents are encoded using one embedding for each token, and similarity between queries and documents is measured by the MaxSim similarity measure. However, the improved recall of multi-vector retrieval comes at the expense of significantly increased latency. This necessitates designing efficient approximate nearest neighbor search (ANNS) algorithms for multi-vector search. In this work, we introduce LEMUR, a simple-yet-efficient framework for multi-vector similarity search. LEMUR consists of two consecutive problem reductions: We first formulate multi-vector similarity search as a supervised learning problem that can be solved using a one-hidden-layer neural network. Second, we reduce inference under this model to single-vector similarity search in its latent space, which enables the use of existing single-vector ANNS methods for speeding up retrieval. In addition to performance evaluation on ColBERTv2 embeddings, we evaluate LEMUR on embeddings generated by modern multi-vector text models and multi-vector visual document retrieval models. LEMUR is an order of magnitude faster than earlier multi-vector similarity search methods.", "AI": {"tldr": "LEMUR\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u5c42\u95ee\u9898\u7b80\u5316\u5c06\u591a\u5411\u91cf\u641c\u7d22\u8f6c\u5316\u4e3a\u5355\u5411\u91cfANN\u641c\u7d22\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u591a\u5411\u91cf\u8868\u793a\uff08\u5982ColBERT\uff09\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u8d28\u91cf\u4f18\u4e8e\u5355\u5411\u91cf\u8868\u793a\uff0c\u4f46\u68c0\u7d22\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7b97\u6cd5\u6765\u89e3\u51b3\u591a\u5411\u91cf\u641c\u7d22\u7684\u6548\u7387\u95ee\u9898\u3002", "method": "LEMUR\u91c7\u7528\u4e24\u5c42\u95ee\u9898\u7b80\u5316\uff1a1\uff09\u5c06\u591a\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u8f6c\u5316\u4e3a\u53ef\u901a\u8fc7\u5355\u9690\u85cf\u5c42\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u7684\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff1b2\uff09\u5c06\u8be5\u6a21\u578b\u7684\u63a8\u7406\u7b80\u5316\u4e3a\u5728\u5176\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5355\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u4ece\u800c\u5229\u7528\u73b0\u6709\u5355\u5411\u91cfANNS\u65b9\u6cd5\u52a0\u901f\u68c0\u7d22\u3002", "result": "LEMUR\u5728ColBERTv2\u5d4c\u5165\u3001\u73b0\u4ee3\u591a\u5411\u91cf\u6587\u672c\u6a21\u578b\u548c\u591a\u5411\u91cf\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u6bd4\u65e9\u671f\u591a\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LEMUR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u5411\u91cf\u641c\u7d22\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u5411\u91cfANN\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u7d22\u8d28\u91cf\u3002"}}
{"id": "2601.21986", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.21986", "abs": "https://arxiv.org/abs/2601.21986", "authors": ["Yu Cui", "Feng Liu", "Zhaoxiang Wang", "Changwang Zhang", "Jun Wang", "Can Wang", "Jiawei Chen"], "title": "SpecTran: Spectral-Aware Transformer-based Adapter for LLM-Enhanced Sequential Recommendation", "comment": null, "summary": "Traditional sequential recommendation (SR) models learn low-dimensional item ID embeddings from user-item interactions, often overlooking textual information such as item titles or descriptions. Recent advances in Large Language Models (LLMs) have inspired a surge of research that encodes item textual information with high-dimensional semantic embeddings, and designs transformation methods to inject such embeddings into SR models. These embedding transformation strategies can be categorized into two types, both of which exhibits notable drawbacks: 1) adapter-based methods suffer from pronounced dimension collapse, concentrating information into a few dominant dimensions; 2) SVD-based methods are rigid and manual, considering only a few principal spectral components while discarding rich information in the remaining spectrum.\n  To address these limitations, we propose SpecTran, a spectral-aware transformer-based adapter that operates in the spectral domain, attending to the full spectrum to select and aggregates informative components. A learnable spectral-position encoding injects singular-value cues as an inductive bias, guiding attention toward salient spectral components and promoting diversity across embedding dimensions. Across four real-world datasets and three SR backbones, it consistently outperforms strong baselines, achieving an average improvement of 9.17%.", "AI": {"tldr": "\u63d0\u51faSpecTran\uff0c\u4e00\u79cd\u57fa\u4e8e\u9891\u8c31\u611f\u77e5\u7684transformer\u9002\u914d\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edfSR\u6a21\u578b\u4e2d\u6587\u672c\u4fe1\u606f\u6ce8\u5165\u7684\u7ef4\u5ea6\u584c\u9677\u548c\u9891\u8c31\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u4ec5\u5b66\u4e60\u4f4e\u7ef4\u9879\u76eeID\u5d4c\u5165\uff0c\u5ffd\u7565\u4e86\u9879\u76ee\u6807\u9898\u6216\u63cf\u8ff0\u7b49\u6587\u672c\u4fe1\u606f\u3002\u73b0\u6709LLM\u6587\u672c\u5d4c\u5165\u6ce8\u5165\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u9002\u914d\u5668\u65b9\u6cd5\u6709\u7ef4\u5ea6\u584c\u9677\u95ee\u9898\uff0cSVD\u65b9\u6cd5\u5219\u4e22\u5f03\u4e86\u5927\u91cf\u9891\u8c31\u4fe1\u606f\u3002", "method": "\u63d0\u51faSpecTran\uff0c\u4e00\u79cd\u5728\u9891\u8c31\u57df\u64cd\u4f5c\u7684\u9891\u8c31\u611f\u77e5transformer\u9002\u914d\u5668\uff0c\u5173\u6ce8\u5b8c\u6574\u9891\u8c31\u4ee5\u9009\u62e9\u548c\u805a\u5408\u4fe1\u606f\u7ec4\u4ef6\u3002\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u9891\u8c31\u4f4d\u7f6e\u7f16\u7801\u5c06\u5947\u5f02\u503c\u7ebf\u7d22\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u5f15\u5bfc\u6ce8\u610f\u529b\u5173\u6ce8\u663e\u8457\u9891\u8c31\u7ec4\u4ef6\u5e76\u4fc3\u8fdb\u5d4c\u5165\u7ef4\u5ea6\u7684\u591a\u6837\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e09\u4e2aSR\u9aa8\u5e72\u7f51\u7edc\u4e0a\uff0cSpecTran\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u53479.17%\u3002", "conclusion": "SpecTran\u901a\u8fc7\u9891\u8c31\u611f\u77e5\u7684transformer\u9002\u914d\u5668\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5d4c\u5165\u6ce8\u5165\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2601.22008", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.22008", "abs": "https://arxiv.org/abs/2601.22008", "authors": ["Jia-Huei Ju", "Fran\u00e7ois G. Landry", "Eugene Yang", "Suzan Verberne", "Andrew Yates"], "title": "LANCER: LLM Reranking for Nugget Coverage", "comment": "ECIR 2026", "summary": "Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $\u03b1$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.", "AI": {"tldr": "LANCER\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u957f\u683c\u5f0fRAG\u4efb\u52a1\uff0c\u901a\u8fc7\u751f\u6210\u5b50\u95ee\u9898\u5e76\u9884\u6d4b\u6587\u6863\u56de\u7b54\u8fd9\u4e9b\u5b50\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4f18\u5316\u4fe1\u606f\u8986\u76d6\u800c\u975e\u4ec5\u76f8\u5173\u6027\u6392\u5e8f\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u76f8\u5173\u6027\u6392\u5e8f\u4f18\u5316\uff0c\u800c\u957f\u683c\u5f0fRAG\uff08\u5982\u81ea\u52a8\u62a5\u544a\u751f\u6210\uff09\u9700\u8981\u8986\u76d6\u5e7f\u6cdb\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u8981\u6c42\u68c0\u7d22\u63d0\u4f9b\u5168\u9762\u7684\u4fe1\u606f\u8986\u76d6\u800c\u4e0d\u4ec5\u4ec5\u662f\u76f8\u5173\u6027\u3002", "method": "LANCER\u4f7f\u7528LLM\u9884\u6d4b\u4fe1\u606f\u9700\u6c42\u5e94\u56de\u7b54\u7684\u5b50\u95ee\u9898\uff0c\u9884\u6d4b\u54ea\u4e9b\u6587\u6863\u80fd\u56de\u7b54\u8fd9\u4e9b\u5b50\u95ee\u9898\uff0c\u7136\u540e\u91cd\u65b0\u6392\u5e8f\u6587\u6863\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u5757\u8986\u76d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLANCER\u5728\u4fe1\u606f\u5757\u8986\u76d6\u6307\u6807\uff08\u03b1-nDCG\u548c\u4fe1\u606f\u8986\u76d6\u7387\uff09\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5b50\u95ee\u9898\u751f\u6210\u5728\u63d0\u5347\u6027\u80fd\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "LANCER\u901a\u8fc7\u4e13\u6ce8\u4e8e\u4fe1\u606f\u8986\u76d6\u800c\u975e\u4ec5\u76f8\u5173\u6027\u6392\u5e8f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u683c\u5f0fRAG\u4efb\u52a1\u7684\u68c0\u7d22\u8d28\u91cf\uff0c\u5b50\u95ee\u9898\u751f\u6210\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u7ec4\u4ef6\u3002"}}

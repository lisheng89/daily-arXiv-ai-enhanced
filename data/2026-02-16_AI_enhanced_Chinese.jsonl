{"id": "2602.12315", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12315", "abs": "https://arxiv.org/abs/2602.12315", "authors": ["Sunghwan Kim", "Ryang Heo", "Yongsik Seo", "Jinyoung Yeo", "Dongha Lee"], "title": "AgenticShop: Benchmarking Agentic Product Curation for Personalized Web Shopping", "comment": "Accepted at WWW 2026", "summary": "The proliferation of e-commerce has made web shopping platforms key gateways for customers navigating the vast digital marketplace. Yet this rapid expansion has led to a noisy and fragmented information environment, increasing cognitive burden as shoppers explore and purchase products online. With promising potential to alleviate this challenge, agentic systems have garnered growing attention for automating user-side tasks in web shopping. Despite significant advancements, existing benchmarks fail to comprehensively evaluate how well agentic systems can curate products in open-web settings. Specifically, they have limited coverage of shopping scenarios, focusing only on simplified single-platform lookups rather than exploratory search. Moreover, they overlook personalization in evaluation, leaving unclear whether agents can adapt to diverse user preferences in realistic shopping contexts. To address this gap, we present AgenticShop, the first benchmark for evaluating agentic systems on personalized product curation in open-web environment. Crucially, our approach features realistic shopping scenarios, diverse user profiles, and a verifiable, checklist-driven personalization evaluation framework. Through extensive experiments, we demonstrate that current agentic systems remain largely insufficient, emphasizing the need for user-side systems that effectively curate tailored products across the modern web.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgenticShop\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u771f\u5b9e\u8d2d\u7269\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u4fe1\u606f\u73af\u5883\u5608\u6742\u548c\u788e\u7247\u5316\uff0c\u589e\u52a0\u4e86\u7528\u6237\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u867d\u7136\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u7528\u6237\u7aef\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u8fdb\u884c\u4ea7\u54c1\u63a8\u8350\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8d2d\u7269\u573a\u666f\u8986\u76d6\u548c\u4e2a\u6027\u5316\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86AgenticShop\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5177\u6709\u4e09\u4e2a\u5173\u952e\u7279\u5f81\uff1a1\uff09\u771f\u5b9e\u7684\u8d2d\u7269\u573a\u666f\uff1b2\uff09\u591a\u6837\u5316\u7684\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff1b3\uff09\u53ef\u9a8c\u8bc1\u7684\u3001\u57fa\u4e8e\u68c0\u67e5\u8868\u7684\u4e2a\u6027\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u901a\u8fc7\u8fd9\u4e2a\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5f53\u524d\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u5728\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5728\u73b0\u4ee3\u7f51\u7edc\u4e2d\u4e3a\u7528\u6237\u63a8\u8350\u5b9a\u5236\u5316\u4ea7\u54c1\u7684\u7528\u6237\u7aef\u7cfb\u7edf\u3002", "conclusion": "AgenticShop\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u4e2a\u6027\u5316\u4ea7\u54c1\u63a8\u8350\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7528\u6237\u7aef\u8d2d\u7269\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u5411\u3002"}}
{"id": "2602.12354", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12354", "abs": "https://arxiv.org/abs/2602.12354", "authors": ["Lars Hertel", "Gaurav Srivastava", "Syed Ali Naqvi", "Satyam Kumar", "Yue Zhang", "Borja Ocejo", "Benjamin Zelditch", "Adrian Englhardt", "Hailing Cheng", "Andy Hu", "Antonio Alonso", "Daming Li", "Siddharth Dangi", "Chen Zhu", "Mingzhou Zhou", "Wanning Li", "Tao Huang", "Fedor Borisyuk", "Ganesh Parameswaran", "Birjodh Singh Tiwana", "Sriram Sankar", "Qing Lan", "Julie Choi", "Souvik Ghosh"], "title": "An Industrial-Scale Sequential Recommender for LinkedIn Feed Ranking", "comment": null, "summary": "LinkedIn Feed enables professionals worldwide to discover relevant content, build connections, and share knowledge at scale. We present Feed Sequential Recommender (Feed-SR), a transformer-based sequential ranking model for LinkedIn Feed that replaces a DCNv2-based ranker and meets strict production constraints. We detail the modeling choices, training techniques, and serving optimizations that enable deployment at LinkedIn scale. Feed-SR is currently the primary member experience on LinkedIn's Feed and shows significant improvements in member engagement (+2.10% time spent) in online A/B tests compared to the existing production model. We also describe our deployment experience with alternative sequential and LLM-based ranking architectures and why Feed-SR provided the best combination of online metrics and production efficiency.", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684Feed-SR\u5e8f\u5217\u63a8\u8350\u6a21\u578b\uff0c\u66ff\u4ee3\u539f\u6709DCNv2\u6392\u5e8f\u5668\uff0c\u5728\u4e25\u683c\u751f\u4ea7\u7ea6\u675f\u4e0b\u663e\u8457\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6", "motivation": "LinkedIn Feed\u9700\u8981\u4e3a\u5168\u7403\u4e13\u4e1a\u4eba\u58eb\u63d0\u4f9b\u76f8\u5173\u5185\u5bb9\u53d1\u73b0\u3001\u5efa\u7acb\u8054\u7cfb\u548c\u77e5\u8bc6\u5206\u4eab\uff0c\u73b0\u6709DCNv2\u6a21\u578b\u5728\u5e8f\u5217\u63a8\u8350\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u63d0\u5347\u63a8\u8350\u6548\u679c", "method": "\u91c7\u7528Transformer\u67b6\u6784\u7684\u5e8f\u5217\u63a8\u8350\u6a21\u578bFeed-SR\uff0c\u8be6\u7ec6\u8bbe\u8ba1\u4e86\u5efa\u6a21\u9009\u62e9\u3001\u8bad\u7ec3\u6280\u672f\u548c\u670d\u52a1\u4f18\u5316\uff0c\u4ee5\u6ee1\u8db3LinkedIn\u89c4\u6a21\u7684\u751f\u4ea7\u90e8\u7f72\u8981\u6c42", "result": "Feed-SR\u6210\u4e3aLinkedIn Feed\u7684\u4e3b\u8981\u6210\u5458\u4f53\u9a8c\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u76f8\u6bd4\u73b0\u6709\u751f\u4ea7\u6a21\u578b\uff0c\u7528\u6237\u53c2\u4e0e\u5ea6\u663e\u8457\u63d0\u5347\uff08+2.10%\u65f6\u95f4\u82b1\u8d39\uff09", "conclusion": "Feed-SR\u5728\u5728\u7ebf\u6307\u6807\u548c\u751f\u4ea7\u6548\u7387\u65b9\u9762\u63d0\u4f9b\u4e86\u6700\u4f73\u7ec4\u5408\uff0c\u6210\u529f\u90e8\u7f72\u4e3aLinkedIn Feed\u7684\u4e3b\u8981\u63a8\u8350\u7cfb\u7edf\uff0c\u76f8\u6bd4\u5176\u4ed6\u5e8f\u5217\u548cLLM\u67b6\u6784\u5177\u6709\u66f4\u597d\u7684\u5b9e\u7528\u6027"}}
{"id": "2602.12485", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12485", "abs": "https://arxiv.org/abs/2602.12485", "authors": ["Keerthi Gopalakrishnan", "Tianning Dong", "Chia-Yen Ho", "Yokila Arora", "Topojoy Biswas", "Jason Cho", "Sushant Kumar", "Kannan Achan"], "title": "Latent Customer Segmentation and Value-Based Recommendation Leveraging a Two-Stage Model with Missing Labels", "comment": null, "summary": "The success of businesses depends on their ability to convert consumers into loyal customers. A customer's value proposition is a primary determinant in this process, requiring a balance between affordability and long-term brand equity. Broad marketing campaigns can erode perceived brand value and reduce return on investment, while existing economic algorithms often misidentify highly engaged customers as ideal targets, leading to inefficient engagement and conversion outcomes.\n  This work introduces a two-stage multi-model architecture employing Self-Paced Loss to improve customer categorization. The first stage uses a multi-class neural network to distinguish customers influenced by campaigns, organically engaged customers, and low-engagement customers. The second stage applies a binary label correction model to identify true campaign-driven intent using a missing-label framework, refining customer segmentation during training.\n  By separating prompted engagement from organic behavior, the system enables more precise campaign targeting, reduces exposure costs, and improves conversion efficiency. A/B testing demonstrates over 100 basis points improvement in key success metrics, highlighting the effectiveness of intent-aware segmentation for value-driven marketing strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u591a\u6a21\u578b\u67b6\u6784\uff0c\u4f7f\u7528\u81ea\u6b65\u635f\u5931\u6539\u8fdb\u5ba2\u6237\u5206\u7c7b\uff0c\u533a\u5206\u6d3b\u52a8\u5f71\u54cd\u3001\u6709\u673a\u53c2\u4e0e\u548c\u4f4e\u53c2\u4e0e\u5ba2\u6237\uff0c\u63d0\u9ad8\u8425\u9500\u6d3b\u52a8\u7cbe\u51c6\u5ea6\u548c\u8f6c\u5316\u6548\u7387", "motivation": "\u4f20\u7edf\u8425\u9500\u6d3b\u52a8\u4f1a\u4fb5\u8680\u54c1\u724c\u4ef7\u503c\u4e14\u6295\u8d44\u56de\u62a5\u7387\u4f4e\uff0c\u73b0\u6709\u7ecf\u6d4e\u7b97\u6cd5\u5e38\u5c06\u9ad8\u53c2\u4e0e\u5ea6\u5ba2\u6237\u8bef\u5224\u4e3a\u7406\u60f3\u76ee\u6807\uff0c\u5bfc\u81f4\u53c2\u4e0e\u548c\u8f6c\u5316\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u5ba2\u6237\u5206\u7c7b\u65b9\u6cd5", "method": "\u4e24\u9636\u6bb5\u591a\u6a21\u578b\u67b6\u6784\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u591a\u7c7b\u795e\u7ecf\u7f51\u7edc\u533a\u5206\u6d3b\u52a8\u5f71\u54cd\u5ba2\u6237\u3001\u6709\u673a\u53c2\u4e0e\u5ba2\u6237\u548c\u4f4e\u53c2\u4e0e\u5ba2\u6237\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u4e8c\u5143\u6807\u7b7e\u6821\u6b63\u6a21\u578b\uff0c\u5728\u7f3a\u5931\u6807\u7b7e\u6846\u67b6\u4e0b\u8bc6\u522b\u771f\u6b63\u7684\u6d3b\u52a8\u9a71\u52a8\u610f\u56fe\uff0c\u5728\u8bad\u7ec3\u4e2d\u7ec6\u5316\u5ba2\u6237\u7ec6\u5206", "result": "\u901a\u8fc7\u5206\u79bb\u4e3b\u52a8\u53c2\u4e0e\u548c\u6709\u673a\u884c\u4e3a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u6d3b\u52a8\u5b9a\u5411\uff0c\u964d\u4f4e\u66dd\u5149\u6210\u672c\uff0c\u63d0\u9ad8\u8f6c\u5316\u6548\u7387\u3002A/B\u6d4b\u8bd5\u663e\u793a\u5173\u952e\u6210\u529f\u6307\u6807\u63d0\u5347\u8d85\u8fc7100\u4e2a\u57fa\u70b9", "conclusion": "\u610f\u56fe\u611f\u77e5\u7ec6\u5206\u5bf9\u4ef7\u503c\u9a71\u52a8\u8425\u9500\u7b56\u7565\u6709\u6548\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u80fd\u663e\u8457\u6539\u5584\u5ba2\u6237\u5206\u7c7b\u7cbe\u5ea6\u548c\u8425\u9500\u6548\u7387"}}
{"id": "2602.12510", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.12510", "abs": "https://arxiv.org/abs/2602.12510", "authors": ["Ara Yeroyan"], "title": "Visual RAG Toolkit: Scaling Multi-Vector Visual Retrieval with Training-Free Pooling and Multi-Stage Search", "comment": "4 pages, 3 figures. Submitted to SIGIR 2026 Demonstrations Track. Project website: https://github.com/Ara-Yeroyan/visual-rag-toolkit", "summary": "Multi-vector visual retrievers (e.g., ColPali-style late interaction models) deliver strong accuracy, but scale poorly because each page yields thousands of vectors, making indexing and search increasingly expensive. We present Visual RAG Toolkit, a practical system for scaling visual multi-vector retrieval with training-free, model-aware pooling and multi-stage retrieval. Motivated by Matryoshka Embeddings, our method performs static spatial pooling - including a lightweight sliding-window averaging variant - over patch embeddings to produce compact tile-level and global representations for fast candidate generation, followed by exact MaxSim reranking using full multi-vector embeddings.\n  Our design yields a quadratic reduction in vector-to-vector comparisons by reducing stored vectors per page from thousands to dozens, notably without requiring post-training, adapters, or distillation. Across experiments with interaction-style models such as ColPali and ColSmol-500M, we observe that over the limited ViDoRe v2 benchmark corpus 2-stage retrieval typically preserves NDCG and Recall @ 5/10 with minimal degradation, while substantially improving throughput (approximately 4x QPS); with sensitivity mainly at very large k. The toolkit additionally provides robust preprocessing - high resolution PDF to image conversion, optional margin/empty-region cropping and token hygiene (indexing only visual tokens) - and a reproducible evaluation pipeline, enabling rapid exploration of two-, three-, and cascaded retrieval variants. By emphasizing efficiency at common cutoffs (e.g., k <= 10), the toolkit lowers hardware barriers and makes state-of-the-art visual retrieval more accessible in practice.", "AI": {"tldr": "Visual RAG Toolkit\u901a\u8fc7\u8bad\u7ec3\u65e0\u5173\u7684\u6a21\u578b\u611f\u77e5\u6c60\u5316\u548c\u591a\u9636\u6bb5\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u591a\u5411\u91cf\u68c0\u7d22\u7684\u6548\u7387\uff0c\u5c06\u6bcf\u9875\u5411\u91cf\u4ece\u6570\u5343\u4e2a\u51cf\u5c11\u5230\u6570\u5341\u4e2a\uff0c\u5b9e\u73b0\u4e8c\u6b21\u65b9\u7ea7\u6bd4\u8f83\u51cf\u5c11\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u53474\u500d\u541e\u5410\u91cf\u3002", "motivation": "\u591a\u5411\u91cf\u89c6\u89c9\u68c0\u7d22\u5668\uff08\u5982ColPali\u98ce\u683c\u7684\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\uff09\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u6269\u5c55\u6027\u5dee\uff0c\u56e0\u4e3a\u6bcf\u9875\u4ea7\u751f\u6570\u5343\u4e2a\u5411\u91cf\uff0c\u5bfc\u81f4\u7d22\u5f15\u548c\u641c\u7d22\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u5b9e\u7528\u7684\u7cfb\u7edf\u6765\u6269\u5c55\u89c6\u89c9\u591a\u5411\u91cf\u68c0\u7d22\uff0c\u540c\u65f6\u964d\u4f4e\u786c\u4ef6\u95e8\u69db\u3002", "method": "1. \u57fa\u4e8eMatryoshka Embeddings\u601d\u60f3\uff0c\u91c7\u7528\u9759\u6001\u7a7a\u95f4\u6c60\u5316\uff08\u5305\u62ec\u8f7b\u91cf\u7ea7\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747\u53d8\u4f53\uff09\u5904\u7406\u8865\u4e01\u5d4c\u5165\uff0c\u751f\u6210\u7d27\u51d1\u7684\u74e6\u7247\u7ea7\u548c\u5168\u5c40\u8868\u793a\u7528\u4e8e\u5feb\u901f\u5019\u9009\u751f\u6210\uff1b2. \u4f7f\u7528\u5b8c\u6574\u591a\u5411\u91cf\u5d4c\u5165\u8fdb\u884c\u7cbe\u786e\u7684MaxSim\u91cd\u6392\u5e8f\uff1b3. \u63d0\u4f9b\u5f3a\u5927\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff08\u9ad8\u5206\u8fa8\u7387PDF\u8f6c\u56fe\u50cf\u3001\u8fb9\u7f18\u88c1\u526a\u3001\u89c6\u89c9\u6807\u8bb0\u7d22\u5f15\uff09\u548c\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u7ba1\u9053\u3002", "result": "\u5728ViDoRe v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u9636\u6bb5\u68c0\u7d22\u901a\u5e38\u80fd\u4fdd\u6301NDCG\u548cRecall @ 5/10\u7684\u7cbe\u5ea6\uff0c\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\uff08\u7ea64\u500dQPS\uff09\u3002\u5728\u5e38\u89c1\u622a\u65ad\u503c\uff08k\u226410\uff09\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4e3b\u8981\u654f\u611f\u5ea6\u51fa\u73b0\u5728\u975e\u5e38\u5927\u7684k\u503c\u65f6\u3002", "conclusion": "Visual RAG Toolkit\u901a\u8fc7\u5f3a\u8c03\u5e38\u89c1\u622a\u65ad\u503c\u7684\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u786c\u4ef6\u95e8\u69db\uff0c\u4f7f\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u68c0\u7d22\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u52a0\u53ef\u8bbf\u95ee\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u540e\u8bad\u7ec3\u3001\u9002\u914d\u5668\u6216\u84b8\u998f\uff0c\u5b9e\u73b0\u4e86\u4e8c\u6b21\u65b9\u7ea7\u5411\u91cf\u6bd4\u8f83\u51cf\u5c11\u3002"}}
{"id": "2602.12528", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12528", "abs": "https://arxiv.org/abs/2602.12528", "authors": ["Qi Liu", "Kun Ai", "Jiaxin Mao", "Yanzhao Zhang", "Mingxin Li", "Dingkun Long", "Pengjun Xie", "Fengbin Zhu", "Ji-Rong Wen"], "title": "DiffuRank: Effective Document Reranking with Diffusion Language Models", "comment": "The code is available at https://github.com/liuqi6777/DiffusionRank", "summary": "Recent advances in large language models (LLMs) have inspired new paradigms for document reranking. While this paradigm better exploits the reasoning and contextual understanding capabilities of LLMs, most existing LLM-based rerankers rely on autoregressive generation, which limits their efficiency and flexibility. In particular, token-by-token decoding incurs high latency, while the fixed left-to-right generation order causes early prediction errors to propagate and is difficult to revise. To address these limitations, we explore the use of diffusion language models (dLLMs) for document reranking and propose DiffuRank, a reranking framework built upon dLLMs. Unlike autoregressive models, dLLMs support more flexible decoding and generation processes that are not constrained to a left-to-right order, and enable parallel decoding, which may lead to improved efficiency and controllability. Specifically, we investigate three reranking strategies based on dLLMs: (1) a pointwise approach that uses dLLMs to estimate the relevance of each query-document pair; (2) a logit-based listwise approach that prompts dLLMs to jointly assess the relevance of multiple documents and derives ranking lists directly from model logits; and (3) a permutation-based listwise approach that adapts the canonical decoding process of dLLMs to the reranking tasks. For each approach, we design corresponding training methods to fully exploit the advantages of dLLMs. We evaluate both zero-shot and fine-tuned reranking performance on multiple benchmarks. Experimental results show that dLLMs achieve performance comparable to, and in some cases exceeding, that of autoregressive LLMs with similar model sizes. These findings demonstrate the promise of diffusion-based language models as a compelling alternative to autoregressive architectures for document reranking.", "AI": {"tldr": "DiffuRank\uff1a\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u6587\u6863\u91cd\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u89e3\u7801\u548c\u7075\u6d3b\u751f\u6210\u987a\u5e8f\u89e3\u51b3\u81ea\u56de\u5f52\u6a21\u578b\u6548\u7387\u4f4e\u3001\u9519\u8bef\u4f20\u64ad\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6587\u6863\u91cd\u6392\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5b58\u5728\u6548\u7387\u4f4e\uff08\u9010token\u89e3\u7801\u5ef6\u8fdf\u9ad8\uff09\u548c\u7075\u6d3b\u6027\u5dee\uff08\u56fa\u5b9a\u4ece\u5de6\u5230\u53f3\u987a\u5e8f\u5bfc\u81f4\u65e9\u671f\u9519\u8bef\u4f20\u64ad\u4e14\u96be\u4ee5\u4fee\u6b63\uff09\u7684\u95ee\u9898", "method": "\u63d0\u51faDiffuRank\u6846\u67b6\uff0c\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b(dLLMs)\u5b9e\u73b0\u4e09\u79cd\u91cd\u6392\u7b56\u7565\uff1a1)\u70b9\u5f0f\u65b9\u6cd5\u8bc4\u4f30\u5355\u4e2a\u67e5\u8be2-\u6587\u6863\u5bf9\u76f8\u5173\u6027\uff1b2)\u57fa\u4e8elogit\u7684\u5217\u8868\u65b9\u6cd5\u8054\u5408\u8bc4\u4f30\u591a\u4e2a\u6587\u6863\u76f8\u5173\u6027\uff1b3)\u57fa\u4e8e\u6392\u5217\u7684\u5217\u8868\u65b9\u6cd5\u5c06dLLMs\u6807\u51c6\u89e3\u7801\u8fc7\u7a0b\u9002\u914d\u5230\u91cd\u6392\u4efb\u52a1", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cdLLMs\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u90fd\u53d6\u5f97\u4e86\u4e0e\u76f8\u4f3c\u89c4\u6a21\u81ea\u56de\u5f52LLM\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6587\u6863\u91cd\u6392\u66ff\u4ee3\u67b6\u6784\u7684\u6f5c\u529b", "conclusion": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e3a\u6587\u6863\u91cd\u6392\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u66f4\u7075\u6d3b\u7684\u751f\u6210\u987a\u5e8f\u548c\u5e76\u884c\u89e3\u7801\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u53ef\u80fd\u63d0\u5347\u6548\u7387\u548c\u53ef\u63a7\u6027"}}
{"id": "2602.12530", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12530", "abs": "https://arxiv.org/abs/2602.12530", "authors": ["Kehan Zheng", "Deyao Hong", "Qian Li", "Jun Zhang", "Huan Yu", "Jie Jiang", "Hongning Wang"], "title": "Reasoning to Rank: An End-to-End Solution for Exploiting Large Language Models for Recommendation", "comment": null, "summary": "Recommender systems are tasked to infer users' evolving preferences and rank items aligned with their intents, which calls for in-depth reasoning beyond pattern-based scoring. Recent efforts start to leverage large language models (LLMs) for recommendation, but how to effectively optimize the model for improved recommendation utility is still under explored. In this work, we propose Reasoning to Rank, an end-to-end training framework that internalizes recommendation utility optimization into the learning of step-by-step reasoning in LLMs. To avoid position bias in LLM reasoning and enable direct optimization of the reasoning process, our framework performs reasoning at the user-item level and employs reinforcement learning for end-to-end training of the LLM. Experiments on three Amazon datasets and a large-scale industrial dataset showed consistent gains over strong conventional and LLM-based solutions. Extensive in-depth analyses validate the necessity of the key components in the proposed framework and shed lights on the future developments of this line of work.", "AI": {"tldr": "\u63d0\u51faReasoning to Rank\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7aef\u5230\u7aef\u8bad\u7ec3LLM\u8fdb\u884c\u63a8\u8350\u63a8\u7406\uff0c\u4f18\u5316\u63a8\u8350\u6548\u7528", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u9700\u8981\u8d85\u8d8a\u57fa\u4e8e\u6a21\u5f0f\u7684\u8bc4\u5206\uff0c\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u6765\u7406\u89e3\u7528\u6237\u504f\u597d\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u5229\u7528LLM\u8fdb\u884c\u63a8\u8350\uff0c\u4f46\u5982\u4f55\u6709\u6548\u4f18\u5316\u6a21\u578b\u4ee5\u63d0\u5347\u63a8\u8350\u6548\u7528\u4ecd\u5f85\u63a2\u7d22", "method": "\u63d0\u51faReasoning to Rank\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u63a8\u8350\u6548\u7528\u4f18\u5316\u5185\u5316\u5230LLM\u7684\u9010\u6b65\u63a8\u7406\u5b66\u4e60\u4e2d\u3002\u5728\u7528\u6237-\u7269\u54c1\u7ea7\u522b\u8fdb\u884c\u63a8\u7406\u4ee5\u907f\u514d\u4f4d\u7f6e\u504f\u5dee\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3", "result": "\u5728\u4e09\u4e2aAmazon\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8eLLM\u7684\u89e3\u51b3\u65b9\u6848\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u63d0\u5347\u3002\u6df1\u5165\u5206\u6790\u9a8c\u8bc1\u4e86\u6846\u67b6\u5173\u952e\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u542f\u793a"}}
{"id": "2602.12564", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12564", "abs": "https://arxiv.org/abs/2602.12564", "authors": ["Xiaoyou Zhou", "Yuqi Liu", "Zhao Liu", "Xiao Lv", "Bo Chen", "Ruiming Tang", "Guorui Zhou"], "title": "CAPTS: Channel-Aware, Preference-Aligned Trigger Selection for Multi-Channel Item-to-Item Retrieval", "comment": "10 pages, 6 figures", "summary": "Large-scale industrial recommender systems commonly adopt multi-channel retrieval for candidate generation, combining direct user-to-item (U2I) retrieval with two-hop user-to-item-to-item (U2I2I) pipelines. In U2I2I, the system selects a small set of historical interactions as triggers to seed downstream item-to-item (I2I) retrieval across multiple channels. In production, triggers are often selected using rule-based policies or learned scorers and tuned in a channel-by-channel manner. However, these practices face two persistent challenges: biased value attribution that values triggers by on-trigger feedback rather than their downstream utility as retrieval seeds, and uncoordinated multi-channel routing where channels select triggers independently under a shared quota, increasing cross-channel overlap. To address these challenges, we propose Channel-Aware, Preference-Aligned Trigger Selection (CAPTS), a unified and flexible framework that treats multi-channel trigger selection as a learnable routing problem. CAPTS introduces a Value Attribution Module (VAM) that provides look-ahead supervision by crediting each trigger with the subsequent engagement generated by items retrieved from it on each I2I channel, and a Channel-Adaptive Trigger Routing (CATR) module that coordinates trigger-to-channel assignment to maximize the overall value of multi-channel retrieval. Extensive offline experiments and large-scale online A/B tests on Kwai, Kuaishou's international short-video platform, show that CAPTS consistently improves multi-channel recall offline and delivers a +0.351% lift in average time spent per device online.", "AI": {"tldr": "CAPTS\u6846\u67b6\u901a\u8fc7\u4ef7\u503c\u5f52\u56e0\u6a21\u5757\u548c\u901a\u9053\u81ea\u9002\u5e94\u89e6\u53d1\u8def\u7531\uff0c\u89e3\u51b3\u4e86\u591a\u901a\u9053\u68c0\u7d22\u4e2d\u89e6\u53d1\u9009\u62e9\u7684\u504f\u89c1\u548c\u534f\u8c03\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u5728\u591a\u901a\u9053\u68c0\u7d22\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u504f\u89c1\u4ef7\u503c\u5f52\u56e0 - \u4ec5\u57fa\u4e8e\u89e6\u53d1\u53cd\u9988\u800c\u975e\u4e0b\u6e38\u68c0\u7d22\u6548\u7528\u8bc4\u4f30\u89e6\u53d1\u4ef7\u503c\uff1b2\uff09\u65e0\u534f\u8c03\u591a\u901a\u9053\u8def\u7531 - \u5404\u901a\u9053\u72ec\u7acb\u9009\u62e9\u89e6\u53d1\u5bfc\u81f4\u8de8\u901a\u9053\u91cd\u53e0\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u591a\u901a\u9053\u68c0\u7d22\u7684\u6574\u4f53\u6548\u679c\u3002", "method": "\u63d0\u51faCAPTS\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u4ef7\u503c\u5f52\u56e0\u6a21\u5757\uff08VAM\uff09- \u901a\u8fc7\u524d\u77bb\u76d1\u7763\u4e3a\u6bcf\u4e2a\u89e6\u53d1\u5206\u914d\u4e0b\u6e38I2I\u901a\u9053\u68c0\u7d22\u4ea7\u751f\u7684\u540e\u7eed\u53c2\u4e0e\u4ef7\u503c\uff1b2\uff09\u901a\u9053\u81ea\u9002\u5e94\u89e6\u53d1\u8def\u7531\uff08CATR\uff09- \u534f\u8c03\u89e6\u53d1\u5230\u901a\u9053\u7684\u5206\u914d\uff0c\u6700\u5927\u5316\u591a\u901a\u9053\u68c0\u7d22\u7684\u6574\u4f53\u4ef7\u503c\u3002", "result": "\u5728\u5feb\u624b\u56fd\u9645\u77ed\u89c6\u9891\u5e73\u53f0Kwai\u4e0a\u8fdb\u884c\u7684\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cCAPTS\u6301\u7eed\u63d0\u5347\u591a\u901a\u9053\u53ec\u56de\u7387\uff0c\u5728\u7ebf\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8bbe\u5907\u4f7f\u7528\u65f6\u95f4\u63d0\u5347+0.351%\u3002", "conclusion": "CAPTS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u901a\u9053\u89e6\u53d1\u9009\u62e9\u4e2d\u7684\u4ef7\u503c\u5f52\u56e0\u504f\u89c1\u548c\u901a\u9053\u534f\u8c03\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5b66\u4e60\u8def\u7531\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2602.12593", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12593", "abs": "https://arxiv.org/abs/2602.12593", "authors": ["Ziye Tong", "Jiahao Liu", "Weimin Zhang", "Hongji Ruan", "Derick Tang", "Zhanpeng Zeng", "Qinsong Zeng", "Peng Zhang", "Tun Lu", "Ning Gu"], "title": "RQ-GMM: Residual Quantized Gaussian Mixture Model for Multimodal Semantic Discretization in CTR Prediction", "comment": "Under review", "summary": "Multimodal content is crucial for click-through rate (CTR) prediction. However, directly incorporating continuous embeddings from pre-trained models into CTR models yields suboptimal results due to misaligned optimization objectives and convergence speed inconsistency during joint training. Discretizing embeddings into semantic IDs before feeding them into CTR models offers a more effective solution, yet existing methods suffer from limited codebook utilization, reconstruction accuracy, and semantic discriminability. We propose RQ-GMM (Residual Quantized Gaussian Mixture Model), which introduces probabilistic modeling to better capture the statistical structure of multimodal embedding spaces. Through Gaussian Mixture Models combined with residual quantization, RQ-GMM achieves superior codebook utilization and reconstruction accuracy. Experiments on public datasets and online A/B tests on a large-scale short-video platform serving hundreds of millions of users demonstrate substantial improvements: RQ-GMM yields a 1.502% gain in Advertiser Value over strong baselines. The method has been fully deployed, serving daily recommendations for hundreds of millions of users.", "AI": {"tldr": "RQ-GMM\u4f7f\u7528\u6b8b\u5dee\u91cf\u5316\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5c06\u591a\u6a21\u6001\u5d4c\u5165\u79bb\u6563\u5316\u4e3a\u8bed\u4e49ID\uff0c\u89e3\u51b3CTR\u9884\u6d4b\u4e2d\u9884\u8bad\u7ec3\u5d4c\u5165\u4e0eCTR\u6a21\u578b\u76ee\u6807\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u5e7f\u544a\u4e3b\u4ef7\u503c\u3002", "motivation": "\u591a\u6a21\u6001\u5185\u5bb9\u5bf9CTR\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8fde\u7eed\u5d4c\u5165\u8f93\u5165CTR\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4f18\u5316\u76ee\u6807\u548c\u6536\u655b\u901f\u5ea6\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u79bb\u6563\u5316\u65b9\u6cd5\u5b58\u5728\u7801\u672c\u5229\u7528\u7387\u4f4e\u3001\u91cd\u5efa\u7cbe\u5ea6\u5dee\u548c\u8bed\u4e49\u533a\u5206\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRQ-GMM\uff08\u6b8b\u5dee\u91cf\u5316\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7ed3\u5408\u6b8b\u5dee\u91cf\u5316\u6765\u66f4\u597d\u5730\u6355\u6349\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u7edf\u8ba1\u7ed3\u6784\uff0c\u5b9e\u73b0\u66f4\u4f18\u7684\u7801\u672c\u5229\u7528\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5927\u578b\u77ed\u89c6\u9891\u5e73\u53f0\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cRQ-GMM\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f971.502%\u7684\u5e7f\u544a\u4e3b\u4ef7\u503c\u63d0\u5347\uff0c\u5df2\u5b8c\u5168\u90e8\u7f72\u5e76\u4e3a\u6570\u4ebf\u7528\u6237\u63d0\u4f9b\u65e5\u5e38\u63a8\u8350\u670d\u52a1\u3002", "conclusion": "RQ-GMM\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5d4c\u5165\u79bb\u6563\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86CTR\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2602.12612", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12612", "abs": "https://arxiv.org/abs/2602.12612", "authors": ["Sein Kim", "Sangwu Park", "Hongseok Kang", "Wonjoong Kim", "Jimin Seo", "Yeonjun In", "Kanghoon Yoon", "Chanyoung Park"], "title": "Self-EvolveRec: Self-Evolving Recommender Systems with LLM-based Directional Feedback", "comment": null, "summary": "Traditional methods for automating recommender system design, such as Neural Architecture Search (NAS), are often constrained by a fixed search space defined by human priors, limiting innovation to pre-defined operators. While recent LLM-driven code evolution frameworks shift fixed search space target to open-ended program spaces, they primarily rely on scalar metrics (e.g., NDCG, Hit Ratio) that fail to provide qualitative insights into model failures or directional guidance for improvement. To address this, we propose Self-EvolveRec, a novel framework that establishes a directional feedback loop by integrating a User Simulator for qualitative critiques and a Model Diagnosis Tool for quantitative internal verification. Furthermore, we introduce a Diagnosis Tool - Model Co-Evolution strategy to ensure that evaluation criteria dynamically adapt as the recommendation architecture evolves. Extensive experiments demonstrate that Self-EvolveRec significantly outperforms state-of-the-art NAS and LLM-driven code evolution baselines in both recommendation performance and user satisfaction. Our code is available at https://github.com/Sein-Kim/self_evolverec.", "AI": {"tldr": "Self-EvolveRec\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u63a8\u8350\u7cfb\u7edf\u81ea\u52a8\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7528\u6237\u6a21\u62df\u5668\u8fdb\u884c\u5b9a\u6027\u6279\u5224\u548c\u6a21\u578b\u8bca\u65ad\u5de5\u5177\u8fdb\u884c\u5b9a\u91cf\u9a8c\u8bc1\uff0c\u5efa\u7acb\u5b9a\u5411\u53cd\u9988\u5faa\u73af\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edfNAS\u548cLLM\u9a71\u52a8\u7684\u4ee3\u7801\u8fdb\u5316\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u81ea\u52a8\u5316\u8bbe\u8ba1\u65b9\u6cd5\uff08\u5982NAS\uff09\u53d7\u9650\u4e8e\u4eba\u7c7b\u5148\u9a8c\u5b9a\u4e49\u7684\u56fa\u5b9a\u641c\u7d22\u7a7a\u95f4\uff0c\u800c\u6700\u8fd1\u7684LLM\u9a71\u52a8\u4ee3\u7801\u8fdb\u5316\u6846\u67b6\u867d\u7136\u8f6c\u5411\u5f00\u653e\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u4f46\u4e3b\u8981\u4f9d\u8d56NDCG\u3001\u547d\u4e2d\u7387\u7b49\u6807\u91cf\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5931\u8d25\u7684\u5b9a\u6027\u6d1e\u5bdf\u548c\u6539\u8fdb\u65b9\u5411\u6307\u5bfc\u3002", "method": "\u63d0\u51faSelf-EvolveRec\u6846\u67b6\uff0c\u5efa\u7acb\u5b9a\u5411\u53cd\u9988\u5faa\u73af\uff1a1\uff09\u96c6\u6210\u7528\u6237\u6a21\u62df\u5668\u63d0\u4f9b\u5b9a\u6027\u6279\u5224\uff1b2\uff09\u6a21\u578b\u8bca\u65ad\u5de5\u5177\u8fdb\u884c\u5b9a\u91cf\u5185\u90e8\u9a8c\u8bc1\uff1b3\uff09\u5f15\u5165\u8bca\u65ad\u5de5\u5177-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7b56\u7565\uff0c\u786e\u4fdd\u8bc4\u4f30\u6807\u51c6\u968f\u63a8\u8350\u67b6\u6784\u6f14\u5316\u800c\u52a8\u6001\u9002\u5e94\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSelf-EvolveRec\u5728\u63a8\u8350\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684NAS\u548cLLM\u9a71\u52a8\u7684\u4ee3\u7801\u8fdb\u5316\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Self-EvolveRec\u901a\u8fc7\u5efa\u7acb\u5b9a\u5411\u53cd\u9988\u5faa\u73af\u548c\u52a8\u6001\u8bc4\u4f30\u9002\u5e94\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u52a8\u5316\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u653e\u5f0f\u7684\u63a8\u8350\u67b6\u6784\u8fdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.12727", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12727", "abs": "https://arxiv.org/abs/2602.12727", "authors": ["Benben Wang", "Minghao Tang", "Hengran Zhang", "Jiafeng Guo", "Keping Bi"], "title": "Training Dense Retrievers with Multiple Positive Passages", "comment": null, "summary": "Modern knowledge-intensive systems, such as retrieval-augmented generation (RAG), rely on effective retrievers to establish the performance ceiling for downstream modules. However, retriever training has been bottlenecked by sparse, single-positive annotations, which lead to false-negative noise and suboptimal supervision. While the advent of large language models (LLMs) makes it feasible to collect comprehensive multi-positive relevance labels at scale, the optimal strategy for incorporating these dense signals into training remains poorly understood. In this paper, we present a systematic study of multi-positive optimization objectives for retriever training. We unify representative objectives, including Joint Likelihood (JointLH), Summed Marginal Likelihood (SumMargLH), and Log-Sum-Exp Pairwise (LSEPair) loss, under a shared contrastive learning framework. Our theoretical analysis characterizes their distinct gradient behaviors, revealing how each allocates probability mass across positive document sets. Empirically, we conduct extensive evaluations on Natural Questions, MS MARCO, and the BEIR benchmark across two realistic regimes: homogeneous LLM-annotated data and heterogeneous mixtures of human and LLM labels. Our results show that LSEPair consistently achieves superior robustness and performance across settings, while JointLH and SumMargLH exhibit high sensitivity to the quality of positives. Furthermore, we find that the simple strategy of random sampling (Rand1LH) serves as a reliable baseline. By aligning theoretical insights with empirical findings, we provide practical design principles for leveraging dense, LLM-augmented supervision to enhance retriever effectiveness.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u68c0\u7d22\u5668\u8bad\u7ec3\u4e2d\u7684\u591a\u6b63\u4f8b\u4f18\u5316\u76ee\u6807\uff0c\u53d1\u73b0LSEPair\u635f\u5931\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u7a33\u5065\uff0c\u800cJointLH\u548cSumMargLH\u5bf9\u6b63\u4f8b\u8d28\u91cf\u654f\u611f\u3002", "motivation": "\u77e5\u8bc6\u5bc6\u96c6\u578b\u7cfb\u7edf\uff08\u5982RAG\uff09\u4f9d\u8d56\u68c0\u7d22\u5668\u6027\u80fd\uff0c\u4f46\u4f20\u7edf\u8bad\u7ec3\u53d7\u9650\u4e8e\u7a00\u758f\u7684\u5355\u6b63\u4f8b\u6807\u6ce8\uff0c\u5bfc\u81f4\u5047\u9634\u6027\u566a\u58f0\u548c\u6b21\u4f18\u76d1\u7763\u3002\u867d\u7136LLM\u53ef\u4ee5\u5927\u89c4\u6a21\u6536\u96c6\u5bc6\u96c6\u7684\u591a\u6b63\u4f8b\u76f8\u5173\u6027\u6807\u7b7e\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5bc6\u96c6\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u5c06JointLH\u3001SumMargLH\u548cLSEPair\u7b49\u4ee3\u8868\u6027\u76ee\u6807\u7edf\u4e00\u5230\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e0b\uff0c\u8fdb\u884c\u7406\u8bba\u5206\u6790\u63ed\u793a\u5176\u68af\u5ea6\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u5728Natural Questions\u3001MS MARCO\u548cBEIR\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6db5\u76d6\u540c\u8d28LLM\u6807\u6ce8\u6570\u636e\u548c\u5f02\u8d28\u6df7\u5408\u6807\u6ce8\u4e24\u79cd\u73b0\u5b9e\u573a\u666f\u3002", "result": "LSEPair\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7a33\u5065\u6027\u548c\u6027\u80fd\uff0c\u800cJointLH\u548cSumMargLH\u5bf9\u6b63\u4f8b\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\u3002\u7b80\u5355\u7684\u968f\u673a\u91c7\u6837\u7b56\u7565\uff08Rand1LH\uff09\u53ef\u4f5c\u4e3a\u53ef\u9760\u57fa\u7ebf\u3002\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u8bc1\u7ed3\u679c\u4e00\u81f4\uff0c\u4e3a\u5229\u7528\u5bc6\u96c6LLM\u589e\u5f3a\u76d1\u7763\u63d0\u5347\u68c0\u7d22\u5668\u6548\u679c\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u591a\u6b63\u4f8b\u4f18\u5316\u76ee\u6807\uff0c\u53d1\u73b0LSEPair\u662f\u6700\u7a33\u5065\u7684\u9009\u62e9\uff0c\u4e3a\u5229\u7528LLM\u751f\u6210\u7684\u5bc6\u96c6\u6807\u6ce8\u6539\u8fdb\u68c0\u7d22\u5668\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2602.12783", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.12783", "abs": "https://arxiv.org/abs/2602.12783", "authors": ["Yuejie Li", "Ke Yang", "Yueying Hua", "Berlin Chen", "Jianhao Nie", "Yueping He", "Caixin Kang"], "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "comment": null, "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "AI": {"tldr": "SQuTR\u662f\u4e00\u4e2a\u7528\u4e8e\u53e3\u8bed\u67e5\u8be2\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5728\u590d\u6742\u58f0\u5b66\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53e3\u8bed\u67e5\u8be2\u68c0\u7d22\u8bc4\u4f30\u6570\u636e\u96c6\u901a\u5e38\u5c40\u9650\u4e8e\u7b80\u5355\u67e5\u8be2\u548c\u53d7\u9650\u566a\u58f0\u6761\u4ef6\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u7cfb\u7edf\u5728\u590d\u6742\u58f0\u5b66\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6784\u5efaSQuTR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u805a\u54086\u4e2a\u5e38\u7528\u82f1\u4e2d\u6587\u672c\u68c0\u7d22\u6570\u636e\u96c6\u768437,317\u4e2a\u72ec\u7279\u67e5\u8be2\uff0c\u4f7f\u7528200\u4e2a\u771f\u5b9e\u8bf4\u8bdd\u8005\u8bed\u97f3\u914d\u7f6e\u6587\u4ef6\u5408\u6210\u8bed\u97f3\uff0c\u5e76\u6df7\u540817\u7c7b\u771f\u5b9e\u73af\u5883\u566a\u58f0\u5728\u53ef\u63a7\u4fe1\u566a\u6bd4\u4e0b\u3002", "result": "\u68c0\u7d22\u6027\u80fd\u968f\u566a\u58f0\u589e\u52a0\u800c\u4e0b\u964d\uff0c\u4e0d\u540c\u7cfb\u7edf\u4e0b\u964d\u5e45\u5ea6\u5dee\u5f02\u663e\u8457\uff0c\u5373\u4f7f\u5927\u89c4\u6a21\u68c0\u7d22\u6a21\u578b\u5728\u6781\u7aef\u566a\u58f0\u4e0b\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u9c81\u68d2\u6027\u4ecd\u662f\u5173\u952e\u74f6\u9888\u3002", "conclusion": "SQuTR\u4e3a\u53e3\u8bed\u67e5\u8be2\u5230\u6587\u672c\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u8bca\u65ad\u5206\u6790\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2602.12819", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.12819", "abs": "https://arxiv.org/abs/2602.12819", "authors": ["Prasanna Sridhar", "Horace Lee", "David M. S. Pinto", "Andrew Zisserman", "Abhishek Dutta"], "title": "WISE: A Multimodal Search Engine for Visual Scenes, Audio, Objects, Faces, Speech, and Metadata", "comment": "Software: https://www.robots.ox.ac.uk/~vgg/software/wise/ , Online demos: https://www.robots.ox.ac.uk/~vgg/software/wise/demo/ , Example Queries: https://www.robots.ox.ac.uk/~vgg/software/wise/examples/", "summary": "In this paper, we present WISE, an open-source audiovisual search engine which integrates a range of multimodal retrieval capabilities into a single, practical tool accessible to users without machine learning expertise. WISE supports natural-language and reverse-image queries at both the scene level (e.g. empty street) and object level (e.g. horse) across images and videos; face-based search for specific individuals; audio retrieval of acoustic events using text (e.g. wood creak) or an audio file; search over automatically transcribed speech; and filtering by user-provided metadata. Rich insights can be obtained by combining queries across modalities -- for example, retrieving German trains from a historical archive by applying the object query \"train\" and the metadata query \"Germany\", or searching for a face in a place. By employing vector search techniques, WISE can scale to support efficient retrieval over millions of images or thousands of hours of video. Its modular architecture facilitates the integration of new models. WISE can be deployed locally for private or sensitive collections, and has been applied to various real-world use cases. Our code is open-source and available at https://gitlab.com/vgg/wise/wise.", "AI": {"tldr": "WISE\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u97f3\u89c6\u9891\u641c\u7d22\u5f15\u64ce\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u3001\u56fe\u50cf\u3001\u4eba\u8138\u3001\u97f3\u9891\u7b49\u591a\u79cd\u67e5\u8be2\u65b9\u5f0f\uff0c\u53ef\u6269\u5c55\u5230\u767e\u4e07\u7ea7\u56fe\u50cf\u548c\u5343\u5c0f\u65f6\u89c6\u9891\u68c0\u7d22\u3002", "motivation": "\u4e3a\u6ca1\u6709\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u7684\u7528\u6237\u63d0\u4f9b\u5b9e\u7528\u7684\u591a\u6a21\u6001\u68c0\u7d22\u5de5\u5177\uff0c\u5c06\u591a\u79cd\u68c0\u7d22\u80fd\u529b\u96c6\u6210\u5230\u5355\u4e00\u5de5\u5177\u4e2d\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\u4fdd\u62a4\u9690\u79c1\u654f\u611f\u6570\u636e\u3002", "method": "\u91c7\u7528\u5411\u91cf\u641c\u7d22\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u652f\u6301\u573a\u666f\u7ea7\u548c\u5bf9\u8c61\u7ea7\u7684\u56fe\u50cf/\u89c6\u9891\u67e5\u8be2\u3001\u4eba\u8138\u641c\u7d22\u3001\u97f3\u9891\u4e8b\u4ef6\u68c0\u7d22\u3001\u8bed\u97f3\u8f6c\u5f55\u641c\u7d22\u548c\u5143\u6570\u636e\u8fc7\u6ee4\uff0c\u5177\u6709\u6a21\u5757\u5316\u67b6\u6784\u4fbf\u4e8e\u96c6\u6210\u65b0\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u51fa\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u641c\u7d22\u5f15\u64ce\uff0c\u652f\u6301\u8de8\u6a21\u6001\u7ec4\u5408\u67e5\u8be2\uff08\u5982\"\u706b\u8f66\"\u5bf9\u8c61+\u5fb7\u56fd\u5143\u6570\u636e\uff09\uff0c\u5df2\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u4e2d\u9a8c\u8bc1\uff0c\u4ee3\u7801\u5f00\u6e90\u53ef\u7528\u3002", "conclusion": "WISE\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8fdb\u884c\u590d\u6742\u7684\u8de8\u6a21\u6001\u641c\u7d22\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u654f\u611f\u6216\u79c1\u6709\u6570\u636e\u96c6\u5408\u3002"}}
{"id": "2602.12941", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.12941", "abs": "https://arxiv.org/abs/2602.12941", "authors": ["Nan Lu", "Leyang Li", "Yurong Hu", "Rui Lin", "Shaoyi Xu"], "title": "JARVIS: An Evidence-Grounded Retrieval System for Interpretable Deceptive Reviews Adjudication", "comment": null, "summary": "Deceptive reviews, refer to fabricated feedback designed to artificially manipulate the perceived quality of products. Within modern e-commerce ecosystems, these reviews remain a critical governance challenge. Despite advances in review-level and graph-based detection methods, two pivotal limitations remain: inadequate generalization and lack of interpretability. To address these challenges, we propose JARVIS, a framework providing Judgment via Augmented Retrieval and eVIdence graph Structures. Starting from the review to be evaluated, it retrieves semantically similar evidence via hybrid dense-sparse multimodal retrieval, expands relational signals through shared entities, and constructs a heterogeneous evidence graph. Large language model then performs evidence-grounded adjudication to produce interpretable risk assessments. Offline experiments demonstrate that JARVIS enhances performance on our constructed review dataset, achieving a precision increase from 0.953 to 0.988 and a recall boost from 0.830 to 0.901. In the production environment, our framework achieves a 27% increase in the recall volume and reduces manual inspection time by 75%. Furthermore, the adoption rate of the model-generated analysis reaches 96.4%.", "AI": {"tldr": "JARVIS\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u68c0\u7d22\u548c\u8bc1\u636e\u56fe\u7ed3\u6784\u68c0\u6d4b\u865a\u5047\u8bc4\u8bba\uff0c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faJARVIS\u6846\u67b6\uff1a\u4ece\u5f85\u8bc4\u4f30\u8bc4\u8bba\u51fa\u53d1\uff0c\u901a\u8fc7\u6df7\u5408\u7a20\u5bc6-\u7a00\u758f\u591a\u6a21\u6001\u68c0\u7d22\u83b7\u53d6\u8bed\u4e49\u76f8\u4f3c\u8bc1\u636e\uff0c\u901a\u8fc7\u5171\u4eab\u5b9e\u4f53\u6269\u5c55\u5173\u7cfb\u4fe1\u53f7\uff0c\u6784\u5efa\u5f02\u8d28\u8bc1\u636e\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u88c1\u51b3", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u4e2d\uff0c\u5728\u6784\u5efa\u7684\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\uff0c\u7cbe\u786e\u5ea6\u4ece0.953\u63d0\u5347\u52300.988\uff0c\u53ec\u56de\u7387\u4ece0.830\u63d0\u5347\u52300.901\uff1b\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u53ec\u56de\u91cf\u589e\u52a027%\uff0c\u4eba\u5de5\u68c0\u67e5\u65f6\u95f4\u51cf\u5c1175%\uff0c\u6a21\u578b\u751f\u6210\u5206\u6790\u7684\u91c7\u7eb3\u7387\u8fbe\u523096.4%", "conclusion": "JARVIS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5728\u79bb\u7ebf\u5b9e\u9a8c\u548c\u751f\u4ea7\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347"}}
{"id": "2602.12968", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.12968", "abs": "https://arxiv.org/abs/2602.12968", "authors": ["Junhua Liu", "Yang Jihao", "Cheng Chang", "Kunrong LI", "Bin Fu", "Kwan Hui Lim"], "title": "RGAlign-Rec: Ranking-Guided Alignment for Latent Query Reasoning in Recommendation Systems", "comment": null, "summary": "Proactive intent prediction is a critical capability in modern e-commerce chatbots, enabling \"zero-query\" recommendations by anticipating user needs from behavioral and contextual signals. However, existing industrial systems face two fundamental challenges: (1) the semantic gap between discrete user features and the semantic intents within the chatbot's Knowledge Base, and (2) the objective misalignment between general-purpose LLM outputs and task-specific ranking utilities. To address these issues, we propose RGAlign-Rec, a closed-loop alignment framework that integrates an LLM-based semantic reasoner with a Query-Enhanced (QE) ranking model. We also introduce Ranking-Guided Alignment (RGA), a multi-stage training paradigm that utilizes downstream ranking signals as feedback to refine the LLM's latent reasoning. Extensive experiments on a large-scale industrial dataset from Shopee demonstrate that RGAlign-Rec achieves a 0.12% gain in GAUC, leading to a significant 3.52% relative reduction in error rate, and a 0.56% improvement in Recall@3. Online A/B testing further validates the cumulative effectiveness of our framework: the Query-Enhanced model (QE-Rec) initially yields a 0.98% improvement in CTR, while the subsequent Ranking-Guided Alignment stage contributes an additional 0.13% gain. These results indicate that ranking-aware alignment effectively synchronizes semantic reasoning with ranking objectives, significantly enhancing both prediction accuracy and service quality in real-world proactive recommendation systems.", "AI": {"tldr": "RGAlign-Rec\uff1a\u4e00\u4e2a\u95ed\u73af\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8bed\u4e49\u63a8\u7406\u5668\u4e0e\u67e5\u8be2\u589e\u5f3a\u6392\u5e8f\u6a21\u578b\u7ed3\u5408\uff0c\u89e3\u51b3\u7535\u5546\u804a\u5929\u673a\u5668\u4eba\u4e2d\u4e3b\u52a8\u610f\u56fe\u9884\u6d4b\u7684\u8bed\u4e49\u9e3f\u6c9f\u548c\u76ee\u6807\u9519\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u7535\u5546\u804a\u5929\u673a\u5668\u4eba\u9700\u8981\u4e3b\u52a8\u610f\u56fe\u9884\u6d4b\u80fd\u529b\u4ee5\u5b9e\u73b0\"\u96f6\u67e5\u8be2\"\u63a8\u8350\uff0c\u4f46\u73b0\u6709\u5de5\u4e1a\u7cfb\u7edf\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1a1\uff09\u79bb\u6563\u7528\u6237\u7279\u5f81\u4e0e\u77e5\u8bc6\u5e93\u8bed\u4e49\u610f\u56fe\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff1b2\uff09\u901a\u7528LLM\u8f93\u51fa\u4e0e\u4efb\u52a1\u7279\u5b9a\u6392\u5e8f\u6548\u7528\u4e4b\u95f4\u7684\u76ee\u6807\u9519\u914d\u3002", "method": "\u63d0\u51faRGAlign-Rec\u6846\u67b6\uff0c\u6574\u5408\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u63a8\u7406\u5668\u548c\u67e5\u8be2\u589e\u5f3a\u6392\u5e8f\u6a21\u578b\u3002\u5f15\u5165\u6392\u5e8f\u5f15\u5bfc\u5bf9\u9f50\uff08RGA\uff09\u591a\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u4e0b\u6e38\u6392\u5e8f\u4fe1\u53f7\u4f5c\u4e3a\u53cd\u9988\u6765\u4f18\u5316LLM\u7684\u6f5c\u5728\u63a8\u7406\u3002", "result": "\u5728Shopee\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cRGAlign-Rec\u5b9e\u73b0GAUC\u63d0\u53470.12%\uff0c\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e3.52%\uff0cRecall@3\u63d0\u53470.56%\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u67e5\u8be2\u589e\u5f3a\u6a21\u578b\uff08QE-Rec\uff09\u5e26\u6765CTR\u63d0\u53470.98%\uff0c\u540e\u7eed\u6392\u5e8f\u5f15\u5bfc\u5bf9\u9f50\u9636\u6bb5\u989d\u5916\u8d21\u732e0.13%\u589e\u76ca\u3002", "conclusion": "\u6392\u5e8f\u611f\u77e5\u5bf9\u9f50\u80fd\u6709\u6548\u540c\u6b65\u8bed\u4e49\u63a8\u7406\u4e0e\u6392\u5e8f\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u73b0\u5b9e\u4e16\u754c\u4e3b\u52a8\u63a8\u8350\u7cfb\u7edf\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u670d\u52a1\u8d28\u91cf\uff0c\u4e3a\u89e3\u51b3\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u548c\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.13134", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13134", "abs": "https://arxiv.org/abs/2602.13134", "authors": ["Huishi Luo", "Shuokai Li", "Hanchen Yang", "Zhongbo Sun", "Haojie Ding", "Boheng Zhang", "Zijia Cai", "Renliang Qian", "Fan Yang", "Tingting Gao", "Chenyi Lei", "Wenwu Ou", "Fuzhen Zhuang"], "title": "Awakening Dormant Users: Generative Recommendation with Counterfactual Functional Role Reasoning", "comment": null, "summary": "Awakening dormant users, who remain engaged but exhibit low conversion, is a pivotal driver for incremental GMV growth in large-scale e-commerce platforms. However, existing approaches often yield suboptimal results since they typically rely on single-step estimation of an item's intrinsic value (e.g., immediate click probability). This mechanism overlooks the instrumental effect of items, where specific interactions act as triggers to shape latent intent and drive subsequent decisions along a conversion trajectory. To bridge this gap, we propose RoleGen, a novel framework that synergizes a Conversion Trajectory Reasoner with a Generative Behavioral Backbone. Specifically, the LLM-based Reasoner explicitly models the context-dependent Functional Role of items to reconstruct intent evolution. It further employs counterfactual inference to simulate diverse conversion paths, effectively mitigating interest collapse. These reasoned candidate items are integrated into the generative backbone, which is optimized via a collaborative \"Reasoning-Execution-Feedback-Reflection\" closed-loop strategy to ensure grounded execution. Extensive offline experiments and online A/B testing on the Kuaishou e-commerce platform demonstrate that RoleGen achieves a 6.2% gain in Recall@1 and a 7.3% increase in online order volume, confirming its effectiveness in activating the dormant user base.", "AI": {"tldr": "RoleGen\u662f\u4e00\u4e2a\u5524\u9192\u7535\u5546\u5e73\u53f0\u4f11\u7720\u7528\u6237\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5546\u54c1\u5728\u8f6c\u5316\u8f68\u8ff9\u4e2d\u7684\u529f\u80fd\u6027\u89d2\u8272\uff0c\u7ed3\u5408LLM\u63a8\u7406\u5668\u548c\u751f\u6210\u5f0f\u884c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\u548c\u8ba2\u5355\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u5355\u6b65\u4f30\u8ba1\u5546\u54c1\u5185\u5728\u4ef7\u503c\uff08\u5982\u5373\u65f6\u70b9\u51fb\u6982\u7387\uff09\uff0c\u5ffd\u7565\u4e86\u5546\u54c1\u7684\u5de5\u5177\u6027\u6548\u5e94\u2014\u2014\u7279\u5b9a\u4ea4\u4e92\u53ef\u4ee5\u89e6\u53d1\u6f5c\u5728\u610f\u56fe\u5e76\u9a71\u52a8\u540e\u7eed\u8f6c\u5316\u51b3\u7b56\uff0c\u5bfc\u81f4\u5bf9\u4f11\u7720\u7528\u6237\u7684\u5524\u9192\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faRoleGen\u6846\u67b6\uff1a1) LLM\u63a8\u7406\u5668\u663e\u5f0f\u5efa\u6a21\u5546\u54c1\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u529f\u80fd\u6027\u89d2\u8272\uff0c\u91cd\u6784\u610f\u56fe\u6f14\u5316\u8fc7\u7a0b\uff1b2) \u4f7f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6a21\u62df\u591a\u6837\u5316\u8f6c\u5316\u8def\u5f84\uff0c\u7f13\u89e3\u5174\u8da3\u584c\u9677\uff1b3) \u5c06\u63a8\u7406\u5019\u9009\u5546\u54c1\u96c6\u6210\u5230\u751f\u6210\u5f0f\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\"\u63a8\u7406-\u6267\u884c-\u53cd\u9988-\u53cd\u601d\"\u95ed\u73af\u7b56\u7565\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u5feb\u624b\u7535\u5546\u5e73\u53f0\u7684\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cRoleGen\u5b9e\u73b0\u4e86Recall@1\u63d0\u53476.2%\uff0c\u5728\u7ebf\u8ba2\u5355\u91cf\u589e\u52a07.3%\uff0c\u6709\u6548\u6fc0\u6d3b\u4e86\u4f11\u7720\u7528\u6237\u7fa4\u4f53\u3002", "conclusion": "RoleGen\u901a\u8fc7\u5efa\u6a21\u5546\u54c1\u5728\u8f6c\u5316\u8f68\u8ff9\u4e2d\u7684\u529f\u80fd\u6027\u89d2\u8272\uff0c\u7ed3\u5408LLM\u63a8\u7406\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u5546\u5e73\u53f0\u4f11\u7720\u7528\u6237\u7684\u5524\u9192\u6548\u679c\uff0c\u4e3aGMV\u589e\u957f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13165", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13165", "abs": "https://arxiv.org/abs/2602.13165", "authors": ["Asmit Kumar Singh", "Haozhe Wang", "Laxmi Naga Santosh Attaluri", "Tak Chiam", "Weihua Zhu"], "title": "Asynchronous Verified Semantic Caching for Tiered LLM Architectures", "comment": null, "summary": "Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \\textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.", "AI": {"tldr": "Krites\u662f\u4e00\u79cd\u5f02\u6b65LLM\u5224\u65ad\u7684\u8bed\u4e49\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7LLM\u9a8c\u8bc1\u6765\u6269\u5c55\u9759\u6001\u7f13\u5b58\u7684\u8986\u76d6\u8303\u56f4\uff0c\u800c\u4e0d\u5f71\u54cd\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u3002", "motivation": "LLM\u5728\u641c\u7d22\u3001\u52a9\u624b\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u5904\u4e8e\u5173\u952e\u8def\u5f84\uff0c\u9700\u8981\u8bed\u4e49\u7f13\u5b58\u6765\u964d\u4f4e\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u3002\u4f20\u7edf\u9759\u6001-\u52a8\u6001\u5206\u5c42\u7f13\u5b58\u4f7f\u7528\u5355\u4e00\u5d4c\u5165\u76f8\u4f3c\u5ea6\u9608\u503c\uff0c\u5b58\u5728\u4fdd\u5b88\u9608\u503c\u9519\u8fc7\u5b89\u5168\u91cd\u7528\u673a\u4f1a\u4e0e\u6fc0\u8fdb\u9608\u503c\u5bfc\u81f4\u8bed\u4e49\u9519\u8bef\u54cd\u5e94\u7684\u786c\u6027\u6743\u8861\u3002", "method": "Krites\u91c7\u7528\u5f02\u6b65LLM\u5224\u65ad\u7b56\u7565\uff1a\u5728\u5173\u952e\u8def\u5f84\u4e0a\u4fdd\u6301\u6807\u51c6\u9759\u6001\u9608\u503c\u7b56\u7565\u884c\u4e3a\uff1b\u5f53\u63d0\u793a\u7684\u6700\u90bb\u8fd1\u9759\u6001\u90bb\u5c45\u7565\u4f4e\u4e8e\u9759\u6001\u9608\u503c\u65f6\uff0c\u5f02\u6b65\u8c03\u7528LLM\u6cd5\u5b98\u9a8c\u8bc1\u9759\u6001\u54cd\u5e94\u662f\u5426\u9002\u7528\u4e8e\u65b0\u63d0\u793a\uff1b\u6279\u51c6\u7684\u5339\u914d\u88ab\u63d0\u5347\u5230\u52a8\u6001\u7f13\u5b58\u4e2d\uff0c\u5141\u8bb8\u672a\u6765\u91cd\u590d\u548c\u6539\u5199\u91cd\u7528\u7b56\u5212\u7684\u9759\u6001\u7b54\u6848\u3002", "result": "\u5728\u5bf9\u8bdd\u548c\u641c\u7d22\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8ddf\u8e2a\u9a71\u52a8\u6a21\u62df\u4e2d\uff0cKrites\u5c06\u4f7f\u7528\u7b56\u5212\u9759\u6001\u7b54\u6848\u670d\u52a1\u7684\u8bf7\u6c42\u6bd4\u4f8b\uff08\u76f4\u63a5\u9759\u6001\u547d\u4e2d\u52a0\u9a8c\u8bc1\u63d0\u5347\uff09\u63d0\u9ad8\u4e86\u6700\u591a3.9\u500d\uff0c\u540c\u65f6\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "Krites\u901a\u8fc7\u5f02\u6b65LLM\u9a8c\u8bc1\u673a\u5236\u6709\u6548\u6269\u5c55\u4e86\u9759\u6001\u7f13\u5b58\u7684\u8986\u76d6\u8303\u56f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9608\u503c\u7b56\u7565\u7684\u786c\u6027\u6743\u8861\u95ee\u9898\uff0c\u5728\u4e0d\u589e\u52a0\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7387\u3002"}}
{"id": "2602.13179", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13179", "abs": "https://arxiv.org/abs/2602.13179", "authors": ["Jiankun Zhang", "Shenglai Zeng", "Kai Guo", "Xinnan Dai", "Hui Liu", "Jiliang Tang", "Yi Chang"], "title": "Fix Before Search: Benchmarking Agentic Query Visual Pre-processing in Multimodal Retrieval-augmented Generation", "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a key paradigm for grounding MLLMs with external knowledge. While query pre-processing (e.g., rewriting) is standard in text-based RAG, existing MRAG pipelines predominantly treat visual inputs as static and immutable, implicitly assuming they are noise-free. However, real-world visual queries are often ``imperfect'' -- suffering from geometric distortions, quality degradation, or semantic ambiguity -- leading to catastrophic retrieval failures. To address this gap, we propose V-QPP-Bench, the first comprehensive benchmark dedicated to Visual Query Pre-processing (V-QPP). We formulate V-QPP as an agentic decision-making task where MLLMs must autonomously diagnose imperfections and deploy perceptual tools to refine queries. Our extensive evaluation across 46,700 imperfect queries and diverse MRAG paradigms reveals three critical insights: (1) Vulnerability -- visual imperfections severely degrade both retrieval recall and end-to-end MRAG performance; (2) Restoration Potential \\& Bottleneck -- while oracle preprocessing recovers near-perfect performance, off-the-shelf MLLMs struggle with tool selection and parameter prediction without specialized training; and (3) Training Enhancement -- supervised fine-tuning enables compact models to achieve comparable or superior performance to larger proprietary models, demonstrating the benchmark's value for developing robust MRAG systems The code is available at https://github.com/phycholosogy/VQQP_Bench", "AI": {"tldr": "V-QPP-Bench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u89c6\u89c9\u67e5\u8be2\u9884\u5904\u7406\uff08V-QPP\uff09\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MRAG\uff09\u4e2d\u89c6\u89c9\u67e5\u8be2\u4e0d\u5b8c\u7f8e\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\u89c6\u89c9\u7f3a\u9677\u4f1a\u4e25\u91cd\u964d\u4f4e\u68c0\u7d22\u6027\u80fd\uff0c\u800c\u9002\u5f53\u7684\u9884\u5904\u7406\u53ef\u4ee5\u663e\u8457\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MRAG\u7cfb\u7edf\u901a\u5e38\u5c06\u89c6\u89c9\u8f93\u5165\u89c6\u4e3a\u9759\u6001\u4e14\u65e0\u566a\u58f0\u7684\uff0c\u4f46\u73b0\u5b9e\u4e2d\u7684\u89c6\u89c9\u67e5\u8be2\u5f80\u5f80\u5b58\u5728\u51e0\u4f55\u53d8\u5f62\u3001\u8d28\u91cf\u9000\u5316\u6216\u8bed\u4e49\u6a21\u7cca\u7b49\"\u4e0d\u5b8c\u7f8e\"\u95ee\u9898\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u68c0\u7d22\u5931\u8d25\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9\u67e5\u8be2\u9884\u5904\u7406\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5c06V-QPP\u5b9a\u4e49\u4e3a\u667a\u80fd\u4f53\u51b3\u7b56\u4efb\u52a1\uff0c\u8981\u6c42MLLMs\u81ea\u4e3b\u8bca\u65ad\u89c6\u89c9\u7f3a\u9677\u5e76\u90e8\u7f72\u611f\u77e5\u5de5\u5177\u6765\u4f18\u5316\u67e5\u8be2\u3002\u6784\u5efa\u4e86\u5305\u542b46,700\u4e2a\u4e0d\u5b8c\u7f8e\u67e5\u8be2\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u79cdMRAG\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u9884\u5904\u7406\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "1) \u89c6\u89c9\u7f3a\u9677\u4f1a\u4e25\u91cd\u964d\u4f4e\u68c0\u7d22\u53ec\u56de\u7387\u548c\u7aef\u5230\u7aefMRAG\u6027\u80fd\uff1b2) \u7406\u60f3\u7684\u9884\u5904\u7406\u53ef\u4ee5\u6062\u590d\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6027\u80fd\uff0c\u4f46\u73b0\u6210\u7684MLLMs\u5728\u5de5\u5177\u9009\u62e9\u548c\u53c2\u6570\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1b3) \u76d1\u7763\u5fae\u8c03\u80fd\u4f7f\u7d27\u51d1\u6a21\u578b\u8fbe\u5230\u4e0e\u5927\u578b\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "V-QPP-Bench\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684MRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u67e5\u8be2\u9884\u5904\u7406\u5bf9\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u4e13\u95e8\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u9884\u5904\u7406\u80fd\u529b\u3002"}}

{"id": "2512.10963", "categories": ["cs.IR", "cs.AI", "cs.HC", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.10963", "abs": "https://arxiv.org/abs/2512.10963", "authors": ["Zheqi Hu", "Xuanjing Chen", "Jinlin Hu"], "title": "Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis", "comment": null, "summary": "With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u610f\u56fe\u8bc6\u522b\u6a21\u578b\uff08MMEI\uff09\u7684\u4e91\u539f\u751f\u4e2a\u6027\u5316AIGC\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u60c5\u611f\u5206\u6790\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u884c\u4e3a\u6570\u636e\uff08\u70b9\u51fb\u3001\u89c2\u770b\u3001\u8bc4\u5206\uff09\uff0c\u5ffd\u89c6\u4e86\u7528\u6237\u5728\u5185\u5bb9\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u65f6\u60c5\u611f\u548c\u610f\u56fe\u72b6\u6001\u3002\u968f\u7740AIGC\u5728\u5404\u9886\u57df\u7684\u5feb\u901f\u589e\u957f\uff0c\u5bf9\u60c5\u611f\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eBERT\u7684\u8de8\u6a21\u6001Transformer\u4e0e\u6ce8\u610f\u529b\u878d\u5408\u7684\u591a\u6a21\u6001\u60c5\u611f\u610f\u56fe\u8bc6\u522b\u6a21\u578b\uff08MMEI\uff09\u3002\u7cfb\u7edf\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff08ViT\u3001Wav2Vec2\u3001BERT\uff09\u8054\u5408\u5904\u7406\u89c6\u89c9\uff08\u9762\u90e8\u8868\u60c5\uff09\u3001\u542c\u89c9\uff08\u8bed\u97f3\u8bed\u8c03\uff09\u548c\u6587\u672c\uff08\u8bc4\u8bba\u6216\u8bdd\u8bed\uff09\u6a21\u6001\uff0c\u7136\u540e\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u5b66\u4e60\u60c5\u611f\u610f\u56fe\u8868\u793a\uff0c\u6700\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u5339\u914d\u5c42\u9a71\u52a8\u4e2a\u6027\u5316\u5185\u5bb9\u63a8\u8350\u3002", "result": "\u5728\u57fa\u51c6\u60c5\u611f\u6570\u636e\u96c6\uff08AIGC-INT\u3001MELD\u3001CMU-MOSEI\uff09\u548cAIGC\u4ea4\u4e92\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMMEI\u6a21\u578b\u76f8\u6bd4\u6700\u4f73\u878d\u5408Transformer\u57fa\u7ebf\u5728F1\u5206\u6570\u4e0a\u63d0\u53474.3%\uff0c\u4ea4\u53c9\u71b5\u635f\u5931\u964d\u4f4e12.3%\u3002\u7528\u6237\u7ea7\u5728\u7ebf\u8bc4\u4f30\u663e\u793a\uff0c\u60c5\u611f\u9a71\u52a8\u63a8\u8350\u4f7f\u53c2\u4e0e\u65f6\u95f4\u589e\u52a015.2%\uff0c\u6ee1\u610f\u5ea6\u8bc4\u5206\u63d0\u534711.8%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u51f8\u663e\u4e86\u8de8\u6a21\u6001\u60c5\u611f\u667a\u80fd\u5728\u4e0b\u4e00\u4ee3AIGC\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u5171\u60c5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63a8\u8350\u4f53\u9a8c\uff0c\u6709\u6548\u5c06AI\u751f\u6210\u5185\u5bb9\u4e0e\u7528\u6237\u60c5\u611f\u548c\u610f\u56fe\u72b6\u6001\u5bf9\u9f50\u3002"}}
{"id": "2512.11254", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.11254", "abs": "https://arxiv.org/abs/2512.11254", "authors": ["Longtao Xiao", "Haolin Zhang", "Guohao Cai", "Jieming Zhu", "Yifan Wang", "Heng Chang", "Zhenhua Dong", "Xiu Li", "Ruixuan Li"], "title": "FAIR: Focused Attention Is All You Need for Generative Recommendation", "comment": null, "summary": "Recently, transformer-based generative recommendation has garnered significant attention for user behavior modeling. However, it often requires discretizing items into multi-code representations (e.g., typically four code tokens or more), which sharply increases the length of the original item sequence. This expansion poses challenges to transformer-based models for modeling user behavior sequences with inherent noises, since they tend to overallocate attention to irrelevant or noisy context. To mitigate this issue, we propose FAIR, the first generative recommendation framework with focused attention, which enhances attention scores to relevant context while suppressing those to irrelevant ones. Specifically, we propose (1) a focused attention mechanism integrated into the standard Transformer, which learns two separate sets of Q and K attention weights and computes their difference as the final attention scores to eliminate attention noise while focusing on relevant contexts; (2) a noise-robustness objective, which encourages the model to maintain stable attention patterns under stochastic perturbations, preventing undesirable shifts toward irrelevant context due to noise; and (3) a mutual information maximization objective, which guides the model to identify contexts that are most informative for next-item prediction. We validate the effectiveness of FAIR on four public benchmarks, demonstrating its superior performance compared to existing methods.", "AI": {"tldr": "FAIR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7126\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u591a\u7801\u8868\u793a\u5bfc\u81f4\u7684\u5e8f\u5217\u8fc7\u957f\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5f0f\u63a8\u8350\u9700\u8981\u5c06\u7269\u54c1\u79bb\u6563\u5316\u4e3a\u591a\u4e2a\u4ee3\u7801\u8868\u793a\uff08\u901a\u5e384\u4e2a\u6216\u66f4\u591a\uff09\uff0c\u8fd9\u663e\u8457\u589e\u52a0\u4e86\u539f\u59cb\u7269\u54c1\u5e8f\u5217\u7684\u957f\u5ea6\u3002\u5e8f\u5217\u6269\u5c55\u4f7f\u5f97Transformer\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u56fa\u6709\u566a\u58f0\u7684\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u503e\u5411\u4e8e\u8fc7\u5ea6\u5173\u6ce8\u4e0d\u76f8\u5173\u6216\u566a\u58f0\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faFAIR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u805a\u7126\u6ce8\u610f\u529b\u673a\u5236\uff0c\u96c6\u6210\u5230\u6807\u51c6Transformer\u4e2d\uff0c\u5b66\u4e60\u4e24\u7ec4\u72ec\u7acb\u7684Q\u548cK\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u8ba1\u7b97\u5b83\u4eec\u7684\u5dee\u5f02\u4f5c\u4e3a\u6700\u7ec8\u6ce8\u610f\u529b\u5206\u6570\uff0c\u4ee5\u6d88\u9664\u6ce8\u610f\u529b\u566a\u58f0\u5e76\u805a\u7126\u76f8\u5173\u4e0a\u4e0b\u6587\uff1b2\uff09\u566a\u58f0\u9c81\u68d2\u6027\u76ee\u6807\uff0c\u9f13\u52b1\u6a21\u578b\u5728\u968f\u673a\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u9632\u6b62\u56e0\u566a\u58f0\u800c\u5411\u4e0d\u76f8\u5173\u4e0a\u4e0b\u6587\u504f\u79fb\uff1b3\uff09\u4e92\u4fe1\u606f\u6700\u5927\u5316\u76ee\u6807\uff0c\u5f15\u5bfc\u6a21\u578b\u8bc6\u522b\u5bf9\u4e0b\u4e00\u7269\u54c1\u9884\u6d4b\u6700\u4fe1\u606f\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86FAIR\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "FAIR\u662f\u9996\u4e2a\u91c7\u7528\u805a\u7126\u6ce8\u610f\u529b\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bad\u7ec3\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7801\u8868\u793a\u5bfc\u81f4\u7684\u5e8f\u5217\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u7684\u8d28\u91cf\u3002"}}

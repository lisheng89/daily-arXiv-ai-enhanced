{"id": "2602.13543", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.13543", "abs": "https://arxiv.org/abs/2602.13543", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "LiveNewsBench: Evaluating LLM Web Search Capabilities with Freshly Curated News", "comment": "An earlier version of this work was publicly available on OpenReview as an ICLR 2026 submission in September 2025", "summary": "Large Language Models (LLMs) with agentic web search capabilities show strong potential for tasks requiring real-time information access and complex fact retrieval, yet evaluating such systems remains challenging. We introduce \\bench, a rigorous and regularly updated benchmark designed to assess the agentic web search abilities of LLMs. \\bench automatically generates fresh question-answer pairs from recent news articles, ensuring that questions require information beyond an LLM's training data and enabling clear separation between internal knowledge and search capability. The benchmark features intentionally difficult questions requiring multi-hop search queries, page visits, and reasoning, making it well-suited for evaluating agentic search behavior. Our automated data curation and question generation pipeline enables frequent benchmark updates and supports construction of a large-scale training dataset for agentic web search models, addressing the scarcity of such data in the research community. To ensure reliable evaluation, we include a subset of human-verified samples in the test set. We evaluate a broad range of systems using \\bench, including commercial and open-weight LLMs as well as LLM-based web search APIs. The leaderboard, datasets, and code are publicly available at livenewsbench.com.", "AI": {"tldr": "LiveNewsBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5f0f\u7f51\u9875\u641c\u7d22\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4ece\u8fd1\u671f\u65b0\u95fb\u81ea\u52a8\u751f\u6210\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u786e\u4fdd\u95ee\u9898\u9700\u8981\u8d85\u51fa\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7684\u4fe1\u606f\uff0c\u5e76\u652f\u6301\u591a\u8df3\u641c\u7d22\u548c\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u5177\u6709\u4ee3\u7406\u5f0f\u7f51\u9875\u641c\u7d22\u80fd\u529b\u7684LLM\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u533a\u5206\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u548c\u5b9e\u9645\u641c\u7d22\u80fd\u529b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u540c\u65f6\u89e3\u51b3\u76f8\u5173\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u6570\u636e\u6536\u96c6\u548c\u95ee\u9898\u751f\u6210\u6d41\u7a0b\uff0c\u4ece\u8fd1\u671f\u65b0\u95fb\u6587\u7ae0\u751f\u6210\u65b0\u9c9c\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u5305\u542b\u9700\u8981\u591a\u8df3\u641c\u7d22\u3001\u9875\u9762\u8bbf\u95ee\u548c\u63a8\u7406\u7684\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u5728\u6d4b\u8bd5\u96c6\u4e2d\u5305\u542b\u4eba\u5de5\u9a8c\u8bc1\u6837\u672c\u3002", "result": "\u521b\u5efa\u4e86LiveNewsBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5b9a\u671f\u66f4\u65b0\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u4ee3\u7406\u5f0f\u7f51\u9875\u641c\u7d22\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u5305\u62ec\u5546\u4e1a\u548c\u5f00\u6e90LLM\u4ee5\u53ca\u57fa\u4e8eLLM\u7684\u7f51\u9875\u641c\u7d22API\u5728\u5185\u7684\u5e7f\u6cdb\u7cfb\u7edf\u3002", "conclusion": "LiveNewsBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u4e14\u53ef\u5b9a\u671f\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLM\u7684\u4ee3\u7406\u5f0f\u7f51\u9875\u641c\u7d22\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.13573", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13573", "abs": "https://arxiv.org/abs/2602.13573", "authors": ["Ming Xia", "Zhiqin Zhou", "Guoxin Ma", "Dongmin Huang"], "title": "Unleash the Potential of Long Semantic IDs for Generative Recommendation", "comment": "14 pages, 12 figures, conference", "summary": "Semantic ID-based generative recommendation represents items as sequences of discrete tokens, but it inherently faces a trade-off between representational expressiveness and computational efficiency. Residual Quantization (RQ)-based approaches restrict semantic IDs to be short to enable tractable sequential modeling, while Optimized Product Quantization (OPQ)-based methods compress long semantic IDs through naive rigid aggregation, inevitably discarding fine-grained semantic information. To resolve this dilemma, we propose ACERec, a novel framework that decouples the granularity gap between fine-grained tokenization and efficient sequential modeling. It employs an Attentive Token Merger to distill long expressive semantic tokens into compact latents and introduces a dedicated Intent Token serving as a dynamic prediction anchor. To capture cohesive user intents, we guide the learning process via a dual-granularity objective, harmonizing fine-grained token prediction with global item-level semantic alignment. Extensive experiments on six real-world benchmarks demonstrate that ACERec consistently outperforms state-of-the-art baselines, achieving an average improvement of 14.40\\% in NDCG@10, effectively reconciling semantic expressiveness and computational efficiency.", "AI": {"tldr": "ACERec\u901a\u8fc7\u89e3\u8026\u7ec6\u7c92\u5ea6\u6807\u8bb0\u5316\u4e0e\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u7684\u7c92\u5ea6\u5dee\u8ddd\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u6807\u8bb0\u5408\u5e76\u5668\u548c\u610f\u56fe\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u8868\u8fbe\u529b\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u4e49ID\u751f\u6210\u63a8\u8350\u65b9\u6cd5\u9762\u4e34\u8868\u8fbe\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\uff1a\u57fa\u4e8e\u6b8b\u5dee\u91cf\u5316\u7684\u65b9\u6cd5\u9650\u5236\u8bed\u4e49ID\u957f\u5ea6\u4ee5\u4fdd\u8bc1\u5e8f\u5217\u5efa\u6a21\u53ef\u884c\u6027\uff0c\u800c\u57fa\u4e8e\u4f18\u5316\u4e58\u79ef\u91cf\u5316\u7684\u65b9\u6cd5\u901a\u8fc7\u7b80\u5355\u805a\u5408\u538b\u7f29\u957f\u8bed\u4e49ID\uff0c\u4f46\u4f1a\u4e22\u5931\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51faACERec\u6846\u67b6\uff1a1) \u4f7f\u7528\u6ce8\u610f\u529b\u6807\u8bb0\u5408\u5e76\u5668\u5c06\u957f\u8868\u8fbe\u6027\u8bed\u4e49\u6807\u8bb0\u84b8\u998f\u4e3a\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff1b2) \u5f15\u5165\u4e13\u95e8\u7684\u610f\u56fe\u6807\u8bb0\u4f5c\u4e3a\u52a8\u6001\u9884\u6d4b\u951a\u70b9\uff1b3) \u901a\u8fc7\u53cc\u7c92\u5ea6\u76ee\u6807\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u534f\u8c03\u7ec6\u7c92\u5ea6\u6807\u8bb0\u9884\u6d4b\u4e0e\u5168\u5c40\u9879\u76ee\u7ea7\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cACERec\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cNDCG@10\u5e73\u5747\u63d0\u534714.40%\uff0c\u6709\u6548\u8c03\u548c\u4e86\u8bed\u4e49\u8868\u8fbe\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "ACERec\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u4e49ID\u751f\u6210\u63a8\u8350\u4e2d\u8868\u8fbe\u529b\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u7c92\u5ea6\u5dee\u8ddd\u3001\u6ce8\u610f\u529b\u6807\u8bb0\u5408\u5e76\u548c\u53cc\u7c92\u5ea6\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2602.13581", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13581", "abs": "https://arxiv.org/abs/2602.13581", "authors": ["Da Guo", "Shijia Wang", "Qiang Xiao", "Yintao Ren", "Weisheng Li", "Songpei Xu", "Ming Yue", "Bin Huang", "Guanlin Wu", "Chuanjiang Luo"], "title": "Climber-Pilot: A Non-Myopic Generative Recommendation Model Towards Better Instruction-Following", "comment": null, "summary": "Generative retrieval has emerged as a promising paradigm in recommender systems, offering superior sequence modeling capabilities over traditional dual-tower architectures. However, in large-scale industrial scenarios, such models often suffer from inherent myopia: due to single-step inference and strict latency constraints, they tend to collapse diverse user intents into locally optimal predictions, failing to capture long-horizon and multi-item consumption patterns. Moreover, real-world retrieval systems must follow explicit retrieval instructions, such as category-level control and policy constraints. Incorporating such instruction-following behavior into generative retrieval remains challenging, as existing conditioning or post-hoc filtering approaches often compromise relevance or efficiency. In this work, we present Climber-Pilot, a unified generative retrieval framework to address both limitations. First, we introduce Time-Aware Multi-Item Prediction (TAMIP), a novel training paradigm designed to mitigate inherent myopia in generative retrieval. By distilling long-horizon, multi-item foresight into model parameters through time-aware masking, TAMIP alleviates locally optimal predictions while preserving efficient single-step inference. Second, to support flexible instruction-following retrieval, we propose Condition-Guided Sparse Attention (CGSA), which incorporates business constraints directly into the generative process via sparse attention, without introducing additional inference steps. Extensive offline experiments and online A/B testing at NetEase Cloud Music, one of the largest music streaming platforms, demonstrate that Climber-Pilot significantly outperforms state-of-the-art baselines, achieving a 4.24\\% lift of the core business metric.", "AI": {"tldr": "Climber-Pilot\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u591a\u9879\u76ee\u9884\u6d4b\u7f13\u89e3\u751f\u6210\u5f0f\u68c0\u7d22\u7684\u77ed\u89c6\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u5f15\u5bfc\u7a00\u758f\u6ce8\u610f\u529b\u5b9e\u73b0\u7075\u6d3b\u7684\u6307\u4ee4\u8ddf\u968f\u68c0\u7d22\uff0c\u5728\u7f51\u6613\u4e91\u97f3\u4e50\u5e73\u53f0\u663e\u8457\u63d0\u5347\u4e1a\u52a1\u6307\u6807\u3002", "motivation": "\u751f\u6210\u5f0f\u68c0\u7d22\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u4f18\u52bf\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u7531\u4e8e\u5355\u6b65\u63a8\u7406\u548c\u4e25\u683c\u5ef6\u8fdf\u9650\u5236\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u591a\u6837\u7528\u6237\u610f\u56fe\u574d\u7f29\u4e3a\u5c40\u90e8\u6700\u4f18\u9884\u6d4b\uff0c\u65e0\u6cd5\u6355\u6349\u957f\u671f\u548c\u591a\u9879\u76ee\u6d88\u8d39\u6a21\u5f0f\uff1b2) \u5b9e\u9645\u68c0\u7d22\u7cfb\u7edf\u9700\u8981\u9075\u5faa\u660e\u786e\u7684\u68c0\u7d22\u6307\u4ee4\uff08\u5982\u7c7b\u522b\u7ea7\u63a7\u5236\u548c\u7b56\u7565\u7ea6\u675f\uff09\uff0c\u73b0\u6709\u6761\u4ef6\u6216\u540e\u5904\u7406\u8fc7\u6ee4\u65b9\u6cd5\u5f80\u5f80\u635f\u5bb3\u76f8\u5173\u6027\u6216\u6548\u7387\u3002", "method": "\u63d0\u51faClimber-Pilot\u7edf\u4e00\u6846\u67b6\uff1a1) \u65f6\u95f4\u611f\u77e5\u591a\u9879\u76ee\u9884\u6d4b(TAMIP)\uff1a\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u63a9\u7801\u5c06\u957f\u671f\u591a\u9879\u76ee\u524d\u77bb\u6027\u84b8\u998f\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u7f13\u89e3\u5c40\u90e8\u6700\u4f18\u9884\u6d4b\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u5355\u6b65\u63a8\u7406\uff1b2) \u6761\u4ef6\u5f15\u5bfc\u7a00\u758f\u6ce8\u610f\u529b(CGSA)\uff1a\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u5c06\u4e1a\u52a1\u7ea6\u675f\u76f4\u63a5\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u63a8\u7406\u6b65\u9aa4\u3002", "result": "\u5728\u7f51\u6613\u4e91\u97f3\u4e50\u5e73\u53f0\u8fdb\u884c\u5927\u91cf\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\uff0cClimber-Pilot\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6838\u5fc3\u4e1a\u52a1\u6307\u6807\u63d0\u53474.24%\u3002", "conclusion": "Climber-Pilot\u6210\u529f\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u68c0\u7d22\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u77ed\u89c6\u95ee\u9898\u548c\u6307\u4ee4\u8ddf\u968f\u6311\u6218\uff0c\u901a\u8fc7TAMIP\u548cCGSA\u7684\u521b\u65b0\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u63d0\u5347\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.13631", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13631", "abs": "https://arxiv.org/abs/2602.13631", "authors": ["Yu Zhou", "Chengcheng Guo", "Kuo Cai", "Ji Liu", "Qiang Luo", "Ruiming Tang", "Han Li", "Kun Gai", "Guorui Zhou"], "title": "GEMs: Breaking the Long-Sequence Barrier in Generative Recommendation with a Multi-Stream Decoder", "comment": null, "summary": "While generative recommendations (GR) possess strong sequential reasoning capabilities, they face significant challenges when processing extremely long user behavior sequences: the high computational cost forces practical sequence lengths to be limited, preventing models from capturing users' lifelong interests; meanwhile, the inherent \"recency bias\" of attention mechanisms further weakens learning from long-term history. To overcome this bottleneck, we propose GEMs (Generative rEcommendation with a Multi-stream decoder), a novel and unified framework designed to break the long-sequence barrier by capturing users' lifelong interaction sequences through a multi-stream perspective. Specifically, GEMs partitions user behaviors into three temporal streams$\\unicode{x2014}$Recent, Mid-term, and Lifecycle$\\unicode{x2014}$and employs tailored inference schemes for each: a one-stage real-time extractor for immediate dynamics, a lightweight indexer for cross attention to balance accuracy and cost for mid-term sequences, and a two-stage offline-online compression module for lifelong modeling. These streams are integrated via a parameter-free fusion strategy to enable holistic interest representation. Extensive experiments on large-scale industrial datasets demonstrate that GEMs significantly outperforms state-of-the-art methods in recommendation accuracy. Notably, GEMs is the first lifelong GR framework successfully deployed in a high-concurrency industrial environment, achieving superior inference efficiency while processing user sequences of over 100,000 interactions.", "AI": {"tldr": "GEMs\u63d0\u51fa\u591a\u6d41\u89e3\u7801\u5668\u6846\u67b6\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u957f\u5e8f\u5217\u5904\u7406\u96be\u9898\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u884c\u4e3a\u5206\u4e3a\u8fd1\u671f\u3001\u4e2d\u671f\u548c\u751f\u547d\u5468\u671f\u4e09\u4e2a\u65f6\u95f4\u6d41\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u63a8\u7406\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u8d8510\u4e07\u4ea4\u4e92\u5e8f\u5217\u7684\u7ec8\u8eab\u5174\u8da3\u5efa\u6a21\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u5e8f\u5217\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5904\u7406\u6781\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u9ad8\u8ba1\u7b97\u6210\u672c\u8feb\u4f7f\u5b9e\u9645\u5e8f\u5217\u957f\u5ea6\u53d7\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u7ec8\u8eab\u5174\u8da3\uff1b2\uff09\u6ce8\u610f\u529b\u673a\u5236\u56fa\u6709\u7684\"\u8fd1\u671f\u504f\u597d\"\u8fdb\u4e00\u6b65\u524a\u5f31\u4e86\u4ece\u957f\u671f\u5386\u53f2\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "GEMs\u5c06\u7528\u6237\u884c\u4e3a\u5212\u5206\u4e3a\u4e09\u4e2a\u65f6\u95f4\u6d41\uff1a\u8fd1\u671f\u6d41\u3001\u4e2d\u671f\u6d41\u548c\u751f\u547d\u5468\u671f\u6d41\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6d41\u8bbe\u8ba1\u5b9a\u5236\u5316\u63a8\u7406\u65b9\u6848\uff1a\u8fd1\u671f\u6d41\u4f7f\u7528\u5355\u9636\u6bb5\u5b9e\u65f6\u63d0\u53d6\u5668\u5904\u7406\u5373\u65f6\u52a8\u6001\uff1b\u4e2d\u671f\u6d41\u91c7\u7528\u8f7b\u91cf\u7ea7\u7d22\u5f15\u5668\u8fdb\u884c\u8de8\u6ce8\u610f\u529b\u8ba1\u7b97\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u6210\u672c\uff1b\u751f\u547d\u5468\u671f\u6d41\u4f7f\u7528\u4e24\u9636\u6bb5\u79bb\u7ebf-\u5728\u7ebf\u538b\u7f29\u6a21\u5757\u8fdb\u884c\u7ec8\u8eab\u5efa\u6a21\u3002\u8fd9\u4e9b\u6d41\u901a\u8fc7\u53c2\u6570\u65e0\u5173\u7684\u878d\u5408\u7b56\u7565\u6574\u5408\uff0c\u5b9e\u73b0\u6574\u4f53\u5174\u8da3\u8868\u793a\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGEMs\u5728\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002GEMs\u662f\u9996\u4e2a\u6210\u529f\u90e8\u7f72\u5728\u9ad8\u5e76\u53d1\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u7ec8\u8eab\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u5728\u5904\u7406\u8d85\u8fc710\u4e07\u4ea4\u4e92\u7684\u7528\u6237\u5e8f\u5217\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u63a8\u7406\u6548\u7387\u3002", "conclusion": "GEMs\u901a\u8fc7\u591a\u6d41\u89e3\u7801\u5668\u6846\u67b6\u6210\u529f\u7a81\u7834\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u957f\u5e8f\u5217\u5904\u7406\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7528\u6237\u7ec8\u8eab\u5174\u8da3\u7684\u9ad8\u6548\u5efa\u6a21\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13647", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13647", "abs": "https://arxiv.org/abs/2602.13647", "authors": ["Rui Yu", "Tianyi Wang", "Ruixia Liu", "Yinglong Wang"], "title": "PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers", "comment": null, "summary": "Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.", "AI": {"tldr": "PT-RAG\u662f\u4e00\u4e2a\u9488\u5bf9\u5b66\u672f\u8bba\u6587\u95ee\u7b54\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u8bba\u6587\u539f\u751f\u5c42\u6b21\u7ed3\u6784\u6765\u964d\u4f4e\u68c0\u7d22\u71b5\uff0c\u63d0\u9ad8\u8bc1\u636e\u5206\u914d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u5b66\u672f\u8bba\u6587\u65f6\uff0c\u901a\u5e38\u5c06\u8bba\u6587\u6241\u5e73\u5316\u4e3a\u975e\u7ed3\u6784\u5316\u5757\uff0c\u7834\u574f\u4e86\u539f\u751f\u5c42\u6b21\u7ed3\u6784\u3002\u8fd9\u5bfc\u81f4\u68c0\u7d22\u5728\u65e0\u5e8f\u7a7a\u95f4\u4e2d\u8fdb\u884c\uff0c\u4ea7\u751f\u788e\u7247\u5316\u4e0a\u4e0b\u6587\uff0c\u5728\u6709\u9650token\u9884\u7b97\u4e0b\u5c06token\u5206\u914d\u7ed9\u975e\u8bc1\u636e\u533a\u57df\uff0c\u589e\u52a0\u4e86\u4e0b\u6e38\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8d1f\u62c5\u3002", "method": "PT-RAG\u5c06\u5b66\u672f\u8bba\u6587\u7684\u539f\u751f\u5c42\u6b21\u7ed3\u6784\u4f5c\u4e3a\u4f4e\u71b5\u68c0\u7d22\u5148\u9a8c\u3002\u9996\u5148\u7ee7\u627f\u539f\u751f\u5c42\u6b21\u6784\u5efa\u7ed3\u6784\u4fdd\u771f\u7684PaperTree\u7d22\u5f15\uff0c\u9632\u6b62\u6e90\u5934\u7684\u71b5\u589e\u52a0\uff1b\u7136\u540e\u8bbe\u8ba1\u8def\u5f84\u5f15\u5bfc\u7684\u68c0\u7d22\u673a\u5236\uff0c\u5c06\u67e5\u8be2\u8bed\u4e49\u5bf9\u9f50\u5230\u76f8\u5173\u7ae0\u8282\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u9009\u62e9\u9ad8\u76f8\u5173\u6027\u7684\u6839\u5230\u53f6\u8def\u5f84\uff0c\u4ea7\u751f\u7d27\u51d1\u3001\u8fde\u8d2f\u3001\u4f4e\u71b5\u7684\u68c0\u7d22\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u4e09\u4e2a\u5b66\u672f\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPT-RAG\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7ae0\u8282\u71b5\u548c\u8bc1\u636e\u5bf9\u9f50\u4ea4\u53c9\u71b5\uff0c\u8868\u660e\u51cf\u5c11\u4e86\u4e0a\u4e0b\u6587\u788e\u7247\u5316\uff0c\u66f4\u7cbe\u786e\u5730\u5c06token\u5206\u914d\u7ed9\u8bc1\u636e\u533a\u57df\u3002\u8fd9\u4e9b\u7ed3\u6784\u4f18\u52bf\u76f4\u63a5\u8f6c\u5316\u4e3a\u66f4\u9ad8\u7684\u7b54\u6848\u8d28\u91cf\u3002", "conclusion": "PT-RAG\u901a\u8fc7\u4fdd\u7559\u5b66\u672f\u8bba\u6587\u7684\u539f\u751f\u5c42\u6b21\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u7834\u574f\u6027\u9884\u5904\u7406\u5bfc\u81f4\u7684\u71b5\u589e\u52a0\uff0c\u4e3a\u540e\u7eed\u68c0\u7d22\u63d0\u4f9b\u4e86\u539f\u751f\u4f4e\u71b5\u7ed3\u6784\u57fa\u7840\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u8bc1\u636e\u5206\u914d\u548c\u66f4\u597d\u7684\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2602.13704", "categories": ["cs.IR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.13704", "abs": "https://arxiv.org/abs/2602.13704", "authors": ["Lei Chen", "Chen Ju", "Xu Chen", "Zhicheng Wang", "Yuheng Jiao", "Hongfeng Zhan", "Zhaoyang Li", "Shihao Xu", "Zhixiang Zhao", "Tong Jia", "Jinsong Lan", "Xiaoyong Zhu", "Bo Zheng"], "title": "Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search", "comment": null, "summary": "In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.", "AI": {"tldr": "Pailitao-VL\u662f\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u641c\u7d22\u7684\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u79cd\u8303\u5f0f\u8f6c\u53d8\u89e3\u51b3\u73b0\u6709\u65b9\u6848\u7684\u4e09\u5927\u6311\u6218\uff1a\u4ece\u5bf9\u6bd4\u5b66\u4e60\u8f6c\u5411\u7edd\u5bf9ID\u8bc6\u522b\u4efb\u52a1\uff0c\u4ee5\u53ca\u4ece\u70b9\u5f0f\u8bc4\u4f30\u8f6c\u5411\u5217\u8868\u5f0f\u6bd4\u8f83\u6821\u51c6\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u68c0\u7d22\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u68c0\u7d22\u7c92\u5ea6\u4e0d\u8db3\u3001\u5bf9\u73af\u5883\u566a\u58f0\u7684\u8106\u5f31\u6027\uff0c\u4ee5\u53ca\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002\u8fd9\u4e9b\u6311\u6218\u9650\u5236\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "1. \u5d4c\u5165\u8303\u5f0f\u8f6c\u53d8\uff1a\u4ece\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u8f6c\u5411\u7edd\u5bf9ID\u8bc6\u522b\u4efb\u52a1\uff0c\u5c06\u5b9e\u4f8b\u951a\u5b9a\u5230\u7531\u6570\u5341\u4ebf\u8bed\u4e49\u539f\u578b\u5b9a\u4e49\u7684\u5168\u5c40\u4e00\u81f4\u6f5c\u5728\u7a7a\u95f4\u30022. \u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u5668\u6f14\u8fdb\uff1a\u4ece\u5b64\u7acb\u70b9\u5f0f\u8bc4\u4f30\u8f6c\u5411\u6bd4\u8f83\u6821\u51c6\u5217\u8868\u5f0f\u7b56\u7565\uff0c\u7ed3\u5408\u57fa\u4e8e\u5206\u5757\u7684\u6bd4\u8f83\u63a8\u7406\u548c\u6821\u51c6\u7684\u7edd\u5bf9\u76f8\u5173\u6027\u8bc4\u5206\u3002", "result": "\u5728\u963f\u91cc\u5df4\u5df4\u7535\u5546\u5e73\u53f0\u7684\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cPailitao-VL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4ea7\u751f\u4e86\u663e\u8457\u7684\u5546\u4e1a\u5f71\u54cd\u3002\u7cfb\u7edf\u514b\u670d\u4e86\u73b0\u6709\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u7684\u968f\u673a\u6027\u548c\u7c92\u5ea6\u74f6\u9888\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u91cd\u6392\u5e8f\u65b9\u6cd5\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u8981\u6c42\u82db\u523b\u7684\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u5148\u8fdbMLLM\uff08\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff09\u68c0\u7d22\u67b6\u6784\u7684\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.13715", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13715", "abs": "https://arxiv.org/abs/2602.13715", "authors": ["Mingyao Huang", "Qidong Liu", "Wenxuan Yang", "Moranxin Wang", "Yuqi Sun", "Haiping Zhu", "Feng Tian", "Yan Chen"], "title": "DMESR: Dual-view MLLM-based Enhancing Framework for Multimodal Sequential Recommendation", "comment": "9 pages, 4 figures", "summary": "Sequential Recommender Systems (SRS) aim to predict users' next interaction based on their historical behaviors, while still facing the challenge of data sparsity. With the rapid advancement of Multimodal Large Language Models (MLLMs), leveraging their multimodal understanding capabilities to enrich item semantic representation has emerged as an effective enhancement strategy for SRS. However, existing MLLM-enhanced recommendation methods still suffer from two key limitations. First, they struggle to effectively align multimodal representations, leading to suboptimal utilization of semantic information across modalities. Second, they often overly rely on MLLM-generated content while overlooking the fine-grained semantic cues contained in the original textual data of items. To address these issues, we propose a Dual-view MLLM-based Enhancing framework for multimodal Sequential Recommendation (DMESR). For the misalignment issue, we employ a contrastive learning mechanism to align the cross-modal semantic representations generated by MLLMs. For the loss of fine-grained semantics, we introduce a cross-attention fusion module that integrates the coarse-grained semantic knowledge obtained from MLLMs with the fine-grained original textual semantics. Finally, these two fused representations can be seamlessly integrated into the downstream sequential recommendation models. Extensive experiments conducted on three real-world datasets and three popular sequential recommendation architectures demonstrate the superior effectiveness and generalizability of our proposed approach.", "AI": {"tldr": "DMESR\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408MLLM\u7c97\u7c92\u5ea6\u8bed\u4e49\u4e0e\u539f\u59cb\u6587\u672c\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u63d0\u5347\u5e8f\u5217\u63a8\u8350\u6027\u80fd", "motivation": "\u73b0\u6709MLLM\u589e\u5f3a\u7684\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u591a\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\uff1b2\uff09\u8fc7\u5ea6\u4f9d\u8d56MLLM\u751f\u6210\u5185\u5bb9\uff0c\u5ffd\u7565\u4e86\u7269\u54c1\u539f\u59cb\u6587\u672c\u6570\u636e\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7ebf\u7d22", "method": "\u63d0\u51faDMESR\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u5bf9\u9f50MLLM\u751f\u6210\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u8868\u793a\uff1b2\uff09\u5f15\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u5c06MLLM\u83b7\u53d6\u7684\u7c97\u7c92\u5ea6\u8bed\u4e49\u77e5\u8bc6\u4e0e\u539f\u59cb\u6587\u672c\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u76f8\u7ed3\u5408\uff1b3\uff09\u5c06\u878d\u5408\u540e\u7684\u8868\u793a\u65e0\u7f1d\u96c6\u6210\u5230\u4e0b\u6e38\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u4e2d", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e09\u79cd\u6d41\u884c\u5e8f\u5217\u63a8\u8350\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "DMESR\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5bf9\u9f50\u591a\u6a21\u6001\u8868\u793a\u5e76\u878d\u5408\u7c97\u7c92\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MLLM\u589e\u5f3a\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027"}}
{"id": "2602.13830", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.13830", "abs": "https://arxiv.org/abs/2602.13830", "authors": ["Zhuofan Shi", "Ming Ma", "Zekun Yao", "Fangkai Yang", "Jue Zhang", "Dongge Han", "Victor R\u00fchle", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "A Tale of Two Graphs: Separating Knowledge Exploration from Outline Structure for Open-Ended Deep Research", "comment": "26 pages, 4 figures", "summary": "Open-Ended Deep Research (OEDR) pushes LLM agents beyond short-form QA toward long-horizon workflows that iteratively search, connect, and synthesize evidence into structured reports. However, existing OEDR agents largely follow either linear ``search-then-generate'' accumulation or outline-centric planning. The former suffers from lost-in-the-middle failures as evidence grows, while the latter relies on the LLM to implicitly infer knowledge gaps from the outline alone, providing weak supervision for identifying missing relations and triggering targeted exploration. We present DualGraph memory, an architecture that separates what the agent knows from how it writes. DualGraph maintains two co-evolving graphs: an Outline Graph (OG), and a Knowledge Graph (KG), a semantic memory that stores fine-grained knowledge units, including core entities, concepts, and their relations. By analyzing the KG topology together with structural signals from the OG, DualGraph generates targeted search queries, enabling more efficient and comprehensive iterative knowledge-driven exploration and refinement. Across DeepResearch Bench, DeepResearchGym, and DeepConsult, DualGraph consistently outperforms state-of-the-art baselines in report depth, breadth, and factual grounding; for example, it reaches a 53.08 RACE score on DeepResearch Bench with GPT-5. Moreover, ablation studies confirm the central role of the dual-graph design.", "AI": {"tldr": "DualGraph\u8bb0\u5fc6\u67b6\u6784\u901a\u8fc7\u5206\u79bb\u77e5\u8bc6\u5b58\u50a8\u4e0e\u5199\u4f5c\u8fc7\u7a0b\uff0c\u4f7f\u7528\u77e5\u8bc6\u56fe\u548c\u63d0\u7eb2\u56fe\u534f\u540c\u6f14\u5316\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u6df1\u5ea6\u7814\u7a76\u4e2d\u77e5\u8bc6\u79ef\u7d2f\u4e0e\u63a2\u7d22\u6548\u7387\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u6df1\u5ea6\u7814\u7a76(OEDR)\u4ee3\u7406\u4e3b\u8981\u91c7\u7528\u7ebf\u6027\"\u641c\u7d22-\u751f\u6210\"\u6216\u63d0\u7eb2\u4e2d\u5fc3\u89c4\u5212\u65b9\u6cd5\uff0c\u524d\u8005\u5728\u8bc1\u636e\u589e\u957f\u65f6\u51fa\u73b0\"\u8ff7\u5931\u4e2d\u95f4\"\u95ee\u9898\uff0c\u540e\u8005\u4ec5\u4f9d\u8d56\u63d0\u7eb2\u9690\u542b\u63a8\u65ad\u77e5\u8bc6\u7f3a\u53e3\uff0c\u5bfc\u81f4\u5173\u7cfb\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u63a2\u7d22\u7684\u76d1\u7763\u8f83\u5f31\u3002", "method": "\u63d0\u51faDualGraph\u8bb0\u5fc6\u67b6\u6784\uff0c\u5206\u79bb\u4ee3\u7406\u6240\u77e5\u4e0e\u5199\u4f5c\u8fc7\u7a0b\uff0c\u7ef4\u62a4\u4e24\u4e2a\u534f\u540c\u6f14\u5316\u7684\u56fe\uff1a\u63d0\u7eb2\u56fe(OG)\u548c\u77e5\u8bc6\u56fe(KG)\u3002\u77e5\u8bc6\u56fe\u5b58\u50a8\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u5355\u5143\uff08\u6838\u5fc3\u5b9e\u4f53\u3001\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\uff09\uff0c\u901a\u8fc7\u5206\u6790\u77e5\u8bc6\u56fe\u62d3\u6251\u548c\u63d0\u7eb2\u56fe\u7ed3\u6784\u4fe1\u53f7\uff0c\u751f\u6210\u9488\u5bf9\u6027\u641c\u7d22\u67e5\u8be2\u3002", "result": "\u5728DeepResearch Bench\u3001DeepResearchGym\u548cDeepConsult\u4e09\u4e2a\u57fa\u51c6\u4e0a\uff0cDualGraph\u5728\u62a5\u544a\u6df1\u5ea6\u3001\u5e7f\u5ea6\u548c\u4e8b\u5b9e\u57fa\u7840\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff1b\u5728DeepResearch Bench\u4e0a\u4f7f\u7528GPT-5\u8fbe\u523053.08 RACE\u5206\u6570\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u53cc\u56fe\u8bbe\u8ba1\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "conclusion": "DualGraph\u67b6\u6784\u901a\u8fc7\u5206\u79bb\u77e5\u8bc6\u5b58\u50a8\u4e0e\u5199\u4f5c\u8fc7\u7a0b\uff0c\u4f7f\u7528\u534f\u540c\u6f14\u5316\u7684\u77e5\u8bc6\u56fe\u548c\u63d0\u7eb2\u56fe\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u5168\u9762\u7684\u8fed\u4ee3\u77e5\u8bc6\u9a71\u52a8\u63a2\u7d22\u548c\u7cbe\u70bc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2602.13971", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.13971", "abs": "https://arxiv.org/abs/2602.13971", "authors": ["Zhihao Lv", "Longtao Zhang", "Ailong He", "Shuzhi Cao", "Shuguang Han", "Jufeng Chen"], "title": "DAIAN: Deep Adaptive Intent-Aware Network for CTR Prediction in Trigger-Induced Recommendation", "comment": null, "summary": "Recommendation systems are essential for personalizing e-commerce shopping experiences. Among these, Trigger-Induced Recommendation (TIR) has emerged as a key scenario, which utilizes a trigger item (explicitly represents a user's instantaneous interest), enabling precise, real-time recommendations. Although several trigger-based techniques have been proposed, most of them struggle to address the intent myopia issue, that is, a recommendation system overemphasizes the role of trigger items and narrowly focuses on suggesting commodities that are highly relevant to trigger items. Meanwhile, existing methods rely on collaborative behavior patterns between trigger and recommended items to identify the user's preferences, yet the sparsity of ID-based interaction restricts their effectiveness. To this end, we propose the Deep Adaptive Intent-Aware Network (DAIAN) that dynamically adapts to users' intent preferences. In general, we first extract the users' personalized intent representations by analyzing the correlation between a user's click and the trigger item, and accordingly retrieve the user's related historical behaviors to mine the user's diverse intent. Besides, sparse collaborative behaviors constrain the performance in capturing items associated with user intent. Hence, we reinforce similarity by leveraging a hybrid enhancer with ID and semantic information, followed by adaptive selection based on varying intents. Experimental results on public datasets and our industrial e-commerce datasets demonstrate the effectiveness of DAIAN.", "AI": {"tldr": "\u63d0\u51faDAIAN\u6a21\u578b\u89e3\u51b3\u89e6\u53d1\u63a8\u8350\u4e2d\u7684\u610f\u56fe\u77ed\u89c6\u95ee\u9898\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u610f\u56fe\u8868\u793a\u548c\u6df7\u5408\u589e\u5f3a\u5668\u52a8\u6001\u9002\u5e94\u7528\u6237\u610f\u56fe\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u89e6\u53d1\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u610f\u56fe\u77ed\u89c6\u95ee\u9898\uff0c\u8fc7\u5ea6\u5f3a\u8c03\u89e6\u53d1\u9879\u4f5c\u7528\uff0c\u4e14\u57fa\u4e8eID\u7684\u534f\u540c\u884c\u4e3a\u7a00\u758f\u6027\u9650\u5236\u4e86\u6027\u80fd\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u610f\u56fe\u611f\u77e5\u63a8\u8350\u3002", "method": "DAIAN\u6a21\u578b\uff1a1) \u901a\u8fc7\u5206\u6790\u7528\u6237\u70b9\u51fb\u4e0e\u89e6\u53d1\u9879\u76f8\u5173\u6027\u63d0\u53d6\u4e2a\u6027\u5316\u610f\u56fe\u8868\u793a\uff1b2) \u68c0\u7d22\u76f8\u5173\u5386\u53f2\u884c\u4e3a\u6316\u6398\u591a\u6837\u5316\u610f\u56fe\uff1b3) \u4f7f\u7528ID\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u6df7\u5408\u589e\u5f3a\u5668\u5f3a\u5316\u76f8\u4f3c\u6027\uff1b4) \u57fa\u4e8e\u4e0d\u540c\u610f\u56fe\u8fdb\u884c\u81ea\u9002\u5e94\u9009\u62e9\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u7535\u5546\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86DAIAN\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u7528\u6237\u610f\u56fe\u504f\u597d\u3002", "conclusion": "DAIAN\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u7528\u6237\u610f\u56fe\u504f\u597d\uff0c\u89e3\u51b3\u4e86\u89e6\u53d1\u63a8\u8350\u4e2d\u7684\u610f\u56fe\u77ed\u89c6\u95ee\u9898\uff0c\u7ed3\u5408ID\u548c\u8bed\u4e49\u4fe1\u606f\u589e\u5f3a\u4e86\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2602.14110", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14110", "abs": "https://arxiv.org/abs/2602.14110", "authors": ["Xu Huang", "Hao Zhang", "Zhifang Fan", "Yunwen Huang", "Zhuoxing Wei", "Zheng Chai", "Jinan Ni", "Yuchao Zheng", "Qiwei Chen"], "title": "MixFormer: Co-Scaling Up Dense and Sequence in Industrial Recommenders", "comment": null, "summary": "As industrial recommender systems enter a scaling-driven regime, Transformer architectures have become increasingly attractive for scaling models towards larger capacity and longer sequence. However, existing Transformer-based recommendation models remain structurally fragmented, where sequence modeling and feature interaction are implemented as separate modules with independent parameterization. Such designs introduce a fundamental co-scaling challenge, as model capacity must be suboptimally allocated between dense feature interaction and sequence modeling under a limited computational budget. In this work, we propose MixFormer, a unified Transformer-style architecture tailored for recommender systems, which jointly models sequential behaviors and feature interactions within a single backbone. Through a unified parameterization, MixFormer enables effective co-scaling across both dense capacity and sequence length, mitigating the trade-off observed in decoupled designs. Moreover, the integrated architecture facilitates deep interaction between sequential and non-sequential representations, allowing high-order feature semantics to directly inform sequence aggregation and enhancing overall expressiveness. To ensure industrial practicality, we further introduce a user-item decoupling strategy for efficiency optimizations that significantly reduce redundant computation and inference latency. Extensive experiments on large-scale industrial datasets demonstrate that MixFormer consistently exhibits superior accuracy and efficiency. Furthermore, large-scale online A/B tests on two production recommender systems, Douyin and Douyin Lite, show consistent improvements in user engagement metrics, including active days and in-app usage duration.", "AI": {"tldr": "MixFormer\uff1a\u4e3a\u63a8\u8350\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7edf\u4e00Transformer\u67b6\u6784\uff0c\u5c06\u5e8f\u5217\u5efa\u6a21\u548c\u7279\u5f81\u4ea4\u4e92\u96c6\u6210\u5728\u5355\u4e00\u4e3b\u5e72\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u79bb\u8bbe\u8ba1\u7684\u534f\u540c\u7f29\u653e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u63a8\u8350\u6a21\u578b\u5b58\u5728\u7ed3\u6784\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e8f\u5217\u5efa\u6a21\u548c\u7279\u5f81\u4ea4\u4e92\u4f5c\u4e3a\u72ec\u7acb\u6a21\u5757\u5b9e\u73b0\uff0c\u5bfc\u81f4\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u6a21\u578b\u5bb9\u91cf\u5fc5\u987b\u5728\u5bc6\u96c6\u7279\u5f81\u4ea4\u4e92\u548c\u5e8f\u5217\u5efa\u6a21\u4e4b\u95f4\u8fdb\u884c\u6b21\u4f18\u5206\u914d\uff0c\u5b58\u5728\u534f\u540c\u7f29\u653e\u6311\u6218\u3002", "method": "\u63d0\u51faMixFormer\u7edf\u4e00\u67b6\u6784\uff0c\u5728\u5355\u4e00\u4e3b\u5e72\u4e2d\u8054\u5408\u5efa\u6a21\u5e8f\u5217\u884c\u4e3a\u548c\u7279\u5f81\u4ea4\u4e92\uff1b\u91c7\u7528\u7edf\u4e00\u53c2\u6570\u5316\u5b9e\u73b0\u5bc6\u96c6\u5bb9\u91cf\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u6709\u6548\u534f\u540c\u7f29\u653e\uff1b\u5f15\u5165\u7528\u6237-\u7269\u54c1\u89e3\u8026\u7b56\u7565\u8fdb\u884c\u6548\u7387\u4f18\u5316\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eMixFormer\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff1b\u5728\u6296\u97f3\u548c\u6296\u97f3\u8f7b\u91cf\u7248\u4e24\u4e2a\u751f\u4ea7\u63a8\u8350\u7cfb\u7edf\u4e0a\u7684\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0c\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\uff08\u5305\u62ec\u6d3b\u8dc3\u5929\u6570\u548c\u5e94\u7528\u5185\u4f7f\u7528\u65f6\u957f\uff09\u6301\u7eed\u6539\u5584\u3002", "conclusion": "MixFormer\u901a\u8fc7\u7edf\u4e00\u7684Transformer\u67b6\u6784\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e8f\u5217\u5efa\u6a21\u548c\u7279\u5f81\u4ea4\u4e92\u7684\u534f\u540c\u7f29\u653e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u5de5\u4e1a\u5b9e\u7528\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.14358", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.14358", "abs": "https://arxiv.org/abs/2602.14358", "authors": ["Dillon Davis", "Huiji Gao", "Thomas Legrand", "Juan Manuel Caicedo Carvajal", "Malay Haldar", "Kedar Bellare", "Moutupsi Paul", "Soumyadip Banerjee", "Liwei He", "Stephanie Moyerman", "Sanjeev Katariya"], "title": "High Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace", "comment": "KDD TSMO 2025: https://sites.google.com/view/tsmo2025/accepted-papers?authuser=0", "summary": "Airbnb search must balance a worldwide, highly varied supply of homes with guests whose location, amenity, style, and price expectations differ widely. Meeting those expectations hinges on an efficient retrieval stage that surfaces only the listings a guest might realistically book, before resource intensive ranking models are applied to determine the best results. Unlike many recommendation engines, our system faces a distinctive challenge, location retrieval, that sits upstream of ranking and determines which geographic areas are queried in order to filter inventory to a candidate set. The preexisting approach employs a deep bayesian bandit based system to predict a rectangular retrieval bounds area that can be used for filtering. The purpose of this paper is to demonstrate the methodology, challenges, and impact of rearchitecting search to retrieve from the subset of most bookable high precision rectangular map cells defined by dividing the world into 25M uniform cells.", "AI": {"tldr": "Airbnb\u91cd\u6784\u641c\u7d22\u7cfb\u7edf\uff0c\u4ece\u57fa\u4e8e\u6df1\u5ea6\u8d1d\u53f6\u65afbandit\u7684\u77e9\u5f62\u68c0\u7d22\u533a\u57df\u9884\u6d4b\uff0c\u8f6c\u5411\u4f7f\u7528\u5168\u74032500\u4e07\u4e2a\u5747\u5300\u7f51\u683c\u5355\u5143\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u68c0\u7d22", "motivation": "Airbnb\u641c\u7d22\u9762\u4e34\u72ec\u7279\u7684\u4f4d\u7f6e\u68c0\u7d22\u6311\u6218\uff1a\u9700\u8981\u5728\u8d44\u6e90\u5bc6\u96c6\u578b\u6392\u5e8f\u6a21\u578b\u4e4b\u524d\uff0c\u9ad8\u6548\u7b5b\u9009\u51fa\u7528\u6237\u53ef\u80fd\u5b9e\u9645\u9884\u8ba2\u7684\u623f\u6e90\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u8d1d\u53f6\u65afbandit\u7684\u77e9\u5f62\u533a\u57df\u68c0\u7d22\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u5339\u914d\u7528\u6237\u7684\u5730\u7406\u4f4d\u7f6e\u9700\u6c42\u3002", "method": "\u5c06\u5168\u7403\u5212\u5206\u4e3a2500\u4e07\u4e2a\u5747\u5300\u7f51\u683c\u5355\u5143\uff0c\u91cd\u65b0\u67b6\u6784\u641c\u7d22\u7cfb\u7edf\uff0c\u4ece\u6700\u53ef\u80fd\u88ab\u9884\u8ba2\u7684\u9ad8\u7cbe\u5ea6\u77e9\u5f62\u5730\u56fe\u5355\u5143\u5b50\u96c6\u4e2d\u8fdb\u884c\u68c0\u7d22\uff0c\u53d6\u4ee3\u539f\u6709\u7684\u77e9\u5f62\u8fb9\u754c\u533a\u57df\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u65b0\u67b6\u6784\u7684\u65b9\u6cd5\u8bba\u3001\u6311\u6218\u548c\u5f71\u54cd\uff0c\u901a\u8fc7\u66f4\u7cbe\u786e\u7684\u5730\u7406\u4f4d\u7f6e\u8fc7\u6ee4\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u7387\uff0c\u4e3a\u540e\u7eed\u6392\u5e8f\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u76f8\u5173\u7684\u5019\u9009\u623f\u6e90\u96c6\u5408\u3002", "conclusion": "\u91c7\u7528\u5168\u7403\u5747\u5300\u7f51\u683c\u5355\u5143\u7684\u9ad8\u7cbe\u5ea6\u68c0\u7d22\u67b6\u6784\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406Airbnb\u641c\u7d22\u4e2d\u7684\u4f4d\u7f6e\u68c0\u7d22\u6311\u6218\uff0c\u63d0\u5347\u623f\u6e90\u7b5b\u9009\u7684\u51c6\u786e\u6027\u548c\u7cfb\u7edf\u6548\u7387\u3002"}}
{"id": "2602.14502", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14502", "abs": "https://arxiv.org/abs/2602.14502", "authors": ["Chaosheng Dong", "Michinari Momma", "Yijia Wang", "Yan Gao", "Yi Sun"], "title": "Behavioral Feature Boosting via Substitute Relationships for E-commerce Search", "comment": "5 pages, 5 figures", "summary": "On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.", "AI": {"tldr": "\u63d0\u51faBFS\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u66ff\u4ee3\u4ea7\u54c1\u7684\u884c\u4e3a\u7279\u5f81\u6765\u7f13\u89e3\u7535\u5546\u5e73\u53f0\u65b0\u54c1\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u63d0\u5347\u641c\u7d22\u76f8\u5173\u6027\u548c\u4ea7\u54c1\u53d1\u73b0", "motivation": "\u7535\u5546\u5e73\u53f0\u65b0\u54c1\u9762\u4e34\u51b7\u542f\u52a8\u95ee\u9898\uff1a\u4ea4\u4e92\u6570\u636e\u6709\u9650\u5bfc\u81f4\u641c\u7d22\u53ef\u89c1\u6027\u964d\u4f4e\u548c\u76f8\u5173\u6027\u6392\u5e8f\u53d7\u635f\uff0c\u5f71\u54cd\u65b0\u54c1\u7ade\u4e89\u529b", "method": "BFS\uff08\u884c\u4e3a\u7279\u5f81\u589e\u5f3a\uff09\u65b9\u6cd5\uff1a\u8bc6\u522b\u6ee1\u8db3\u76f8\u4f3c\u7528\u6237\u9700\u6c42\u7684\u66ff\u4ee3\u4ea7\u54c1\uff0c\u805a\u5408\u5176\u70b9\u51fb\u3001\u52a0\u8d2d\u3001\u8d2d\u4e70\u3001\u8bc4\u5206\u7b49\u884c\u4e3a\u4fe1\u53f7\uff0c\u4e3a\u65b0\u54c1\u63d0\u4f9b\"\u70ed\u542f\u52a8\"\u6570\u636e\u652f\u6301", "result": "\u5728\u5927\u578b\u7535\u5546\u5e73\u53f0\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\uff0cBFS\u663e\u8457\u63d0\u5347\u4e86\u51b7\u542f\u52a8\u4ea7\u54c1\u7684\u641c\u7d22\u76f8\u5173\u6027\u548c\u4ea7\u54c1\u53d1\u73b0\u6548\u679c\uff0c\u65b9\u6cd5\u53ef\u6269\u5c55\u4e14\u5b9e\u7528", "conclusion": "BFS\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7535\u5546\u65b0\u54c1\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5df2\u6295\u5165\u751f\u4ea7\u73af\u5883\u81ea2025\u5e74\u8d77\u670d\u52a1\u5ba2\u6237\uff0c\u6539\u5584\u4e86\u7528\u6237\u4f53\u9a8c\u5e76\u589e\u52a0\u4e86\u65b0\u54c1\u66dd\u5149"}}
{"id": "2602.14706", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14706", "abs": "https://arxiv.org/abs/2602.14706", "authors": ["Zihan Li", "Gustavo Escobedo", "Marta Moscati", "Oleg Lesota", "Markus Schedl"], "title": "Adaptive Autoguidance for Item-Side Fairness in Diffusion Recommender Systems", "comment": null, "summary": "Diffusion recommender systems achieve strong recommendation accuracy but often suffer from popularity bias, resulting in unequal item exposure. To address this shortcoming, we introduce A2G-DiffRec, a diffusion recommender that incorporates adaptive autoguidance, where the main model is guided by a less-trained version of itself. Instead of using a fixed guidance weight, A2G-DiffRec learns to adaptively weigh the outputs of the main and weak models during training, supervised by a popularity regularization that promotes balanced exposure across items with different popularity levels. Experimental results on the MovieLens-1M, Foursquare-Tokyo, and Music4All-Onion datasets show that A2G-DiffRec is effective in enhancing item-side fairness at a marginal cost of accuracy reduction compared to existing guided diffusion recommenders and other non-diffusion baselines.", "AI": {"tldr": "A2G-DiffRec\u662f\u4e00\u79cd\u6269\u6563\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u81ea\u52a8\u5f15\u5bfc\u673a\u5236\u51cf\u5c11\u6d41\u884c\u5ea6\u504f\u5dee\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u9879\u76ee\u4fa7\u516c\u5e73\u6027\u3002", "motivation": "\u6269\u6563\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5b58\u5728\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u4e0d\u540c\u6d41\u884c\u5ea6\u7684\u9879\u76ee\u66dd\u5149\u4e0d\u5747\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faA2G-DiffRec\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u81ea\u52a8\u5f15\u5bfc\u673a\u5236\uff1a\u4e3b\u6a21\u578b\u7531\u4e00\u4e2a\u8bad\u7ec3\u8f83\u5f31\u7684\u81ea\u8eab\u7248\u672c\u5f15\u5bfc\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u5b66\u4e60\u5e73\u8861\u4e3b\u6a21\u578b\u548c\u5f31\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u4f7f\u7528\u6d41\u884c\u5ea6\u6b63\u5219\u5316\u76d1\u7763\u8bad\u7ec3\u4ee5\u4fc3\u8fdb\u4e0d\u540c\u6d41\u884c\u5ea6\u9879\u76ee\u7684\u5747\u8861\u66dd\u5149\u3002", "result": "\u5728MovieLens-1M\u3001Foursquare-Tokyo\u548cMusic4All-Onion\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cA2G-DiffRec\u80fd\u6709\u6548\u63d0\u5347\u9879\u76ee\u4fa7\u516c\u5e73\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u5f15\u5bfc\u6269\u6563\u63a8\u8350\u5668\u548c\u5176\u4ed6\u975e\u6269\u6563\u57fa\u7ebf\uff0c\u4ec5\u4ee5\u5fae\u5c0f\u7684\u51c6\u786e\u6027\u4e0b\u964d\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "A2G-DiffRec\u901a\u8fc7\u81ea\u9002\u5e94\u81ea\u52a8\u5f15\u5bfc\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u63a8\u8350\u7cfb\u7edf\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9879\u76ee\u4fa7\u516c\u5e73\u6027\u3002"}}
{"id": "2602.14710", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.14710", "abs": "https://arxiv.org/abs/2602.14710", "authors": ["Shaojie Jiang", "Svitlana Vakulenko", "Maarten de Rijke"], "title": "Orcheo: A Modular Full-Stack Platform for Conversational Search", "comment": "Under review at SIGIR 2026", "summary": "Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.", "AI": {"tldr": "Orcheo\u662f\u4e00\u4e2a\u5f00\u6e90\u5bf9\u8bdd\u641c\u7d22\u5e73\u53f0\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u67b6\u6784\u3001\u751f\u4ea7\u5c31\u7eea\u57fa\u7840\u8bbe\u65bd\u548c50+\u73b0\u6210\u7ec4\u4ef6\uff0c\u65e8\u5728\u89e3\u51b3\u5bf9\u8bdd\u641c\u7d22\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u548c\u96be\u4ee5\u90e8\u7f72\u7aef\u5230\u7aef\u539f\u578b\u7684\u95ee\u9898\u3002", "motivation": "\u5bf9\u8bdd\u641c\u7d22\u7814\u7a76\u9762\u4e34\u4e24\u5927\u969c\u788d\uff1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u9ad8\u6548\u5171\u4eab\u7814\u7a76\u6210\u679c\uff0c\u4ee5\u53ca\u96be\u4ee5\u90e8\u7f72\u7528\u4e8e\u7528\u6237\u8bc4\u4f30\u7684\u7aef\u5230\u7aef\u539f\u578b\u3002\u8fd9\u963b\u788d\u4e86\u793e\u533a\u8d21\u732e\u7684\u5206\u4eab\u548c\u5b9e\u9645\u7cfb\u7edf\u7684\u9a8c\u8bc1\u3002", "method": "Orcheo\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u6587\u4ef6\u8282\u70b9\u6a21\u5757\u4fc3\u8fdb\u7ec4\u4ef6\u91cd\u7528\uff1b\u63d0\u4f9b\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u53cc\u6267\u884c\u6a21\u5f0f\u3001\u5b89\u5168\u51ed\u8bc1\u7ba1\u7406\u548c\u6267\u884c\u9065\u6d4b\uff1b\u5185\u7f6eAI\u7f16\u7801\u652f\u6301\u964d\u4f4e\u5b66\u4e60\u66f2\u7ebf\uff1b\u63d0\u4f9b\u5305\u542b50+\u73b0\u6210\u7ec4\u4ef6\u7684\u5165\u95e8\u5957\u4ef6\u3002", "result": "Orcheo\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\u53d1\u5e03\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6a21\u5757\u6027\u548c\u6613\u7528\u6027\u3002\u5e73\u53f0\u5df2\u5728GitHub\u4e0a\u4ee5MIT\u8bb8\u53ef\u8bc1\u5f00\u6e90\uff0c\u5730\u5740\u4e3ahttps://github.com/ShaojieJiang/orcheo\u3002", "conclusion": "Orcheo\u6210\u529f\u586b\u8865\u4e86\u5bf9\u8bdd\u641c\u7d22\u7814\u7a76\u4e2d\u7684\u6846\u67b6\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u7684\u5e73\u53f0\u4fc3\u8fdb\u4e86\u7ec4\u4ef6\u5171\u4eab\u3001\u7814\u7a76\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u4ece\u539f\u578b\u5230\u751f\u4ea7\u7cfb\u7edf\u7684\u90e8\u7f72\u96be\u5ea6\u3002"}}
{"id": "2602.14784", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14784", "abs": "https://arxiv.org/abs/2602.14784", "authors": ["Christos Koutsiaris"], "title": "Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs", "comment": "8 pages, 4 figures. Code available at https://github.com/unseen1980/IDC", "summary": "Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.", "AI": {"tldr": "IDC\u4f7f\u7528\u9884\u6d4b\u7684\u7528\u6237\u67e5\u8be2\u610f\u56fe\u6307\u5bfc\u6587\u6863\u5206\u5757\uff0c\u901a\u8fc7LLM\u751f\u6210\u610f\u56fe\u548c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5bfb\u627e\u6700\u4f18\u5206\u5757\u8fb9\u754c\uff0c\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6587\u6863\u5206\u5757\u65b9\u6cd5\uff08\u5982\u56fa\u5b9a\u957f\u5ea6\u6216\u57fa\u4e8e\u8fde\u8d2f\u6027\u7684\u5206\u5757\uff09\u5ffd\u7565\u4e86\u7528\u6237\u610f\u56fe\uff0c\u5bfc\u81f4\u5206\u5757\u53ef\u80fd\u5206\u5272\u7b54\u6848\u6216\u5305\u542b\u65e0\u5173\u566a\u58f0\uff0c\u5f71\u54cd\u4fe1\u606f\u68c0\u7d22\u6548\u679c\u3002", "method": "\u63d0\u51fa\u610f\u56fe\u9a71\u52a8\u7684\u52a8\u6001\u5206\u5757\uff08IDC\uff09\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u6587\u6863\u53ef\u80fd\u7684\u7528\u6237\u610f\u56fe/\u67e5\u8be2\uff1b2\uff09\u91c7\u7528\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5bfb\u627e\u5168\u5c40\u6700\u4f18\u7684\u5206\u5757\u8fb9\u754c\uff0c\u907f\u514d\u8d2a\u5a6a\u7b97\u6cd5\u7684\u7f3a\u9677\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff08\u65b0\u95fb\u3001\u7ef4\u57fa\u767e\u79d1\u3001\u5b66\u672f\u8bba\u6587\u3001\u6280\u672f\u6587\u6863\uff09\u4e0a\u8bc4\u4f30\uff0cIDC\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5206\u5757\u7b56\u7565\uff0ctop-1\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u53475%-67%\uff0c\u5728\u7b2c\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e\u6700\u4f73\u57fa\u7ebf\u6301\u5e73\u3002\u540c\u65f6\u4ea7\u751f\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c1140-60%\u7684\u5206\u5757\uff0c\u8fbe\u523093-100%\u7684\u7b54\u6848\u8986\u76d6\u7387\u3002", "conclusion": "\u5c06\u6587\u6863\u7ed3\u6784\u4e0e\u9884\u671f\u7684\u4fe1\u606f\u9700\u6c42\u5bf9\u9f50\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u4e14\u5f02\u6784\u7684\u6587\u6863\u3002IDC\u5c55\u793a\u4e86\u610f\u56fe\u611f\u77e5\u5206\u5757\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.14793", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14793", "abs": "https://arxiv.org/abs/2602.14793", "authors": ["Leslie D. McIntosh", "Alexandra Sinclair", "Simon Linacre"], "title": "Beyond Retractions: Forensic Scientometrics Techniques to Identify Research Misconduct, Citation Leakage, and Funding Anomalies", "comment": null, "summary": "This paper presents a forensic scientometric case study of the Pharmakon Neuroscience Research Network, a fabricated research collective that operated primarily between 2019 and 2022 while embedding itself within legitimate scholarly publishing channels.", "AI": {"tldr": "\u5bf9\u865a\u6784\u7684Pharmakon\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7f51\u7edc\u8fdb\u884c\u6cd5\u8bc1\u79d1\u5b66\u8ba1\u91cf\u5b66\u6848\u4f8b\u5206\u6790\uff0c\u8be5\u7f51\u7edc\u57282019-2022\u5e74\u95f4\u901a\u8fc7\u5b66\u672f\u51fa\u7248\u6e20\u9053\u8fd0\u4f5c", "motivation": "\u7814\u7a76\u865a\u6784\u7814\u7a76\u7f51\u7edc\u5982\u4f55\u5d4c\u5165\u5408\u6cd5\u5b66\u672f\u51fa\u7248\u6e20\u9053\uff0c\u63ed\u793a\u5b66\u672f\u51fa\u7248\u7cfb\u7edf\u4e2d\u7684\u6f0f\u6d1e\u548c\u6f5c\u5728\u98ce\u9669", "method": "\u91c7\u7528\u6cd5\u8bc1\u79d1\u5b66\u8ba1\u91cf\u5b66\u65b9\u6cd5\uff0c\u5bf9\u865a\u6784\u7684Pharmakon\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7f51\u7edc\u8fdb\u884c\u6848\u4f8b\u7814\u7a76", "result": "\u63ed\u793a\u4e86\u865a\u6784\u7814\u7a76\u7f51\u7edc\u5982\u4f55\u57282019-2022\u5e74\u95f4\u6210\u529f\u5d4c\u5165\u5b66\u672f\u51fa\u7248\u7cfb\u7edf\uff0c\u66b4\u9732\u4e86\u51fa\u7248\u6e20\u9053\u7684\u8106\u5f31\u6027", "conclusion": "\u5b66\u672f\u51fa\u7248\u7cfb\u7edf\u5b58\u5728\u88ab\u865a\u6784\u7814\u7a76\u7f51\u7edc\u5229\u7528\u7684\u6f0f\u6d1e\uff0c\u9700\u8981\u52a0\u5f3a\u5ba1\u67e5\u673a\u5236\u548c\u9a8c\u8bc1\u6d41\u7a0b"}}
{"id": "2602.14960", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.14960", "abs": "https://arxiv.org/abs/2602.14960", "authors": ["Pranav Kasela", "Marco Braga", "Ophir Frieder", "Nazli Goharian", "Gabriella Pasi", "Raffaele Perego"], "title": "DRAMA: Domain Retrieval using Adaptive Module Allocation", "comment": null, "summary": "Neural models are increasingly used in Web-scale Information Retrieval (IR). However, relying on these models introduces substantial computational and energy requirements, leading to increasing attention toward their environmental cost and the sustainability of large-scale deployments. While neural IR models deliver high retrieval effectiveness, their scalability is constrained in multi-domain scenarios, where training and maintaining domain-specific models is inefficient and achieving robust cross-domain generalisation within a unified model remains difficult. This paper introduces DRAMA (Domain Retrieval using Adaptive Module Allocation), an energy- and parameter-efficient framework designed to reduce the environmental footprint of neural retrieval. DRAMA integrates domain-specific adapter modules with a dynamic gating mechanism that selects the most relevant domain knowledge for each query. New domains can be added efficiently through lightweight adapter training, avoiding full model retraining. We evaluate DRAMA on multiple Web retrieval benchmarks covering different domains. Our extensive evaluation shows that DRAMA achieves comparable effectiveness to domain-specific models while using only a fraction of their parameters and computational resources. These findings show that energy-aware model design can significantly improve scalability and sustainability in neural IR.", "AI": {"tldr": "DRAMA\u662f\u4e00\u4e2a\u53c2\u6570\u548c\u80fd\u8017\u9ad8\u6548\u7684\u795e\u7ecf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u95e8\u63a7\u673a\u5236\u9009\u62e9\u7279\u5b9a\u9886\u57df\u9002\u914d\u5668\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u548c\u53c2\u6570\u9700\u6c42\u3002", "motivation": "\u795e\u7ecf\u68c0\u7d22\u6a21\u578b\u867d\u7136\u6548\u679c\u4f18\u79c0\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u5b58\u5728\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u6210\u672c\uff0c\u7279\u522b\u662f\u5728\u591a\u9886\u57df\u573a\u666f\u4e0b\uff0c\u8bad\u7ec3\u548c\u7ef4\u62a4\u9886\u57df\u7279\u5b9a\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\uff0c\u7edf\u4e00\u7684\u8de8\u9886\u57df\u6cdb\u5316\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51faDRAMA\u6846\u67b6\uff0c\u96c6\u6210\u9886\u57df\u7279\u5b9a\u9002\u914d\u5668\u6a21\u5757\u548c\u52a8\u6001\u95e8\u63a7\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u9009\u62e9\u6700\u76f8\u5173\u7684\u9886\u57df\u77e5\u8bc6\u3002\u65b0\u9886\u57df\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u8bad\u7ec3\u9ad8\u6548\u6dfb\u52a0\uff0c\u907f\u514d\u5b8c\u6574\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2aWeb\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRAMA\u8fbe\u5230\u4e0e\u9886\u57df\u7279\u5b9a\u6a21\u578b\u76f8\u5f53\u7684\u68c0\u7d22\u6548\u679c\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u5176\u4e00\u5c0f\u90e8\u5206\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u80fd\u91cf\u611f\u77e5\u7684\u6a21\u578b\u8bbe\u8ba1\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u795e\u7ecf\u4fe1\u606f\u68c0\u7d22\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0cDRAMA\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}

{"id": "2510.25220", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25220", "abs": "https://arxiv.org/abs/2510.25220", "authors": ["Zhijie Lin", "Zhuofeng Li", "Chenglei Dai", "Wentian Bao", "Shuai Lin", "Enyun Yu", "Haoxiang Zhang", "Liang Zhao"], "title": "GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction", "comment": "Accepted by CIKM 2025", "summary": "In a multi-stage recommendation system, reranking plays a crucial role in\nmodeling intra-list correlations among items. A key challenge lies in exploring\noptimal sequences within the combinatorial space of permutations. Recent\nresearch follows a two-stage (generator-evaluator) paradigm, where a generator\nproduces multiple feasible sequences, and an evaluator selects the best one. In\npractice, the generator is typically implemented as an autoregressive model.\nHowever, these two-stage methods face two main challenges. First, the\nseparation of the generator and evaluator hinders end-to-end training. Second,\nautoregressive generators suffer from inference efficiency. In this work, we\npropose a Unified Generative Efficient Reranking Framework (GReF) to address\nthe two primary challenges. Specifically, we introduce Gen-Reranker, an\nautoregressive generator featuring a bidirectional encoder and a dynamic\nautoregressive decoder to generate causal reranking sequences. Subsequently, we\npre-train Gen-Reranker on the item exposure order for high-quality parameter\ninitialization. To eliminate the need for the evaluator while integrating\nsequence-level evaluation during training for end-to-end optimization, we\npropose post-training the model through Rerank-DPO. Moreover, for efficient\nautoregressive inference, we introduce ordered multi-token prediction (OMTP),\nwhich trains Gen-Reranker to simultaneously generate multiple future items\nwhile preserving their order, ensuring practical deployment in real-time\nrecommender systems. Extensive offline experiments demonstrate that GReF\noutperforms state-of-the-art reranking methods while achieving latency that is\nnearly comparable to non-autoregressive models. Additionally, GReF has also\nbeen deployed in a real-world video app Kuaishou with over 300 million daily\nactive users, significantly improving online recommendation quality.", "AI": {"tldr": "\u63d0\u51faGReF\u6846\u67b6\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u91cd\u6392\u5e8f\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u751f\u6210\u5668-\u8bc4\u4f30\u5668\u5206\u79bb\u5bfc\u81f4\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u56f0\u96be\uff0c\u4ee5\u53ca\u81ea\u56de\u5f52\u751f\u6210\u5668\u7684\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u91cd\u6392\u5e8f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u751f\u6210\u5668\u548c\u8bc4\u4f30\u5668\u7684\u5206\u79bb\u963b\u788d\u4e86\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u81ea\u56de\u5f52\u751f\u6210\u5668\u5728\u63a8\u7406\u65f6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u9ad8\u6548\u91cd\u6392\u5e8f\u6846\u67b6GReF\uff0c\u5305\u62ec\uff1a1) Gen-Reranker\u81ea\u56de\u5f52\u751f\u6210\u5668\uff0c\u5177\u6709\u53cc\u5411\u7f16\u7801\u5668\u548c\u52a8\u6001\u81ea\u56de\u5f52\u89e3\u7801\u5668\uff1b2) \u5728\u7269\u54c1\u66dd\u5149\u987a\u5e8f\u4e0a\u9884\u8bad\u7ec3\uff1b3) \u901a\u8fc7Rerank-DPO\u8fdb\u884c\u540e\u8bad\u7ec3\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\uff1b4) \u5f15\u5165\u6709\u5e8f\u591a\u4ee4\u724c\u9884\u6d4b(OMTP)\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u663e\u793aGReF\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u63a5\u8fd1\u975e\u81ea\u56de\u5f52\u6a21\u578b\u3002\u5df2\u5728\u5feb\u624bAPP\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u63a8\u8350\u8d28\u91cf\u3002", "conclusion": "GReF\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u91cd\u6392\u5e8f\u4e2d\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.25259", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.25259", "abs": "https://arxiv.org/abs/2510.25259", "authors": ["Yehjin Shin", "Jeongwhan Choi", "Seojin Kim", "Noseong Park"], "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation", "comment": "The 39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Recently, convolutional filters have been increasingly adopted in sequential\nrecommendation for their ability to capture local sequential patterns. However,\nmost of these models complement convolutional filters with self-attention. This\nis because convolutional filters alone, generally fixed filters, struggle to\ncapture global interactions necessary for accurate recommendation. We propose\nTime-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a\nmodel inspired by graph signal processing, where time-variant graph filters\ncapture position-dependent temporal variations in user sequences. By replacing\nboth fixed kernels and self-attention with time-variant filters, TV-Rec\nachieves higher expressive power and better captures complex interaction\npatterns in user behavior. This design not only eliminates the need for\nself-attention but also reduces computation while accelerating inference.\nExtensive experiments on six public benchmarks show that TV-Rec outperforms\nstate-of-the-art baselines by an average of 7.49%.", "AI": {"tldr": "TV-Rec\u4f7f\u7528\u65f6\u95f4\u53d8\u4f53\u5377\u79ef\u6ee4\u6ce2\u5668\u66ff\u4ee3\u56fa\u5b9a\u5377\u79ef\u6838\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5e8f\u5217\u63a8\u8350\u4e2d\u6355\u83b7\u4f4d\u7f6e\u4f9d\u8d56\u7684\u65f6\u95f4\u53d8\u5316\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u901a\u5e38\u7ed3\u5408\u5377\u79ef\u6ee4\u6ce2\u5668\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u56e0\u4e3a\u56fa\u5b9a\u5377\u79ef\u6ee4\u6ce2\u5668\u96be\u4ee5\u6355\u83b7\u5168\u5c40\u4ea4\u4e92\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u4ec5\u4f7f\u7528\u5377\u79ef\u6ee4\u6ce2\u5668\u5c31\u80fd\u6709\u6548\u6355\u83b7\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u53d8\u4f53\u5377\u79ef\u6ee4\u6ce2\u5668\uff0c\u7075\u611f\u6765\u81ea\u56fe\u4fe1\u53f7\u5904\u7406\uff0c\u80fd\u591f\u6355\u83b7\u7528\u6237\u5e8f\u5217\u4e2d\u4f4d\u7f6e\u4f9d\u8d56\u7684\u65f6\u95f4\u53d8\u5316\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4e86\u56fa\u5b9a\u5377\u79ef\u6838\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cTV-Rec\u5e73\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd57.49%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u5e76\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u65f6\u95f4\u53d8\u4f53\u5377\u79ef\u6ee4\u6ce2\u5668\u80fd\u591f\u6709\u6548\u6355\u83b7\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u590d\u6742\u4ea4\u4e92\u6a21\u5f0f\uff0c\u65e0\u9700\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5373\u53ef\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5e8f\u5217\u63a8\u8350\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25285", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25285", "abs": "https://arxiv.org/abs/2510.25285", "authors": ["Qiushi Pan", "Hao Wang", "Guoyuan An", "Luankang Zhang", "Wei Guo", "Yong Liu"], "title": "Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts", "comment": null, "summary": "In recommendation systems, how to effectively scale up recommendation models\nhas been an essential research topic. While significant progress has been made\nin developing advanced and scalable architectures for sequential\nrecommendation(SR) models, there are still challenges due to items'\nmulti-faceted characteristics and dynamic item relevance in the user context.\nTo address these issues, we propose Fuxi-MME, a framework that integrates a\nmulti-embedding strategy with a Mixture-of-Experts (MoE) architecture.\nSpecifically, to efficiently capture diverse item characteristics in a\ndecoupled manner, we decompose the conventional single embedding matrix into\nseveral lower-dimensional embedding matrices. Additionally, by substituting\nrelevant parameters in the Fuxi Block with an MoE layer, our model achieves\nadaptive and specialized transformation of the enriched representations.\nEmpirical results on public datasets show that our proposed framework\noutperforms several competitive baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fuxi-MME\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5d4c\u5165\u7b56\u7565\u548c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u9879\u76ee\u591a\u9762\u7279\u5f81\u548c\u52a8\u6001\u76f8\u5173\u6027\u6311\u6218\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u9879\u76ee\u5177\u6709\u591a\u9762\u7279\u5f81\u548c\u52a8\u6001\u76f8\u5173\u6027\uff0c\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u5728\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u6027\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u5c06\u4f20\u7edf\u5355\u4e00\u5d4c\u5165\u77e9\u9635\u5206\u89e3\u4e3a\u591a\u4e2a\u4f4e\u7ef4\u5d4c\u5165\u77e9\u9635\uff0c\u5e76\u5728Fuxi Block\u4e2d\u7528MoE\u5c42\u66ff\u6362\u76f8\u5173\u53c2\u6570\uff0c\u5b9e\u73b0\u4e30\u5bcc\u8868\u793a\u7684\u81ea\u9002\u5e94\u4e13\u4e1a\u5316\u8f6c\u6362\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u591a\u4e2a\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Fuxi-MME\u6846\u67b6\u901a\u8fc7\u591a\u5d4c\u5165\u548cMoE\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u63a8\u8350\u6a21\u578b\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.25402", "categories": ["cs.IR", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.25402", "abs": "https://arxiv.org/abs/2510.25402", "authors": ["Yuqian Chai", "Chaochao Wang", "Weilei Wang"], "title": "Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework", "comment": null, "summary": "Despite the surge in patent applications and emergence of AI drafting tools,\nsystematic evaluation of patent content quality has received limited research\nattention. To address this gap, We propose to evaluate patents using regulatory\ncompliance, technical coherence, and figure-reference consistency detection\nmodules, and then generate improvement suggestions via an integration module.\nThe framework is validated on a comprehensive dataset comprising 80\nhuman-authored and 80 AI-generated patents from two patent drafting tools.\nExperimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\%\nrespectively across the three detection modules when validated against expert\nannotations. Additional analysis was conducted to examine defect distributions\nacross patent sections, technical domains, and authoring sources. Section-based\nanalysis indicates that figure-text consistency and technical detail precision\nrequire particular attention. Mechanical Engineering and Construction show more\nclaim-specification inconsistencies due to complex technical documentation\nrequirements. AI-generated patents show a significant gap compared to\nhuman-authored ones. While human-authored patents primarily contain\nsurface-level errors like typos, AI-generated patents exhibit more structural\ndefects in figure-text alignment and cross-references.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e13\u5229\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7ba1\u5408\u89c4\u6027\u3001\u6280\u672f\u4e00\u81f4\u6027\u548c\u56fe\u6587\u4e00\u81f4\u6027\u68c0\u6d4b\u6a21\u5757\u6765\u8bc4\u4f30\u4e13\u5229\u8d28\u91cf\uff0c\u5e76\u751f\u6210\u6539\u8fdb\u5efa\u8bae\u3002\u5728\u5305\u542b160\u4e2a\u4e13\u5229\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u68c0\u6d4b\u6a21\u5757\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523099.74%\u300182.12%\u548c91.2%\u3002", "motivation": "\u5c3d\u7ba1\u4e13\u5229\u7533\u8bf7\u6fc0\u589e\u4e14AI\u8d77\u8349\u5de5\u5177\u6d8c\u73b0\uff0c\u4f46\u4e13\u5229\u5185\u5bb9\u8d28\u91cf\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u7814\u7a76\u6709\u9650\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u76d1\u7ba1\u5408\u89c4\u6027\u3001\u6280\u672f\u4e00\u81f4\u6027\u548c\u56fe\u6587\u4e00\u81f4\u6027\u68c0\u6d4b\u4e09\u4e2a\u6a21\u5757\u8bc4\u4f30\u4e13\u5229\u8d28\u91cf\uff0c\u901a\u8fc7\u96c6\u6210\u6a21\u5757\u751f\u6210\u6539\u8fdb\u5efa\u8bae\u3002\u572880\u4e2a\u4eba\u5de5\u64b0\u5199\u548c80\u4e2aAI\u751f\u6210\u7684\u4e13\u5229\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4e09\u4e2a\u68c0\u6d4b\u6a21\u5757\u7684\u5e73\u8861\u51c6\u786e\u7387\u5206\u522b\u4e3a99.74%\u300182.12%\u548c91.2%\u3002\u5206\u6790\u53d1\u73b0\u56fe\u6587\u4e00\u81f4\u6027\u548c\u6280\u672f\u7ec6\u8282\u7cbe\u5ea6\u9700\u8981\u7279\u522b\u5173\u6ce8\uff0c\u673a\u68b0\u5de5\u7a0b\u548c\u5efa\u7b51\u9886\u57df\u5b58\u5728\u66f4\u591a\u6743\u5229\u8981\u6c42-\u8bf4\u660e\u4e66\u4e0d\u4e00\u81f4\u95ee\u9898\uff0cAI\u751f\u6210\u4e13\u5229\u76f8\u6bd4\u4eba\u5de5\u64b0\u5199\u4e13\u5229\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc4\u4f30\u4e13\u5229\u8d28\u91cf\uff0cAI\u751f\u6210\u4e13\u5229\u5728\u7ed3\u6784\u7f3a\u9677\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u56fe\u6587\u5bf9\u9f50\u548c\u4ea4\u53c9\u5f15\u7528\u7b49\u7ed3\u6784\u6027\u95ee\u9898\u3002"}}
{"id": "2510.25428", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.25428", "abs": "https://arxiv.org/abs/2510.25428", "authors": ["Thang-Long Nguyen-Ho", "Minh-Khoi Pham", "Hoang-Bao Le"], "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report", "comment": "Alibaba International E-commerce Product Search Competition @ CIKM\n  2025", "summary": "This report details our methodology and results developed for the\nMultilingual E-commerce Search Competition. The problem aims to recognize\nrelevance between user queries versus product items in a multilingual context\nand improve recommendation performance on e-commerce platforms. Utilizing Large\nLanguage Models (LLMs) and their capabilities in other tasks, our data-centric\nmethod achieved the highest score compared to other solutions during the\ncompetition. Final leaderboard is publised at\nhttps://alibaba-international-cikm2025.github.io. The source code for our\nproject is published at https://github.com/nhtlongcs/e-commerce-product-search.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u7535\u5546\u641c\u7d22\u7ade\u8d5b\uff0c\u5728\u76f8\u5173\u6027\u8bc6\u522b\u548c\u63a8\u8350\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u4f73\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7528\u6237\u67e5\u8be2\u4e0e\u4ea7\u54c1\u9879\u76ee\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u8bc6\u522b\u95ee\u9898\uff0c\u63d0\u5347\u7535\u5546\u5e73\u53f0\u7684\u63a8\u8350\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u5316\u7684\u65b9\u6cd5\u8fdb\u884c\u591a\u8bed\u8a00\u7535\u5546\u641c\u7d22\u3002", "result": "\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u4e2d\u5fc3\u5316\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u7535\u5546\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2510.25488", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25488", "abs": "https://arxiv.org/abs/2510.25488", "authors": ["Yiteng Tu", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Fen Lin", "Qin Liu", "Qingyao Ai"], "title": "Generalized Pseudo-Relevance Feedback", "comment": null, "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It\ntypically employs the retrieval result as relevance feedback to refine the\nquery and thereby addresses the vocabulary mismatch between user queries and\nrelevant documents. Traditional pseudo-relevance feedback (PRF) and its\nvector-based extension (VPRF) improve retrieval performance by leveraging\ntop-retrieved documents as relevance feedback. However, they are constructed\nbased on two major hypotheses: the relevance assumption (top documents are\nrelevant) and the model assumption (rewriting methods need to be designed\nspecifically for particular model architectures). While recent large language\nmodels (LLMs)-based generative relevance feedback (GRF) enables model-free\nquery reformulation, it either suffers from severe LLM hallucination or, again,\nrelies on the relevance assumption to guarantee the effectiveness of rewriting\nquality. To overcome these limitations, we introduce an assumption-relaxed\nframework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which\nperforms model-free, natural language rewriting based on retrieved documents,\nnot only eliminating the model assumption but also reducing dependence on the\nrelevance assumption. Specifically, we design a utility-oriented training\npipeline with reinforcement learning to ensure robustness against noisy\nfeedback. Extensive experiments across multiple benchmarks and retrievers\ndemonstrate that GPRF consistently outperforms strong baselines, establishing\nit as an effective and generalizable framework for query rewriting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e7f\u4e49\u4f2a\u76f8\u5173\u53cd\u9988(GPRF)\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u65e0\u5173\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u91cd\u5199\uff0c\u51cf\u5c11\u5bf9\u76f8\u5173\u5047\u8bbe\u548c\u6a21\u578b\u5047\u8bbe\u7684\u4f9d\u8d56\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u786e\u4fdd\u5bf9\u566a\u58f0\u53cd\u9988\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfPRF\u548cVPRF\u65b9\u6cd5\u57fa\u4e8e\u4e24\u4e2a\u4e3b\u8981\u5047\u8bbe\uff1a\u76f8\u5173\u5047\u8bbe\uff08\u9876\u90e8\u6587\u6863\u76f8\u5173\uff09\u548c\u6a21\u578b\u5047\u8bbe\uff08\u91cd\u5199\u65b9\u6cd5\u9700\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\uff09\u3002LLM-based GRF\u867d\u7136\u5b9e\u73b0\u6a21\u578b\u65e0\u5173\u91cd\u5199\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u6216\u4ecd\u4f9d\u8d56\u76f8\u5173\u5047\u8bbe\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7528\u5bfc\u5411\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fdb\u884c\u6a21\u578b\u65e0\u5173\u7684\u81ea\u7136\u8bed\u8a00\u91cd\u5199\uff0c\u57fa\u4e8e\u68c0\u7d22\u6587\u6863\u800c\u975e\u4ec5\u9876\u90e8\u6587\u6863\uff0c\u51cf\u5c11\u5bf9\u76f8\u5173\u5047\u8bbe\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u68c0\u7d22\u5668\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGPRF\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3a\u67e5\u8be2\u91cd\u5199\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "GPRF\u662f\u4e00\u4e2a\u5047\u8bbe\u653e\u5bbd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d88\u9664\u6a21\u578b\u5047\u8bbe\u548c\u51cf\u5c11\u5bf9\u76f8\u5173\u5047\u8bbe\u7684\u4f9d\u8d56\uff0c\u4e3a\u67e5\u8be2\u91cd\u5199\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.25622", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.25622", "abs": "https://arxiv.org/abs/2510.25622", "authors": ["Yi Xu", "Moyu Zhang", "Chaofan Fan", "Jinxin Hu", "Xiaochen Li", "Yu Zhang", "Xiaoyi Zeng", "Jing Zhang"], "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation", "comment": null, "summary": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs).\nHowever, this method struggles with scalability and generalization in large,\ndynamic datasets that have sparse long-tail data.Content-based Semantic IDs\n(SIDs) address this by sharing knowledge through content quantization. However,\nby ignoring dynamic behavioral properties, purely content-based SIDs have\nlimited expressive power. Existing methods attempt to incorporate behavioral\ninformation but overlook a critical distinction: unlike relatively uniform\ncontent features, user-item interactions are highly skewed and diverse,\ncreating a vast information gap in quality and quantity between popular and\nlong-tail items. This oversight leads to two critical limitations: (1) Noise\nCorruption: Indiscriminate behavior-content alignment allows collaborative\nnoise from long-tail items to corrupt their content representations, leading to\nthe loss of critical multimodal information. (2)Signal Obscurity: The\nequal-weighting scheme for SIDs fails to reflect the varying importance of\ndifferent behavioral signals, making it difficult for downstream tasks to\ndistinguish important SIDs from uninformative ones. To tackle these issues, we\npropose a mixture-of-quantization framework, MMQ-v2, to adaptively Align,\nDenoise, and Amplify multimodal information from content and behavior\nmodalities for semantic IDs learning. The semantic IDs generated by this\nframework named ADA-SID. It introduces two innovations: an adaptive\nbehavior-content alignment that is aware of information richness to shield\nrepresentations from noise, and a dynamic behavioral router to amplify critical\nsignals by applying different weights to SIDs. Extensive experiments on public\nand large-scale industrial datasets demonstrate ADA-SID's significant\nsuperiority in both generative and discriminative recommendation tasks.", "AI": {"tldr": "\u63d0\u51faADA-SID\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u884c\u4e3a-\u5185\u5bb9\u5bf9\u9f50\u548c\u52a8\u6001\u884c\u4e3a\u8def\u7531\uff0c\u89e3\u51b3\u4f20\u7edf\u8bed\u4e49ID\u5728\u957f\u5c3e\u6570\u636e\u4e2d\u7684\u566a\u58f0\u6c61\u67d3\u548c\u4fe1\u53f7\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5185\u5bb9\u7684\u8bed\u4e49ID\u5ffd\u7565\u884c\u4e3a\u7279\u5f81\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u6d41\u884c\u5546\u54c1\u4e0e\u957f\u5c3e\u5546\u54c1\u4e4b\u95f4\u7684\u4fe1\u606f\u8d28\u91cf\u5dee\u5f02\uff0c\u5bfc\u81f4\u566a\u58f0\u6c61\u67d3\u548c\u4fe1\u53f7\u6a21\u7cca\u3002", "method": "\u4f7f\u7528MMQ-v2\u6df7\u5408\u91cf\u5316\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u884c\u4e3a-\u5185\u5bb9\u5bf9\u9f50\uff08\u57fa\u4e8e\u4fe1\u606f\u4e30\u5bcc\u5ea6\uff09\u548c\u52a8\u6001\u884c\u4e3a\u8def\u7531\uff08\u5bf9SID\u65bd\u52a0\u4e0d\u540c\u6743\u91cd\uff09\u3002", "result": "\u5728\u516c\u5f00\u548c\u5927\u89c4\u6a21\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cADA-SID\u5728\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u63a8\u8350\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ADA-SID\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u9f50\u548c\u4fe1\u53f7\u653e\u5927\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49ID\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.25718", "categories": ["cs.IR", "cs.DL"], "pdf": "https://arxiv.org/pdf/2510.25718", "abs": "https://arxiv.org/abs/2510.25718", "authors": ["Jamie Mahowald", "Benjamin Charles Germain Lee"], "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali", "comment": "5 pages, 5 figures", "summary": "Multimodal approaches have shown great promise for searching and navigating\ndigital collections held by libraries, archives, and museums. In this paper, we\nintroduce map-RAS: a retrieval-augmented search system for historic maps. In\naddition to introducing our framework, we detail our publicly-hosted demo for\nsearching 101,233 map images held by the Library of Congress. With our system,\nusers can multimodally query the map collection via ColPali, summarize search\nresults using Llama 3.2, and upload their own collections to perform\ninter-collection search. We articulate potential use cases for archivists,\ncurators, and end-users, as well as future work with our system in both machine\nlearning and the digital humanities. Our demo can be viewed at:\nhttp://www.mapras.com.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86map-RAS\uff1a\u4e00\u4e2a\u7528\u4e8e\u5386\u53f2\u5730\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u641c\u7d22\u7cfb\u7edf\uff0c\u5305\u542b\u516c\u5f00\u6f14\u793a\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6a21\u6001\u67e5\u8be2\u548c\u8de8\u96c6\u5408\u641c\u7d22", "motivation": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u56fe\u4e66\u9986\u3001\u6863\u6848\u9986\u548c\u535a\u7269\u9986\u7684\u6570\u5b57\u9986\u85cf\u641c\u7d22\u4e0e\u5bfc\u822a\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u9700\u8981\u4e3a\u5386\u53f2\u5730\u56fe\u5f00\u53d1\u4e13\u95e8\u7684\u68c0\u7d22\u7cfb\u7edf", "method": "\u4f7f\u7528ColPali\u8fdb\u884c\u591a\u6a21\u6001\u67e5\u8be2\u5730\u56fe\u96c6\u5408\uff0c\u5229\u7528Llama 3.2\u603b\u7ed3\u641c\u7d22\u7ed3\u679c\uff0c\u652f\u6301\u7528\u6237\u4e0a\u4f20\u81ea\u6709\u96c6\u5408\u8fdb\u884c\u8de8\u96c6\u5408\u641c\u7d22", "result": "\u5f00\u53d1\u4e86\u9762\u5411\u7f8e\u56fd\u56fd\u4f1a\u56fe\u4e66\u9986101,233\u5f20\u5730\u56fe\u56fe\u50cf\u7684\u516c\u5f00\u6f14\u793a\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u67e5\u8be2\u548c\u8de8\u96c6\u5408\u641c\u7d22\u529f\u80fd", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6863\u6848\u7ba1\u7406\u5458\u3001\u7b56\u5c55\u4eba\u548c\u6700\u7ec8\u7528\u6237\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5728\u673a\u5668\u5b66\u4e60\u548c\u6570\u5b57\u4eba\u6587\u9886\u57df\u5177\u6709\u672a\u6765\u53d1\u5c55\u6f5c\u529b"}}

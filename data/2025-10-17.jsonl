{"id": "2510.14162", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14162", "abs": "https://arxiv.org/abs/2510.14162", "authors": ["Juhyeong Kim", "Yejin Kim", "Youngbin Lee", "Hyunwoo Byun"], "title": "FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API", "comment": "4 pages, 2 figures, accepted at CIKM 2025 FinAI Workshop", "summary": "We present FinAI Data Assistant, a practical approach for natural-language\nquerying over financial databases that combines large language models (LLMs)\nwith the OpenAI Function Calling API. Rather than synthesizing complete SQL via\ntext-to-SQL, our system routes user requests to a small library of vetted,\nparameterized queries, trading generative flexibility for reliability, low\nlatency, and cost efficiency. We empirically study three questions: (RQ1)\nwhether LLMs alone can reliably recall or extrapolate time-dependent financial\ndata without external retrieval; (RQ2) how well LLMs map company names to stock\nticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for\nend-to-end database query processing. Across controlled experiments on prices\nand fundamentals, LLM-only predictions exhibit non-negligible error and show\nlook-ahead bias primarily for stock prices relative to model knowledge cutoffs.\nTicker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high\nfor S\\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and\ncost and higher reliability than a text-to-SQL baseline on our task suite. We\ndiscuss design trade-offs, limitations, and avenues for deployment."}
{"id": "2510.14223", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14223", "abs": "https://arxiv.org/abs/2510.14223", "authors": ["Sudarshan Srinivasa Ramanujam", "Antonio Alonso", "Saurabh Kataria", "Siddharth Dangi", "Akhilesh Gupta", "Birjodh Singh Tiwana", "Manas Somaiya", "Luke Simon", "David Byrne", "Sojeong Ha", "Sen Zhou", "Andrei Akterskii", "Zhanglong Liu", "Samira Sriram", "Crescent Xiong", "Zhoutao Pei", "Angela Shao", "Alex Li", "Annie Xiao", "Caitlin Kolb", "Thomas Kistler", "Zach Moore", "Hamed Firooz"], "title": "Large Scale Retrieval for the LinkedIn Feed using Causal Language Models", "comment": "9 pages, 4 figures", "summary": "In large scale recommendation systems like the LinkedIn Feed, the retrieval\nstage is critical for narrowing hundreds of millions of potential candidates to\na manageable subset for ranking. LinkedIn's Feed serves suggested content from\noutside of the member's network (based on the member's topical interests),\nwhere 2000 candidates are retrieved from a pool of hundreds of millions\ncandidate with a latency budget of a few milliseconds and inbound QPS of\nseveral thousand per second. This paper presents a novel retrieval approach\nthat fine-tunes a large causal language model (Meta's LLaMA 3) as a dual\nencoder to generate high quality embeddings for both users (members) and\ncontent (items), using only textual input. We describe the end to end pipeline,\nincluding prompt design for embedding generation, techniques for fine-tuning at\nLinkedIn's scale, and infrastructure for low latency, cost effective online\nserving. We share our findings on how quantizing numerical features in the\nprompt enables the information to get properly encoded in the embedding,\nfacilitating greater alignment between the retrieval and ranking layer. The\nsystem was evaluated using offline metrics and an online A/B test, which showed\nsubstantial improvements in member engagement. We observed significant gains\namong newer members, who often lack strong network connections, indicating that\nhigh-quality suggested content aids retention. This work demonstrates how\ngenerative language models can be effectively adapted for real time, high\nthroughput retrieval in industrial applications."}
{"id": "2510.14257", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14257", "abs": "https://arxiv.org/abs/2510.14257", "authors": ["Lingyu Mu", "Hao Deng", "Haibo Xing", "Kaican Lin", "Zhitong Zhu", "Yu Zhang", "Xiaoyi Zeng", "Zhengxiao Liu", "Zheng Lin", "Jinxin Hu"], "title": "Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation", "comment": null, "summary": "The integration of large language models (LLMs) into recommendation systems\nhas revealed promising potential through their capacity to extract world\nknowledge for enhanced reasoning capabilities. However, current methodologies\nthat adopt static schema-based prompting mechanisms encounter significant\nlimitations: (1) they employ universal template structures that neglect the\nmulti-faceted nature of user preference diversity; (2) they implement\nsuperficial alignment between semantic knowledge representations and behavioral\nfeature spaces without achieving comprehensive latent space integration. To\naddress these challenges, we introduce CoCo, an end-to-end framework that\ndynamically constructs user-specific contextual knowledge embeddings through a\ndual-mechanism approach. Our method realizes profound integration of semantic\nand behavioral latent dimensions via adaptive knowledge fusion and\ncontradiction resolution modules. Experimental evaluations across diverse\nbenchmark datasets and an enterprise-level e-commerce platform demonstrate\nCoCo's superiority, achieving a maximum 8.58% improvement over seven\ncutting-edge methods in recommendation accuracy. The framework's deployment on\na production advertising system resulted in a 1.91% sales growth, validating\nits practical effectiveness. With its modular design and model-agnostic\narchitecture, CoCo provides a versatile solution for next-generation\nrecommendation systems requiring both knowledge-enhanced reasoning and\npersonalized adaptation."}
{"id": "2510.14321", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14321", "abs": "https://arxiv.org/abs/2510.14321", "authors": ["Jianting Tang", "Dongshuai Li", "Tao Wen", "Fuyu Lv", "Dan Ou", "Linli Xu"], "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm", "comment": null, "summary": "In modern e-commerce search systems, dense retrieval has become an\nindispensable component. By computing similarities between query and item\n(product) embeddings, it efficiently selects candidate products from\nlarge-scale repositories. With the breakthroughs in large language models\n(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs\nfor more accurate text modeling. However, these models still adopt\ndirect-embedding methods, and the semantic accuracy of embeddings remains\ninadequate. Therefore, contrastive learning is heavily employed to achieve\ntight semantic alignment between positive pairs. Consequently, such models tend\nto capture statistical co-occurrence patterns in the training data, biasing\nthem toward shallow lexical and semantic matches. For difficult queries\nexhibiting notable lexical disparity from target items, the performance\ndegrades significantly. In this work, we propose the Large Reasoning Embedding\nModel (LREM), which novelly integrates reasoning processes into representation\nlearning. For difficult queries, LREM first conducts reasoning to achieve a\ndeep understanding of the original query, and then produces a\nreasoning-augmented query embedding for retrieval. This reasoning process\neffectively bridges the semantic gap between original queries and target items,\nsignificantly improving retrieval accuracy. Specifically, we adopt a two-stage\ntraining process: the first stage optimizes the LLM on carefully curated\nQuery-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary\nreasoning and embedding capabilities, and the second stage further refines the\nreasoning trajectories via reinforcement learning (RL). Extensive offline and\nonline experiments validate the effectiveness of LREM, leading to its\ndeployment on China's largest e-commerce platform since August 2025."}
{"id": "2510.14330", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14330", "abs": "https://arxiv.org/abs/2510.14330", "authors": ["Yuto Nakamizo", "Ryuhei Miyazato", "Hikaru Tanabe", "Ryuta Yamakura", "Kiori Hatanaka"], "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations", "comment": "5th place solution at Meta KDD Cup 2025", "summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta\nCRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question\nanswering (VQA) dataset focused on factual questions about images, including\negocentric images. The competition was contested based on VQA accuracy, as\njudged by an LLM-based automatic evaluator. Since incorrect answers result in\nnegative scores, our strategy focused on reducing hallucinations from the\ninternal representations of the VLM. Specifically, we trained logistic\nregression-based hallucination detection models using both the hidden_state and\nthe outputs of specific attention heads. We then employed an ensemble of these\nmodels. As a result, while our method sacrificed some correct answers, it\nsignificantly reduced hallucinations and allowed us to place among the top\nentries on the final leaderboard. For implementation details and code, please\nrefer to\nhttps://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit."}
{"id": "2510.14626", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14626", "abs": "https://arxiv.org/abs/2510.14626", "authors": ["Zhibo Wu", "Yunfan Wu", "Quan Liu", "Lin Jiang", "Ping Yang", "Yao Hu"], "title": "GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation", "comment": null, "summary": "Multi-interest recommendation has gained attention, especially in industrial\nretrieval stage. Unlike classical dual-tower methods, it generates multiple\nuser representations instead of a single one to model comprehensive user\ninterests. However, prior studies have identified two underlying limitations:\nthe first is interest collapse, where multiple representations homogenize. The\nsecond is insufficient modeling of interest evolution, as they struggle to\ncapture latent interests absent from a user's historical behavior. We begin\nwith a thorough review of existing works in tackling these limitations. Then,\nwe attempt to tackle these limitations from a new perspective. Specifically, we\npropose a framework-level refinement for multi-interest recommendation, named\nGemiRec. The proposed framework leverages interest quantization to enforce a\nstructural interest separation and interest generation to learn the evolving\ndynamics of user interests explicitly. It comprises three modules: (a) Interest\nDictionary Maintenance Module (IDMM) maintains a shared quantized interest\ndictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a\ngenerative model to capture the distribution of user future interests. (c)\nMulti-Interest Retrieval Module (MIRM) retrieves items using multiple\nuser-interest representations. Both theoretical and empirical analyses, as well\nas extensive experiments, demonstrate its advantages and effectiveness.\nMoreover, it has been deployed in production since March 2025, showing its\npractical value in industrial applications."}
{"id": "2510.14629", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14629", "abs": "https://arxiv.org/abs/2510.14629", "authors": ["Jiani Huang", "Xingchen Zou", "Lianghao Xia", "Qing Li"], "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs", "comment": null, "summary": "The application of Large Language Models (LLMs) in recommender systems faces\nkey challenges in delivering deep personalization and intelligent reasoning,\nespecially for interactive scenarios. Current methods are often constrained by\nlimited context windows and single-turn reasoning, hindering their ability to\ncapture dynamic user preferences and proactively reason over recommendation\ncontexts. To address these limitations, we propose MR.Rec, a novel framework\nthat synergizes memory and reasoning for LLM-based recommendations. To achieve\npersonalization, we develop a comprehensive Retrieval-Augmented Generation\n(RAG) system that efficiently indexes and retrieves relevant external memory to\nenhance LLM personalization capabilities. Furthermore, to enable the synergy\nbetween memory and reasoning, our RAG system goes beyond conventional\nquery-based retrieval by integrating reasoning enhanced memory retrieval.\nFinally, we design a reinforcement learning framework that trains the LLM to\nautonomously learn effective strategies for both memory utilization and\nreasoning refinement. By combining dynamic memory retrieval with adaptive\nreasoning, this approach ensures more accurate, context-aware, and highly\npersonalized recommendations. Extensive experiments demonstrate that MR.Rec\nsignificantly outperforms state-of-the-art baselines across multiple metrics,\nvalidating its efficacy in delivering intelligent and personalized\nrecommendations. We will release code and data upon paper notification."}
{"id": "2510.14641", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14641", "abs": "https://arxiv.org/abs/2510.14641", "authors": ["Zhibo Wu", "Yunfan Wu", "Lin Jiang", "Ping Yang", "Yao Hu"], "title": "Causality Enhancement for Cross-Domain Recommendation", "comment": null, "summary": "Cross-domain recommendation forms a crucial component in recommendation\nsystems. It leverages auxiliary information through source domain tasks or\nfeatures to enhance target domain recommendations. However, incorporating\ninconsistent source domain tasks may result in insufficient cross-domain\nmodeling or negative transfer. While incorporating source domain features\nwithout considering the underlying causal relationships may limit their\ncontribution to final predictions. Thus, a natural idea is to directly train a\ncross-domain representation on a causality-labeled dataset from the source to\ntarget domain. Yet this direction has been rarely explored, as identifying\nunbiased real causal labels is highly challenging in real-world scenarios. In\nthis work, we attempt to take a first step in this direction by proposing a\ncausality-enhanced framework, named CE-CDR. Specifically, we first reformulate\nthe cross-domain recommendation as a causal graph for principled guidance. We\nthen construct a causality-aware dataset heuristically. Subsequently, we derive\na theoretically unbiased Partial Label Causal Loss to generalize beyond the\nbiased causality-aware dataset to unseen cross-domain patterns, yielding an\nenriched cross-domain representation, which is then fed into the target model\nto enhance target-domain recommendations. Theoretical and empirical analyses,\nas well as extensive experiments, demonstrate the rationality and effectiveness\nof CE-CDR and its general applicability as a model-agnostic plugin. Moreover,\nit has been deployed in production since April 2025, showing its practical\nvalue in real-world applications."}
{"id": "2510.14704", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14704", "abs": "https://arxiv.org/abs/2510.14704", "authors": ["Leonie Winter"], "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?", "comment": "69 pages, 14 figures", "summary": "Offline evaluations in recommender system research depend heavily on\ndatasets, many of which are pruned, such as the widely used MovieLens\ncollections. This thesis examines the impact of data pruning - specifically,\nremoving users with fewer than a specified number of interactions - on both\ndataset characteristics and algorithm performance. Five benchmark datasets were\nanalysed in both their unpruned form and at five successive pruning levels (5,\n10, 20, 50, 100). For each coreset, we examined structural and distributional\ncharacteristics and trained and tested eleven representative algorithms. To\nfurther assess if pruned datasets lead to artificially inflated performance\nresults, we also evaluated models trained on the pruned train sets but tested\non unpruned data. Results show that commonly applied core pruning can be highly\nselective, leaving as little as 2% of the original users in some datasets.\nTraditional algorithms achieved higher nDCG@10 scores when both training and\ntesting on pruned data; however, this advantage largely disappeared when\nevaluated on unpruned test sets. Across all algorithms, performance declined\nwith increasing pruning levels when tested on unpruned data, highlighting the\nimpact of dataset reduction on the performance of recommender algorithms."}
{"id": "2510.14788", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14788", "abs": "https://arxiv.org/abs/2510.14788", "authors": ["Manjie Xu", "Cheng Chen", "Xin Jia", "Jingyi Zhou", "Yongji Wu", "Zejian Wang", "Chi Zhang", "Kai Zuo", "Yibo Chen", "Xu Tang", "Yao Hu", "Yixin Zhu"], "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale", "comment": "The dataset, code, and models will be released soon", "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."}
{"id": "2510.14857", "categories": ["cs.IR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.14857", "abs": "https://arxiv.org/abs/2510.14857", "authors": ["Gabriele Barlacchi", "Margherita Lalli", "Emanuele Ferragina", "Fosca Giannotti", "Luca Pappalardo"], "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems", "comment": "12 pages, 4 figures", "summary": "Recommender systems continuously interact with users, creating feedback loops\nthat shape both individual behavior and collective market dynamics. This paper\nintroduces a simulation framework to model these loops in online retail\nenvironments, where recommenders are periodically retrained on evolving\nuser-item interactions. Using the Amazon e-Commerce dataset, we analyze how\ndifferent recommendation algorithms influence diversity, purchase\nconcentration, and user homogenization over time. Results reveal a systematic\ntrade-off: while the feedback loop increases individual diversity, it\nsimultaneously reduces collective diversity and concentrates demand on a few\npopular items. Moreover, for some recommender systems, the feedback loop\nincreases user homogenization over time, making user purchase profiles\nincreasingly similar. These findings underscore the need for recommender\ndesigns that balance personalization with long-term diversity."}
{"id": "2510.14880", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.14880", "abs": "https://arxiv.org/abs/2510.14880", "authors": ["Rikiya Takehi", "Benjamin Clavi√©", "Sean Lee", "Aamir Shakir"], "title": "Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report", "comment": null, "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different\nparameter counts: 17M and 32M. As part of our research, we conduct numerous\nexperiments to improve retrieval and late-interaction models, which we intend\nto distill into smaller models as proof-of-concepts. Our ultimate aim is to\nsupport retrieval at all scales, from large-scale retrieval which lives in the\ncloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a\nmodel that we hope will serve as a solid foundation backbone for all future\nexperiments, representing the first version of a long series of small\nproof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we\nconducted multiple ablation studies, of which we report the results. In terms\nof downstream performance, mxbai-edge-colbert-v0 is a particularly capable\nsmall model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and\nrepresenting a large step forward in long-context tasks, with unprecedented\nefficiency."}

{"id": "2511.16921", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.16921", "abs": "https://arxiv.org/abs/2511.16921", "authors": ["Liming Xiang", "Jing Feng", "Ziqi Yin", "Zijian Li", "Daihao Xue", "Hongchao Qin", "Ronghua Li", "Guoren Wang"], "title": "\u03b4-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search", "comment": null, "summary": "Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $\u03b5$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-\u03b5$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/\u03b4)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $\u03b4$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $\u03b4$-monotonic geometric constraint during graph construction, $\u03b4$-EMG ensures that any greedy search converges to a $(1/\u03b4)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $\u03b4$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bef\u5dee\u6709\u754c\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u03b4-EMG\u56fe\u7ed3\u6784\u786e\u4fdd\u8fd4\u56de\u7ed3\u679c\u4e3a\u771f\u5b9e\u503c\u7684(1/\u03b4)\u8fd1\u4f3c\uff0c\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5f53\u524d\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u03b5-\u53ec\u56de\u7387\u539f\u5219\uff0c\u53ea\u5173\u6ce8\u6b63\u786e\u7ed3\u679c\u7684\u53ec\u56de\u7387\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u9519\u8bef\u7ed3\u679c\u7684\u504f\u5dee\u8303\u56f4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u7406\u8bba\u8bef\u5dee\u4fdd\u8bc1\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u63d0\u51fa\u03b4-EMG\uff08\u8bef\u5dee\u6709\u754c\u5355\u8c03\u56fe\uff09\uff0c\u901a\u8fc7\u5728\u6784\u5efa\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u03b4-\u5355\u8c03\u51e0\u4f55\u7ea6\u675f\uff0c\u786e\u4fdd\u8d2a\u5fc3\u641c\u7d22\u80fd\u6536\u655b\u5230(1/\u03b4)\u8fd1\u4f3c\u90bb\u5c45\u3002\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u03b4-EMQG\uff08\u8bef\u5dee\u6709\u754c\u5355\u8c03\u91cf\u5316\u56fe\uff09\u6765\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728ANN-Benchmarks\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u53ec\u56de\u7387\u8981\u6c420.99\u65f6\uff0c\u5728SIFT1M\u6570\u636e\u96c6\u4e0a\u8fbe\u523019,000 QPS\uff0c\u6bd4\u5176\u4ed6\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u8d85\u8fc740%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bef\u5dee\u6709\u754c\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u7406\u8bba\u8bef\u5dee\u4fdd\u8bc1\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.16943", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16943", "abs": "https://arxiv.org/abs/2511.16943", "authors": ["Tianyu Zhan", "Kairui Fu", "Zheqi Lv", "Shengyu Zhang"], "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers", "comment": "4 pages", "summary": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.", "AI": {"tldr": "RASTP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u663e\u8457\u6027\u548c\u6ce8\u610f\u529b\u4e2d\u5fc3\u6027\u6765\u52a8\u6001\u526a\u679d\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u4f4e\u4fe1\u606f\u8bed\u4e49\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4f7f\u7528\u8bed\u4e49\u6807\u8bc6\u7b26(SIDs)\u8868\u793a\u7269\u54c1\uff0c\u4f46\u591a\u4e2aSIDs\u4f1a\u663e\u8457\u589e\u52a0\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\u589e\u52a0\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV\u7f13\u5b58\uff0c\u4f46\u76f4\u63a5\u526a\u679d\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u4f4e\u4fe1\u606f\u6807\u8bb0\u53ef\u80fd\u66f4\u6709\u6548\u3002", "method": "\u63d0\u51faRASTP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u663e\u8457\u6027\uff08\u901a\u8fc7\u8868\u793a\u5e45\u5ea6\u8861\u91cf\uff09\u548c\u6ce8\u610f\u529b\u4e2d\u5fc3\u6027\uff08\u6765\u81ea\u7d2f\u79ef\u6ce8\u610f\u529b\u6743\u91cd\uff09\u6765\u8bc4\u4f30\u6807\u8bb0\u91cd\u8981\u6027\uff0c\u52a8\u6001\u526a\u679d\u4f4e\u4fe1\u606f\u6216\u65e0\u5173\u7684\u8bed\u4e49\u6807\u8bb0\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cAmazon\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRASTP\u51cf\u5c11\u4e8626.7%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u9ad8\u4e86\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "RASTP\u901a\u8fc7\u76f4\u63a5\u526a\u679d\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u4f4e\u4fe1\u606f\u8bed\u4e49\u6807\u8bb0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e8f\u5217\u957f\u5ea6\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u7ef4\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.17041", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17041", "abs": "https://arxiv.org/abs/2511.17041", "authors": ["Xiangrui Xiong", "Yichuan Lu", "Zifei Pan", "Chang Sun"], "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation", "comment": null, "summary": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.", "AI": {"tldr": "CLLMRec\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684MOOC\u6982\u5ff5\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u5148\u9a8c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u65e0\u9700\u4f9d\u8d56\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u5373\u53ef\u5b9e\u73b0\u8ba4\u77e5\u611f\u77e5\u7684\u4e2a\u6027\u5316\u6982\u5ff5\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f46\u5728\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e2d\u8fd9\u7c7b\u56fe\u8c31\u5f80\u5f80\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u6982\u5ff5\u63a8\u8350\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u5bf9\u9f50\u6784\u5efa\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u8fdb\u884c\u5148\u9a8c\u77e5\u8bc6\u84b8\u998f\uff0c\u7ed3\u5408\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\u5efa\u6a21\u5b66\u4e60\u8005\u5b9e\u65f6\u8ba4\u77e5\u72b6\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9eMOOC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLLMRec\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u663e\u5f0f\u7ed3\u6784\u5148\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u771f\u6b63\u8ba4\u77e5\u611f\u77e5\u548c\u4e2a\u6027\u5316\u7684\u6982\u5ff5\u63a8\u8350\u3002"}}
{"id": "2511.17044", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17044", "abs": "https://arxiv.org/abs/2511.17044", "authors": ["Zhan Su", "Fengran Mo", "Jian-yun Nie"], "title": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters", "comment": null, "summary": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.", "AI": {"tldr": "Poly-PRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u6f5c\u5728\u8def\u7531\u7f16\u7801\u8fc7\u7a0b\u4f7f\u7528\u5c11\u91cf\u6f5c\u5728LoRA\u9002\u914d\u5668\u7f16\u7801\u6574\u4e2a\u6587\u6863\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPRAG\u65b9\u6cd5\u4e2d\u4e00\u5bf9\u4e00\u6587\u6863\u7f16\u7801\u65b9\u6848\u7684\u6570\u636e\u7a00\u7f3a\u548c\u63a8\u7406\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPRAG\u65b9\u6cd5\u91c7\u7528\u4e00\u5bf9\u4e00\u7684\u6587\u6863\u7f16\u7801\u65b9\u6848\uff0c\u6bcf\u4e2a\u6587\u6863\u4f7f\u7528\u4e13\u7528\u7684LoRA\u9002\u914d\u5668\uff0c\u8fd9\u5bfc\u81f4\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff08\u5355\u4e2aLoRA\u9002\u914d\u5668\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u6709\u9650\uff09\u548c\u63a8\u7406\u65f6\u7684\u9ad8\u5f00\u9500\uff08\u9700\u8981\u4e3a\u6bcf\u4e2a\u5019\u9009\u6bb5\u843d\u5408\u5e76LLM\u6743\u91cd\u548c\u65b0\u7684LoRA\u9002\u914d\u5668\uff09\u3002", "method": "\u5728\u79bb\u7ebf\u7f16\u7801\u9636\u6bb5\uff0c\u5c06\u6587\u6863\u96c6\u5408\u7f16\u7801\u89c6\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u8fc7\u7a0b\uff0c\u6bcf\u4e2a\u6bb5\u843d\u5206\u914d\u552f\u4e00\u4efb\u52a1\u6807\u8bc6\u7b26\u3002\u901a\u8fc7\u8def\u7531\u51fd\u6570\u4f7f\u7528\u5c11\u91cf\u6f5c\u5728LoRA\u9002\u914d\u5668\u7f16\u7801\u6574\u4e2a\u6bb5\u843d\u7a7a\u95f4\u3002\u5728\u7ebf\u63a8\u7406\u65f6\uff0c\u8def\u7531\u51fd\u6570\u6839\u636e\u8f93\u5165\u67e5\u8be2\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u6f5c\u5728\u4e13\u5bb6\u5b50\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578bNLP\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cPoly-PRAG\u5728\u56db\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Poly-PRAG\u901a\u8fc7\u6f5c\u5728\u8def\u7531\u7f16\u7801\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfPRAG\u65b9\u6cd5\u7684\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u53c2\u6570\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}

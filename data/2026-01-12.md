<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 23]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [SP-Rank: A Dataset for Ranked Preferences with Secondary Information](https://arxiv.org/abs/2601.05253)
*Hadi Hosseini,Debmalya Mandal,Amrit Puhan*

Main category: cs.IR

TL;DR: SP-Rank是首个大规模公开数据集，用于评估结合一阶偏好和二阶预测的排序算法，包含12,000+数据点，涵盖地理、电影、绘画三个领域，支持九种不同的收集格式。


<details>
  <summary>Details</summary>
Motivation: 传统数据集通常只捕获个体偏好（一阶信号），缺乏对他人偏好预测（二阶信号）的建模。需要数据集来支持研究如何在专家身份未知但假设存在的情况下，从噪声投票中推断共享的真实排序。

Method: 创建SP-Rank数据集，每个数据点包含个人投票（一阶信号）和对他人的投票预测（二阶信号）。数据集包含12,000+人类生成数据点，涵盖三个领域和九种收集格式。提出SP-Voting方法，联合推理两个信号来推断真实排序。

Result: 结合二阶信号显著提高了排序准确性，优于仅使用投票的方法。在三个核心任务上表现优异：完整真实排序恢复、子集级排序恢复、投票者行为的概率建模。

Conclusion: SP-Rank数据集填补了结合一阶和二阶信号进行排序研究的空白，支持社交选择、学习排序、从噪声众包中提取专家知识、偏好微调中的奖励模型训练等应用。公开数据集和代码以促进人类偏好建模、聚合理论和人机对齐研究。

Abstract: We introduce $\mathbf{SP-Rank}$, the first large-scale, publicly available dataset for benchmarking algorithms that leverage both first-order preferences and second-order predictions in ranking tasks. Each datapoint includes a personal vote (first-order signal) and a meta-prediction of how others will vote (second-order signal), allowing richer modeling than traditional datasets that capture only individual preferences. SP-Rank contains over 12,000 human-generated datapoints across three domains -- geography, movies, and paintings, and spans nine elicitation formats with varying subset sizes. This structure enables empirical analysis of preference aggregation when expert identities are unknown but presumed to exist, and individual votes represent noisy estimates of a shared ground-truth ranking. We benchmark SP-Rank by comparing traditional aggregation methods that use only first-order votes against SP-Voting, a second-order method that jointly reasons over both signals to infer ground-truth rankings. While SP-Rank also supports models that rely solely on second-order predictions, our benchmarks emphasize the gains from combining both signals. We evaluate performance across three core tasks: (1) full ground-truth rank recovery, (2) subset-level rank recovery, and (3) probabilistic modeling of voter behavior. Results show that incorporating second-order signals substantially improves accuracy over vote-only methods. Beyond social choice, SP-Rank supports downstream applications in learning-to-rank, extracting expert knowledge from noisy crowds, and training reward models in preference-based fine-tuning pipelines. We release the dataset, code, and baseline evaluations (available at https://github.com/amrit19/SP-Rank-Dataset ) to foster research in human preference modeling, aggregation theory, and human-AI alignment.

</details>


### [2] [TagRAG: Tag-guided Hierarchical Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05254)
*Wenbiao Tao,Yunshi Lan,Weining Qian*

Main category: cs.IR

TL;DR: TagRAG是一个基于标签引导的分层知识图谱RAG框架，通过提取对象标签及其关系构建层次化领域标签链，实现高效全局推理和可扩展的图谱维护，显著提升小语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法依赖片段级检索，难以处理查询聚焦的摘要查询；GraphRAG虽然引入基于图的全局知识推理，但存在信息提取效率低、资源消耗大、增量更新适应性差等问题。

Method: 提出TagRAG框架，包含两个核心组件：1) 标签知识图谱构建 - 从文档中提取对象标签及其关系，组织成层次化领域标签链；2) 标签引导的检索增强生成 - 检索领域中心的标签链来定位和合成相关知识。

Result: 在涵盖农业、计算机科学、法律和跨领域设置的UltraDomain数据集上，TagRAG相对于基线方法平均胜率达到95.41%，同时相比GraphRAG保持约14.6倍的构建效率和1.9倍的检索效率。

Conclusion: TagRAG通过标签引导的分层知识图谱方法，有效解决了传统RAG和GraphRAG的局限性，实现了高效全局推理、更好的检索粒度、对小语言模型的适应性以及高效的知识增量更新。

Abstract: Retrieval-Augmented Generation enhances language models by retrieving external knowledge to support informed and grounded responses. However, traditional RAG methods rely on fragment-level retrieval, limiting their ability to address query-focused summarization queries. GraphRAG introduces a graph-based paradigm for global knowledge reasoning, yet suffers from inefficiencies in information extraction, costly resource consumption, and poor adaptability to incremental updates. To overcome these limitations, we propose TagRAG, a tag-guided hierarchical knowledge graph RAG framework designed for efficient global reasoning and scalable graph maintenance. TagRAG introduces two key components: (1) Tag Knowledge Graph Construction, which extracts object tags and their relationships from documents and organizes them into hierarchical domain tag chains for structured knowledge representation, and (2) Tag-Guided Retrieval-Augmented Generation, which retrieves domain-centric tag chains to localize and synthesize relevant knowledge during inference. This design significantly adapts to smaller language models, improves retrieval granularity, and supports efficient knowledge increment. Extensive experiments on UltraDomain datasets spanning Agriculture, Computer Science, Law, and cross-domain settings demonstrate that TagRAG achieves an average win rate of 95.41\% against baselines while maintaining about 14.6x construction and 1.9x retrieval efficiency compared with GraphRAG.

</details>


### [3] [CourtNav: Voice-Guided, Anchor-Accurate Navigation of Long Legal Documents in Courtrooms](https://arxiv.org/abs/2601.05255)
*Sai Khadloya,Kush Juvekar,Arghya Bhattacharya,Utkarsh Saxena*

Main category: cs.IR

TL;DR: CourtNav是一个语音引导的法律PDF导航系统，通过语音命令快速定位文档中的特定段落，将法官查找相关信息的时间从3-5分钟缩短到10-15秒。


<details>
  <summary>Details</summary>
Motivation: 司法工作需要阅读冗长的法律文件（数百页），但法官在庭审中时间有限，无法详尽阅读所有材料。特别是在印度，判决书和交叉询问记录尤其冗长，需要高效的工具来快速定位关键信息。

Method: 开发了CourtNav系统：1）语音转录法官命令；2）使用语法优先（精确正则匹配）和LLM辅助的路由器分类查询意图；3）在布局感知的混合索引上进行检索；4）自动滚动到目标段落并高亮显示，同时显示相近替代选项。系统设计只显示有依据的段落，不生成自由文本。

Result: 在代表性起诉书、诉状和命令的试点中：中位查找时间从手动导航的3-5分钟降至10-15秒；包含快速视觉验证后为30-45秒。在固定时间预算下，这种导航优先设计增加了实际查阅的记录广度，同时保持了控制和透明度。

Conclusion: CourtNav通过语音导航系统显著提高了法官查阅法律文档的效率，将查找时间大幅缩短，同时保持了证据的可验证性和可审计性，有助于在有限时间内更全面地查阅案件记录。

Abstract: Judicial work depends on close reading of long records, charge sheets, pleadings, annexures, orders, often spanning hundreds of pages. With limited staff support, exhaustive reading during hearings is impractical. We present CourtNav, a voice-guided, anchor-first navigator for legal PDFs that maps a judge's spoken command (e.g., "go to paragraph 23", "highlight the contradiction in the cross-examination") directly to a highlighted paragraph in seconds. CourtNav transcribes the command, classifies intent with a grammar-first(Exact regex matching), LLM-backed router classifying the queries using few shot examples, retrieves over a layout-aware hybrid index, and auto-scrolls the viewer to the cited span while highlighting it and close alternates. By design, the interface shows only grounded passages, never free text, keeping evidence verifiable and auditable. This need is acute in India, where judgments and cross-examinations are notoriously long.In a pilot on representative charge sheets, pleadings, and orders, median time-to-relevance drops from 3-5 minutes (manual navigation) to 10-15 seconds; with quick visual verification included, 30-45 seconds. Under fixed time budgets, this navigation-first design increases the breadth of the record actually consulted while preserving control and transparency.

</details>


### [4] [KP-Agent: Keyword Pruning in Sponsored Search Advertising via LLM-Powered Contextual Bandits](https://arxiv.org/abs/2601.05257)
*Hou-Wan Long,Yicheng Song,Zidong Wang,Tianshu Sun*

Main category: cs.IR

TL;DR: 提出KP-Agent LLM代理系统，通过上下文多臂老虎机框架和强化学习优化关键词修剪，在美团医药广告数据上实现累计利润提升49.28%


<details>
  <summary>Details</summary>
Motivation: 赞助搜索广告中，关键词修剪（优化关键词集以提升广告效果）研究不足，现有实践存在效率低下问题。基于美团平台医药广告商50万条数据发现当前方法需要改进。

Method: 提出KP-Agent LLM代理系统，包含领域工具集和记忆模块。将关键词修剪建模为上下文多臂老虎机问题，通过强化学习生成代码片段来优化关键词集。

Result: 实验显示KP-Agent相比基线方法，累计利润提升最高达49.28%，显著改善了广告活动效果。

Conclusion: KP-Agent系统有效解决了赞助搜索广告中关键词修剪的挑战，通过LLM代理和强化学习框架显著提升了广告活动性能。

Abstract: Sponsored search advertising (SSA) requires advertisers to constantly adjust keyword strategies. While bid adjustment and keyword generation are well-studied, keyword pruning-refining keyword sets to enhance campaign performance-remains under-explored. This paper addresses critical inefficiencies in current practices as evidenced by a dataset containing 0.5 million SSA records from a pharmaceutical advertiser on search engine Meituan, China's largest delivery platform. We propose KP-Agent, an LLM agentic system with domain tool set and a memory module. By modeling keyword pruning within a contextual bandit framework, KP-Agent generates code snippets to refine keyword sets through reinforcement learning. Experiments show KP-Agent improves cumulative profit by up to 49.28% over baselines.

</details>


### [5] [From Events to Trending: A Multi-Stage Hotspots Detection Method Based on Generative Query Indexing](https://arxiv.org/abs/2601.05258)
*Kaichun Wang,Yanguang Chen,Ting Zhang,Mengyao Bao,Keyu Chen,Xu Hu,Yongliang Wang,Jingsheng Yang,Jinsong Zhang,Fei Lu*

Main category: cs.IR

TL;DR: 提出一个面向对话系统的多阶段趋势查询检测框架，通过离线生成和在线识别优化，显著提升新闻相关趋势查询的处理效果


<details>
  <summary>Details</summary>
Motivation: 现有聊天机器人难以有效处理新闻相关趋势查询，而传统搜索引擎的趋势检测方法在对话场景下表现不佳，需要专门针对对话系统场景的趋势检测方法

Method: 多阶段框架：1) 利用热点事件生成索引查询；2) 采用检索匹配机制进行实时在线检测；3) 引入级联召回和排序架构平衡效率与准确性；4) 使用单召回模块作为冷启动策略收集数据微调重排序器

Result: 框架在离线评估和在线A/B测试中显著优于基线方法，用户满意度在正负反馈比方面相对提升27%

Conclusion: 该多阶段趋势检测框架有效解决了对话系统中趋势查询检测的挑战，通过系统化优化显著提升了用户体验

Abstract: LLM-based conversational systems have become a popular gateway for information access, yet most existing chatbots struggle to handle news-related trending queries effectively. To improve user experience, an effective trending query detection method is urgently needed to enable differentiated processing of such target traffic. However, current research on trending detection tailored to the dialogue system scenario remains largely unexplored, and methods designed for traditional search engines often underperform in conversational contexts due to radically distinct query distributions and expression patterns. To fill this gap, we propose a multi-stage framework for trending detection, which achieves systematic optimization from both offline generation and online identification perspectives. Specifically, our framework first exploits selected hot events to generate index queries, establishing a key bridge between static events and dynamic user queries. It then employs a retrieval matching mechanism for real-time online detection of trending queries, where we introduce a cascaded recall and ranking architecture to balance detection efficiency and accuracy. Furthermore, to better adapt to the practical application scenario, our framework adopts a single-recall module as a cold-start strategy to collect online data for fine-tuning the reranker. Extensive experiments demonstrate that our framework significantly outperforms baseline methods in both offline evaluations and online A/B tests, and user satisfaction is relatively improved by 27\% in terms of positive-negative feedback ratio.

</details>


### [6] [A Technical Report on the Second Place Solution for the CIKM 2025 AnalytiCup Competition](https://arxiv.org/abs/2601.05259)
*Haotao Xie,Ruilin Chen,Yicheng Wu,Zhan Zhao,Yuanyuan Liu*

Main category: cs.IR

TL;DR: 提出基于提示工程和思维链任务分解的单模型框架，用于多语言电商搜索分类相关性判断，在保持高精度的同时显著降低计算和维护复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统集成系统虽然能提高多语言电商搜索分类相关性判断的准确性，但存在训练、推理和维护复杂度高的问题，需要更高效、轻量化的解决方案。

Method: 使用思维链任务分解将相关性判断过程分为四个可解释子任务：翻译、意图理解、类别匹配和相关性判断，然后基于Qwen2.5-14B模型通过LoRA进行轻量化微调。

Result: 在CIKM 2025 AnalytiCup竞赛中，公开榜得分0.8902，私有榜得分0.8889；单A100 GPU上处理速度达20样本/秒，实现了高精度和高推理效率。

Conclusion: 结构化提示与轻量化微调结合的单模型框架能够超越复杂集成系统，为可扩展的工业AI应用提供了新范式，在保持竞争力的同时显著降低了计算和存储开销。

Abstract: In this work, we address the challenge of multilingual category relevance judgment in e-commerce search, where traditional ensemble-based systems improve accuracy but at the cost of heavy training, inference, and maintenance complexity. To overcome this limitation, we propose a simplified yet effective framework that leverages prompt engineering with Chain-of-Thought task decomposition to guide reasoning within a single large language model. Specifically, our approach decomposes the relevance judgment process into four interpretable subtasks: translation, intent understanding, category matching, and relevance judgment -- and fine-tunes a base model (Qwen2.5-14B) using Low-Rank Adaptation (LoRA) for efficient adaptation. This design not only reduces computational and storage overhead but also enhances interpretability by explicitly structuring the model's reasoning path. Experimental results show that our single-model framework achieves competitive accuracy and high inference efficiency, processing 20 samples per second on a single A100 GPU. In the CIKM 2025 AnalytiCup Competition Proposals, our method achieved 0.8902 on the public leaderboard and 0.8889 on the private leaderboard, validating the effectiveness and robustness of the proposed approach. These results highlight that structured prompting combined with lightweight fine-tuning can outperform complex ensemble systems, offering a new paradigm for scalable industrial AI applications.

</details>


### [7] [Quantifying Document Impact in RAG-LLMs](https://arxiv.org/abs/2601.05260)
*Armin Gerami,Kazem Faghih,Ramani Duraiswami*

Main category: cs.IR

TL;DR: 本文提出了一种新的度量标准——影响力分数（IS），用于量化检索增强生成（RAG）系统中单个检索文档对最终输出的贡献，解决了当前RAG评估中缺乏文档级影响力量化的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成（RAG）通过连接外部知识提高了LLM的准确性和时效性，但也带来了事实不一致、来源冲突、偏见传播和安全漏洞等问题，削弱了RAG系统的可信度。当前RAG评估的一个关键缺陷是缺乏量化单个检索文档对最终输出贡献的度量标准。

Method: 基于部分信息分解（Partial Information Decomposition）提出了影响力分数（IS）这一新度量标准，用于测量每个检索文档对生成响应的具体影响。通过两个实验验证IS的有效性：1）在三个数据集上进行毒化攻击模拟；2）进行消融研究，比较仅使用IS排名靠前文档生成的响应与使用剩余文档生成的响应。

Result: 1）毒化攻击实验中，IS在86%的情况下正确识别出恶意文档为最具影响力的文档；2）消融研究表明，仅使用IS排名靠前文档生成的响应比使用剩余文档生成的响应更接近原始响应，证明了IS在隔离和量化文档影响力方面的有效性。

Conclusion: 影响力分数（IS）能够有效隔离和量化RAG系统中单个检索文档的影响力，为提高RAG系统的透明度和可靠性提供了有价值的工具，有助于解决当前RAG评估中的关键缺陷。

Abstract: Retrieval Augmented Generation (RAG) enhances Large Language Models (LLMs) by connecting them to external knowledge, improving accuracy and reducing outdated information. However, this introduces challenges such as factual inconsistencies, source conflicts, bias propagation, and security vulnerabilities, which undermine the trustworthiness of RAG systems. A key gap in current RAG evaluation is the lack of a metric to quantify the contribution of individual retrieved documents to the final output. To address this, we introduce the Influence Score (IS), a novel metric based on Partial Information Decomposition that measures the impact of each retrieved document on the generated response. We validate IS through two experiments. First, a poison attack simulation across three datasets demonstrates that IS correctly identifies the malicious document as the most influential in $86\%$ of cases. Second, an ablation study shows that a response generated using only the top-ranked documents by IS is consistently judged more similar to the original response than one generated from the remaining documents. These results confirm the efficacy of IS in isolating and quantifying document influence, offering a valuable tool for improving the transparency and reliability of RAG systems.

</details>


### [8] [Improving User Experience with Personalized Review Ranking and Summarization](https://arxiv.org/abs/2601.05261)
*Muhammad Mufti,Omar Hammad,Mahfuzur Rahman*

Main category: cs.IR

TL;DR: 该研究提出一个个性化框架，整合评论排序和摘要生成，通过分析用户历史评论建立语义偏好档案，匹配新评论并生成个性化摘要，以解决信息过载问题。


<details>
  <summary>Details</summary>
Motivation: 在线消费者评论数量激增导致信息过载，现有排序系统依赖有用性投票、星级评分和时效性等指标，无法捕捉个体用户兴趣，且将文本情感和评分信号分开处理。

Method: 提出个性化框架：1) 通过星级评分和评论内容的混合分析建模用户情感；2) 使用句子嵌入和聚类从历史评论中提取用户偏好，形成语义档案；3) 基于情感和方面相似性的相关性评分算法匹配新评论；4) 对匹配度高的评论生成反映个体兴趣的摘要。

Result: 70名参与者的用户研究表明，个性化方法提高了满意度、感知相关性和决策信心，同时减少了阅读时间，有效缓解了信息过载问题。

Conclusion: 该方法能有效提供符合用户特定偏好的内容，在评论丰富的决策环境中增强用户体验，强调了个性化在提高决策效率方面的重要性。

Abstract: Online consumer reviews play a crucial role in guiding purchase decisions by offering insights into product quality, usability, and performance. However, the increasing volume of user-generated reviews has led to information overload, making it difficult for consumers to identify content that aligns with their specific preferences. Existing review ranking systems typically rely on metrics such as helpfulness votes, star ratings, and recency, but these fail to capture individual user interests and often treat textual sentiment and rating signals separately. This research addresses these limitations by proposing a personalized framework that integrates review ranking and abstractive summarization to enhance decision-making efficiency. The proposed system begins by modeling each user's sentiment through a hybrid analysis of star ratings and review content. Simultaneously, user preferences were derived from historical reviews using sentence embeddings and clustering, forming semantic profiles aligned with thematic and sentiment dimensions. A relevance scoring algorithm matched these profiles with unseen reviews based on sentiment and aspect similarity. Top-matched reviews were then summarized to reflect individual interests. A user study with 70 participants demonstrated that the personalized approach improved satisfaction, perceived relevance, and decision-making confidence, while reducing time spent reading. The results highlight the method's effectiveness in alleviating information overload and delivering content tailored to user-specific preferences, emphasizing its value in enhancing user experience in review-rich decision-making environments.

</details>


### [9] [LLM2IR: simple unsupervised contrastive learning makes long-context LLM great retriever](https://arxiv.org/abs/2601.05262)
*Xiaocong Yang*

Main category: cs.IR

TL;DR: LLM2IR：一种将解码器专用大语言模型转换为信息检索模型的高效无监督对比学习框架，无需大规模预训练，且发现模型上下文长度与IR能力正相关


<details>
  <summary>Details</summary>
Motivation: 现代密集信息检索模型通常依赖昂贵的大规模预训练，需要一种更高效的方法来构建IR模型，同时探索模型上下文长度与IR能力的关系

Method: 提出LLM2IR框架，通过无监督对比学习将任何解码器专用大语言模型转换为信息检索模型，无需大规模预训练

Result: 在不同LLM和多个IR基准测试（LoCo、LongEmbed、BEIR）上证明有效性，发现同一模型家族中上下文长度越长的模型IR能力越强

Conclusion: LLM2IR为在最新LLM上构建IR模型提供了有效方法，揭示了信息检索能力与模型上下文长度的关系，有助于设计更好的信息检索器

Abstract: Modern dense information retrieval (IR) models usually rely on costly large-scale pretraining. In this paper, we introduce LLM2IR, an efficient unsupervised contrastive learning framework to convert any decoder-only large language model (LLM) to an information retrieval model. Despite its simplicity, the effectiveness is proven among different LLMs on multiple IR benchmarks including LoCo, LongEmbed and BEIR. We also find that models with a longer context length tend to have a stronger IR capacity by comparing task performances of models in the same model family. Our work not only provides an effective way to build IR models on the state-of-the-art LLMs, but also shed light on the relationship between information retrieval ability and model context length, which helps the design of better information retrievers.

</details>


### [10] [A General Metric-Space Formulation of the Time Warp Edit Distance (TWED)](https://arxiv.org/abs/2601.05263)
*Zhen Yi Lau*

Main category: cs.IR

TL;DR: 提出广义时间扭曲编辑距离(GTWED)，将TWED从实数时间序列推广到任意度量空间，保持度量性质


<details>
  <summary>Details</summary>
Motivation: 将时间扭曲编辑距离(TWED)从实数时间序列扩展到更一般的度量空间，使其能应用于符号数据、流形、嵌入等任意域上的序列

Method: 将观测域和时间域都视为度量空间(X,d)和(T,Δ)，定义广义TWED(GTWED)，在温和假设下证明其度量性质，并展示经典TWED是其特例

Result: GTWED在温和假设下保持度量性质，经典TWED是当X=ℝ^d、T⊂ℝ、g(x)=x时的特例，为弹性距离在非时间序列数据上的应用提供理论框架

Conclusion: GTWED为TWED类度量在任意域序列上的应用提供了理论扩展，使弹性距离能用于符号数据、流形、嵌入等更广泛的数据类型

Abstract: This short technical note presents a formal generalization of the Time Warp Edit Distance (TWED) proposed by Marteau (2009) to arbitrary metric spaces. By viewing both the observation and temporal domains as metric spaces $(X, d)$ and $(T, Δ)$, we define a Generalized TWED (GTWED) that remains a true metric under mild assumptions. We provide self-contained proofs of its metric properties and show that the classical TWED is recovered as a special case when $X = \mathbb{R}^d$, $T \subset \mathbb{R}$, and $g(x) = x$. This note focuses on the theoretical structure of GTWED and its implications for extending elastic distances beyond time series, which enables the use of TWED-like metrics on sequences over arbitrary domains such as symbolic data, manifolds, or embeddings.

</details>


### [11] [Engineering the RAG Stack: A Comprehensive Review of the Architecture and Trust Frameworks for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2601.05264)
*Dean Wampler,Dave Nielson,Alireza Seddighi*

Main category: cs.IR

TL;DR: 该论文对2018-2025年间的RAG技术进行了系统性文献综述，提供了现代检索增强生成架构的实践指南和详细概述，包括统一分类法、评估框架和部署指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统扩展，RAG提供了一种模块化方法来集成外部知识而无需增加模型容量。然而，由于RAG方法日益多样化（包括各种融合机制、检索策略和编排方法），研究和工程实践变得碎片化，需要系统性的整合和指导。

Method: 采用系统性文献综述方法，整合2018-2025年间的学术研究、工业应用和实际部署案例。通过构建统一分类法来系统整合现有RAG技术，提供定量评估框架，并分析信任和对齐的影响。

Result: 创建了一个全面的RAG技术分类体系，提供了实用的评估框架，分析了RAG系统在信任和对齐方面的影响，并形成了部署弹性、安全和领域适应性RAG系统的实践框架。

Conclusion: 该论文为RAG技术提供了系统性的整合框架和实践指南，既可作为技术参考，也可作为部署可靠RAG系统的实用框架，弥合了学术研究与工业应用之间的鸿沟。

Abstract: This article provides a comprehensive systematic literature review of academic studies, industrial applications, and real-world deployments from 2018 to 2025, providing a practical guide and detailed overview of modern Retrieval-Augmented Generation (RAG) architectures. RAG offers a modular approach for integrating external knowledge without increasing the capacity of the model as LLM systems expand. Research and engineering practices have been fragmented as a result of the increasing diversity of RAG methodologies, which encompasses a variety of fusion mechanisms, retrieval strategies, and orchestration approaches. We provide quantitative assessment frameworks, analyze the implications for trust and alignment, and systematically consolidate existing RAG techniques into a unified taxonomy. This document is a practical framework for the deployment of resilient, secure, and domain-adaptable RAG systems, synthesizing insights from academic literature, industry reports, and technical implementation guides. It also functions as a technical reference.

</details>


### [12] [Cross-Document Topic-Aligned Chunking for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.05265)
*Mile Stankovic*

Main category: cs.IR

TL;DR: CDTA chunking通过跨文档主题对齐解决知识碎片化问题，在HotpotQA上达到0.93忠实度，比现有最佳方法提升12%


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统的分块方法在单个文档内进行，但复杂查询需要跨多个来源的信息，存在知识碎片化问题

Method: 提出跨文档主题对齐分块方法：1) 识别跨文档主题；2) 将文档片段映射到相应主题；3) 合成统一的知识块

Result: 在HotpotQA多跳推理上达到0.93忠实度，比上下文检索(0.83)和语义分块(0.78)显著提升；在k=3时仍保持0.91忠实度，而传统方法降至0.68

Conclusion: 虽然索引成本较高，但合成信息密集的知识块减少了查询时检索需求，对于高查询量且知识分散的应用，跨文档合成优于文档内优化

Abstract: Chunking quality determines RAG system performance. Current methods partition documents individually, but complex queries need information scattered across multiple sources: the knowledge fragmentation problem. We introduce Cross-Document Topic-Aligned (CDTA) chunking, which reconstructs knowledge at the corpus level. It first identifies topics across documents, maps segments to each topic, and synthesizes them into unified chunks.
  On HotpotQA multi-hop reasoning, our method reached 0.93 faithfulness versus 0.83 for contextual retrieval and 0.78 for semantic chunking, a 12% improvement over current industry best practice (p < 0.05). On UAE Legal texts, it reached 0.94 faithfulness with 0.93 citation accuracy. At k = 3, it maintains 0.91 faithfulness while semantic methods drop to 0.68, with a single CDTA chunk containing information requiring multiple traditional fragments.
  Indexing costs are higher, but synthesis produces information-dense chunks that reduce query-time retrieval needs. For high-query-volume applications with distributed knowledge, cross-document synthesis improves measurably over within-document optimization.

</details>


### [13] [Retrieval-Augmented Multi-LLM Ensemble for Industrial Part Specification Extraction](https://arxiv.org/abs/2601.05266)
*Muzakkiruddin Ahmed Mohammed,John R. Talburt,Leon Claasssens,Adriaan Marais*

Main category: cs.IR

TL;DR: 提出RAGsemble框架，通过检索增强的多LLM集成方法，从非结构化文本中提取工业零件规格，显著提升准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 工业零件规格从非结构化文本中提取是制造业、采购和维护中的持续挑战，手动处理耗时且易出错，现有单模型系统存在局限性。

Method: 采用检索增强的多LLM集成框架，整合9个先进LLM（Gemini、OpenAI、Mistral、Gemma系列），构建三阶段流水线：并行提取、针对性研究增强、智能合成与冲突解决，结合FAISS语义检索提供事实数据支持。

Result: 在真实工业数据集上的实验表明，相比领先的单LLM基线，在提取准确性、技术完整性和结构化输出质量方面取得显著提升。

Conclusion: RAGsemble为工业领域提供了可扩展的集成架构、全流程RAG集成、全面的质量评估机制，以及适用于知识密集型制造环境的即用型解决方案。

Abstract: Industrial part specification extraction from unstructured text remains a persistent challenge in manufacturing, procurement, and maintenance, where manual processing is both time-consuming and error-prone. This paper introduces a retrieval-augmented multi-LLM ensemble framework that orchestrates nine state-of-the-art Large Language Models (LLMs) within a structured three-phase pipeline. RAGsemble addresses key limitations of single-model systems by combining the complementary strengths of model families including Gemini (2.0, 2.5, 1.5), OpenAI (GPT-4o, o4-mini), Mistral Large, and Gemma (1B, 4B, 3n-e4b), while grounding outputs in factual data using FAISS-based semantic retrieval. The system architecture consists of three stages: (1) parallel extraction by diverse LLMs, (2) targeted research augmentation leveraging high-performing models, and (3) intelligent synthesis with conflict resolution and confidence-aware scoring. RAG integration provides real-time access to structured part databases, enabling the system to validate, refine, and enrich outputs through similarity-based reference retrieval. Experimental results using real industrial datasets demonstrate significant gains in extraction accuracy, technical completeness, and structured output quality compared to leading single-LLM baselines. Key contributions include a scalable ensemble architecture for industrial domains, seamless RAG integration throughout the pipeline, comprehensive quality assessment mechanisms, and a production-ready solution suitable for deployment in knowledge-intensive manufacturing environments.

</details>


### [14] [Transforming User Defined Criteria into Explainable Indicators with an Integrated LLM AHP System](https://arxiv.org/abs/2601.05267)
*Geonwoo Bang,Dongho Kim,Moohong Min*

Main category: cs.IR

TL;DR: 提出一个可解释的聚合框架，结合LLM评分与层次分析法，用于跨领域复杂文本评估


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：单提示LLM评估面临复杂性和延迟问题，而基于准则分解的方法依赖朴素平均或不透明的黑盒聚合方法，难以将用户定义准则转化为可量化的可解释指标

Method: 提出可解释聚合框架：1) 通过LLM作为评判者生成准则特定分数；2) 使用Jensen-Shannon距离测量判别能力；3) 通过AHP成对比较矩阵推导统计基础权重

Result: 在亚马逊评论质量评估和抑郁相关文本评分实验中，该方法实现了高可解释性和操作效率，同时保持可比的预测能力，适合实时延迟敏感的Web服务

Conclusion: 该方法解决了复杂文本评估中的可解释性和效率问题，为搜索和推荐系统提供了一种实用的解决方案

Abstract: Evaluating complex texts across domains requires converting user defined criteria into quantitative, explainable indicators, which is a persistent challenge in search and recommendation systems. Single prompt LLM evaluations suffer from complexity and latency issues, while criterion specific decomposition approaches rely on naive averaging or opaque black-box aggregation methods. We present an interpretable aggregation framework combining LLM scoring with the Analytic Hierarchy Process. Our method generates criterion specific scores via LLM as judge, measures discriminative power using Jensen Shannon distance, and derives statistically grounded weights through AHP pairwise comparison matrices. Experiments on Amazon review quality assessment and depression related text scoring demonstrate that our approach achieves high explainability and operational efficiency while maintaining comparable predictive power, making it suitable for real time latency sensitive web services.

</details>


### [15] [Separating Semantic Expansion from Linear Geometry for PubMed-Scale Vector Search](https://arxiv.org/abs/2601.05268)
*Rob Koopman*

Main category: cs.IR

TL;DR: 提出PubMed规模检索框架，将语义解释与度量几何分离，使用LLM扩展查询为生物医学短语，在固定、均值自由、近似各向同性的嵌入空间中进行检索，无需训练参数。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统通常将语义理解和几何检索耦合在一起，本文旨在分离这两个方面，通过语言模型处理语义解释，在固定的几何空间中进行高效检索，以实现在大规模生物医学文献（如MEDLINE的4000万条记录）上的精确检索。

Method: 1. 使用大语言模型将自然语言查询扩展为简洁的生物医学短语；2. 在固定的、均值自由、近似各向同性的嵌入空间中进行检索；3. 文档和查询向量通过加权平均词嵌入形成，投影到干扰轴补空间，并通过Johnson-Lindenstrauss变换压缩；4. 在256维int8向量上使用精确余弦搜索；5. 无需训练任何参数。

Result: 系统能够在完整的MEDLINE语料库（约4000万条记录）上检索出连贯的生物医学聚类，使用纯几何评估指标（头部余弦、紧密度、质心闭合度、各向同性）与随机向量基线进行比较，由于语言模型扩展定义了有效目标集，因此未定义召回率。

Conclusion: 该框架成功实现了语义解释与度量几何的分离，通过语言模型处理语义，在固定的几何空间中进行高效检索，为大规模生物医学文献检索提供了一种无需训练参数的有效方法。

Abstract: We describe a PubMed scale retrieval framework that separates semantic interpretation from metric geometry. A large language model expands a natural language query into concise biomedical phrases; retrieval then operates in a fixed, mean free, approximately isotropic embedding space. Each document and query vector is formed as a weighted mean of token embeddings, projected onto the complement of nuisance axes and compressed by a Johnson Lindenstrauss transform. No parameters are trained. The system retrieves coherent biomedical clusters across the full MEDLINE corpus (about 40 million records) using exact cosine search on 256 dimensional int8 vectors. Evaluation is purely geometric: head cosine, compactness, centroid closure, and isotropy are compared with random vector baselines. Recall is not defined, since the language-model expansion specifies the effective target set.

</details>


### [16] [Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach](https://arxiv.org/abs/2601.05269)
*Yoav Evron,Michal Bar-Asher Siegal,Michael Fire*

Main category: cs.IR

TL;DR: 提出一个快速、可扩展的AI流水线，用于检测、提取和描述数字化手稿中的插图，应用于梵蒂冈图书馆等收藏，处理超过300万页，提取20多万张插图，每页处理时间不到0.06秒。


<details>
  <summary>Details</summary>
Motivation: 虽然数字档案提供了前所未有的历史手稿访问途径，但大规模系统研究插图仍然困难。AI革命为人文学科提供了变革可能性，特别是解锁历史手稿中的视觉内容。

Method: 三阶段流水线：1) 微调图像分类模型过滤纯文本页面；2) 高效目标检测模型识别和裁剪插图；3) 多模态图像描述模型生成简洁、人类可读的描述。结果存储在可搜索数据库中。

Result: 应用于超过300万数字化手稿页面，自动识别和提取超过20万张独特插图，每页处理时间不到0.06秒，在效率和可访问性方面显著优于传统分割技术。

Conclusion: 前沿AI工具能够深刻重塑学术工作流程，为数字手稿时代的多学科研究开辟新途径，使学者能够以新的精度和速度探索视觉主题、艺术风格和跨文化影响。

Abstract: The recent Artificial Intelligence (AI) revolution has opened transformative possibilities for the humanities, particularly in unlocking the visual content embedded in historical manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically study illustrations at a large scale remains challenging. Our study presents a fast and scalable AI approach for detecting, extracting, and describing illustrations in digitized manuscripts. Focusing on collections like the Vatican Library, our system enables efficient visual analysis across millions of pages. Our pipeline consists of three stages: (1) a fine-tuned image classification model filters out text-only pages; (2) an efficient object detection model identifies and crops illustrations; and (3) a multimodal image captioning model generates concise, human-readable descriptions. These are stored in a searchable database, allowing scholars to retrieve relevant visual materials through keyword queries. By harnessing the power of recent AI advancements, we enable large-scale visual research that was previously impractical, empowering scholars in historical studies, art history, and cultural heritage to explore visual motifs, artistic styles, and cross-cultural influences with new precision and speed. Applying our pipeline to over three million digitized manuscript pages, we automatically identified and extracted more than 200,000 unique illustrations. This scale of processing in under 0.06 seconds per page, dramatically outperforms traditional segmentation techniques in both efficiency and accessibility for visual scholarship. Our work demonstrates how cutting-edge AI tools can profoundly reshape scholarly workflows and open new avenues for multidisciplinary research in the age of digital manuscripts.

</details>


### [17] [LiveVectorLake: A Real-Time Versioned Knowledge Base Architecture for Streaming Vector Updates and Temporal Retrieval](https://arxiv.org/abs/2601.05270)
*Tarun Prajapati*

Main category: cs.IR

TL;DR: LiveVectorLake提出双层级时间知识库架构，解决RAG系统中向量索引更新效率与数据湖查询延迟的矛盾，实现实时语义搜索和完整版本历史管理。


<details>
  <summary>Details</summary>
Motivation: 现代RAG系统面临架构矛盾：向量索引优化查询延迟但难以处理连续知识更新，数据湖擅长版本管理但引入查询延迟。需要同时优化查询性能、更新效率和合规性。

Method: 采用双层级时间知识库架构：1) 基于SHA-256哈希的内容寻址分块同步；2) 热层向量索引(Milvus+HNSW)与冷层列式版本存储(Delta Lake+Parquet)分离；3) 时间查询路由支持时间点知识检索。

Result: 在100文档5个时间点的评估中：更新时仅需10-15%内容重处理；当前知识检索延迟<100ms；时间查询延迟<2s；通过热冷层分离优化存储成本。

Conclusion: LiveVectorLake解决了RAG系统在查询性能、更新效率和合规性之间的权衡，为需要同时优化这些指标的生产部署提供了可行方案。

Abstract: Modern Retrieval-Augmented Generation (RAG) systems struggle with a fundamental architectural tension: vector indices are optimized for query latency but poorly handle continuous knowledge updates, while data lakes excel at versioning but introduce query latency penalties. We introduce LiveVectorLake, a dual-tier temporal knowledge base architecture that enables real-time semantic search on current knowledge while maintaining complete version history for compliance, auditability, and point-in-time retrieval. The system introduces three core architectural contributions: (1) Content-addressable chunk-level synchronization using SHA-256 hashing for deterministic change detection without external state tracking; (2) Dual-tier storage separating hot-tier vector indices (Milvus with HNSW) from cold-tier columnar versioning (Delta Lake with Parquet), optimizing query latency and storage cost independently; (3) Temporal query routing enabling point-in-time knowledge retrieval via delta-versioning with ACID consistency across tiers. Evaluation on a 100-document corpus versioned across five time points demonstrates: (i) 10-15% re-processing of content during updates compared to 100% for full re-indexing; (ii) sub-100ms retrieval latency on current knowledge; (iii) sub-2s latency for temporal queries across version history; and (iv) storage cost optimization through hot/cold tier separation (only current chunks in expensive vector indices). The approach enables production RAG deployments requiring simultaneous optimization for query performance, update efficiency, and regulatory compliance. Code and resources: [https://github.com/praj-tarun/LiveVectorLake]

</details>


### [18] [RECOR: Reasoning-focused Multi-turn Conversational Retrieval Benchmark](https://arxiv.org/abs/2601.05461)
*Mohammed Ali,Abdelrahman Abdallah,Amit Agarwal,Hitesh Laxmichand Patel,Adam Jatowt*

Main category: cs.IR

TL;DR: 该论文提出了一个用于推理式对话信息检索的基准测试，包含707个对话和11个领域，通过分解与验证框架确保质量，结果显示结合对话历史和推理能显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将多轮对话和推理密集型检索分开处理，但现实世界的信息搜索需要两者结合。为了弥补这一差距，需要建立一个能够评估推理式对话信息检索的基准。

Method: 提出了分解与验证框架，将复杂查询转化为基于事实的多轮对话，通过多层次验证确保质量：原子事实与来源验证，并为每个对话轮次生成显式检索推理。

Result: 结合对话历史和推理能将检索性能提升一倍（基线0.236 → 历史+推理0.479 nDCG@10），推理专用模型显著优于密集编码器。但隐式推理仍然具有挑战性，特别是当逻辑连接未在文本中明确陈述时。

Conclusion: 该研究填补了推理式对话信息检索的基准空白，展示了结合对话历史和推理的重要性，同时指出了隐式推理仍然是未来研究需要解决的关键挑战。

Abstract: Existing benchmarks treat multi-turn conversation and reasoning-intensive retrieval separately, yet real-world information seeking requires both. To bridge this gap, we present a benchmark for reasoning-based conversational information retrieval comprising 707 conversations (2,971 turns) across eleven domains. To ensure quality, our Decomposition-and-Verification framework transforms complex queries into fact-grounded multi-turn dialogues through multi-level validation, where atomic facts are verified against sources and explicit retrieval reasoning is generated for each turn. Comprehensive evaluation reveals that combining conversation history with reasoning doubles retrieval performance (Baseline .236 $\rightarrow$ History+Reasoning .479 nDCG@10), while reasoning-specialized models substantially outperform dense encoders. Despite these gains, further analysis highlights that implicit reasoning remains challenging, particularly when logical connections are not explicitly stated in the text.

</details>


### [19] [LEAPS: An LLM-Empowered Adaptive Plugin for Taobao AI Search](https://arxiv.org/abs/2601.05513)
*Lei Wang,Jinhang Wu,Zhibin Wang,Biye Li,Haiping Hou*

Main category: cs.IR

TL;DR: LEAPS是一个LLM赋能的淘宝AI搜索自适应插件，采用"扩展-精炼"范式，通过上游查询扩展器和下游相关性验证器提升对话式搜索体验，同时保持传统短文本查询的检索性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型改变了用户搜索认知，从离散关键词搜索转向高维对话交互，但现有电商搜索架构难以适应这种变化。用户面临两难：精确自然语言描述常导致零结果，简化查询又会产生噪声过多的通用结果。

Method: 提出LEAPS系统，在搜索管道两端附加插件：1) 上游查询扩展器作为意图翻译器，采用三阶段训练策略（逆数据增强、后验知识监督微调、多样性感知强化学习）生成自适应互补查询组合；2) 下游相关性验证器作为语义把关者，综合多源数据（OCR文本、评论等）并利用思维链推理精确过滤噪声。

Result: 离线和在线A/B测试表明LEAPS显著提升对话式搜索体验。其非侵入式架构既保持了针对短文本查询优化的检索性能，又支持低成本集成到不同后端。自2025年8月全面部署于淘宝AI搜索，每月服务数亿用户。

Conclusion: LEAPS成功解决了电商搜索中精确自然语言查询与噪声结果之间的两难问题，通过"扩展-精炼"范式无缝升级传统搜索系统，为大语言模型时代的电商搜索提供了有效解决方案。

Abstract: The rapid advancement of large language models has reshaped user search cognition, driving a paradigm shift from discrete keyword-based search to high-dimensional conversational interaction. However, existing e-commerce search architectures face a critical capability deficit in adapting to this change. Users are often caught in a dilemma: precise natural language descriptions frequently trigger zero-result scenarios, while the forced simplification of queries leads to decision overload from noisy, generic results. To tackle this challenge, we propose LEAPS (LLM-Empowered Adaptive Plugin for Taobao AI Search), which seamlessly upgrades traditional search systems via a "Broaden-and-Refine" paradigm. Specifically, it attaches plugins to both ends of the search pipeline: (1) Upstream, a Query Expander acts as an intent translator. It employs a novel three-stage training strategy--inverse data augmentation, posterior-knowledge supervised fine-tuning, and diversity-aware reinforcement learning--to generate adaptive and complementary query combinations that maximize the candidate product set. (2) Downstream, a Relevance Verifier serves as a semantic gatekeeper. By synthesizing multi-source data (e.g., OCR text, reviews) and leveraging chain-of-thought reasoning, it precisely filters noise to resolve selection overload. Extensive offline experiments and online A/B testing demonstrate that LEAPS significantly enhances conversational search experiences. Crucially, its non-invasive architecture preserves established retrieval performance optimized for short-text queries, while simultaneously allowing for low-cost integration into diverse back-ends. Fully deployed on Taobao AI Search since August 2025, LEAPS currently serves hundreds of millions of users monthly.

</details>


### [20] [Efficient Temporal-aware Matryoshka Adaptation for Temporal Information Retrieval](https://arxiv.org/abs/2601.05549)
*Tuan-Luc Huynh,Weiqing Wang,Trung Le,Thuy-Trang Vu,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: TMRL是一种高效方法，通过时间感知的套娃嵌入提升检索器的时间相关性，在保持语义表示的同时增强时间编码，实现灵活的效率-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 时间检索增强生成系统中，检索器是关键瓶颈：如果检索不到时间相关的上下文，无论LLM推理能力如何，都会降低下游生成质量。现有方法在时间感知检索方面存在不足。

Method: 提出时间感知套娃表示学习（TMRL），利用套娃嵌入的嵌套结构引入时间子空间，增强时间编码同时保持通用语义表示。该方法能高效适配多种文本嵌入模型。

Result: 实验表明TMRL在时间检索和时间RAG任务上表现优异，相比先前基于套娃的非时间方法和时间方法都有竞争力，同时支持灵活的精度-效率权衡。

Conclusion: TMRL通过时间感知的套娃嵌入有效解决了时间RAG系统中检索器的时间相关性瓶颈问题，为时间敏感的信息检索提供了高效灵活的解决方案。

Abstract: Retrievers are a key bottleneck in Temporal Retrieval-Augmented Generation (RAG) systems: failing to retrieve temporally relevant context can degrade downstream generation, regardless of LLM reasoning. We propose Temporal-aware Matryoshka Representation Learning (TMRL), an efficient method that equips retrievers with temporal-aware Matryoshka embeddings. TMRL leverages the nested structure of Matryoshka embeddings to introduce a temporal subspace, enhancing temporal encoding while preserving general semantic representations. Experiments show that TMRL efficiently adapts diverse text embedding models, achieving competitive temporal retrieval and temporal RAG performance compared to prior Matryoshka-based non-temporal methods and prior temporal methods, while enabling flexible accuracy-efficiency trade-offs.

</details>


### [21] [Autoregressive Ranking: Bridging the Gap Between Dual and Cross Encoders](https://arxiv.org/abs/2601.05588)
*Benjamin Rozonoyer,Chong You,Michael Boratko,Himanshu Jain,Nilesh Gupta,Srinadh Bhojanapalli,Andrew McCallum,Felix Yu*

Main category: cs.IR

TL;DR: 论文提出SToICaL损失函数，用于改进基于LLM的点式生成排序方法，通过token和item级别的校准提升排序性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的排序方法主要依赖下一个token预测损失，这种损失函数本质上是与排序无关的（特别是在点式监督下）。需要一种能够融入排序感知监督的损失函数来提升LLM在信息检索任务中的表现。

Method: 提出SToICaL（Simple Token-Item Calibrated Loss）损失函数，在点式生成排序框架中同时融入item级别和token级别的排序感知监督。该方法通过抑制无效docID生成概率并改进常见排序指标来优化LLM排序性能。

Result: 在WordNet和ESCI数据集上的实验表明，SToICaL的两个变体成功抑制了无效docID的生成概率，并在top-1检索之外的常见排序指标上取得了改进。

Conclusion: SToICaL损失函数能够有效提升基于LLM的点式生成排序方法的性能，通过token和item级别的校准实现了更好的排序效果，为LLM在信息检索任务中的应用提供了新的优化方向。

Abstract: Dual and cross encoders have long been mainstays of information retrieval (IR), but are being challenged by the emergent capabilities of LLMs. An LLM-based approach we term pointwise generative ranking - generating tokens the length of a single docID as opposed to a list in order to enable ranking via beam search - combines efficiency and expressivity benefits while leveraging the in-context capabilities of Causal Transformers. Although there is ample evidence to suggest that pretrained LLMs are well-suited for ranking, we find that the vast majority of LLM-based approaches rely on next-token prediction, a loss function which is fundamentally rank-agnostic (and especially so with pointwise supervision). In this paper, we first prove that the expressivity of pointwise generative ranking with multi-token docIDs is superior to that of dual encoders. We then propose SToICaL - a Simple Token-Item Calibrated Loss - which can incorporate rank-aware supervision at both the item and token levels within the pointwise setup. We run a suite of experiments on ranking tasks derived from WordNet (Fellbaum, 1998) and ESCI (Reddy et al., arXiv:2206.06588). Two variants of SToICaL successfully suppress the probability of invalid docID generations and improve on common ranking metrics beyond top-1 retrieval.

</details>


### [22] [Revisiting Human-vs-LLM judgments using the TREC Podcast Track](https://arxiv.org/abs/2601.05603)
*Watheq Mansour,J. Shane Culpepper,Joel Mackenzie,Andrew Yates*

Main category: cs.IR

TL;DR: LLM标注相关性在播客检索场景下的用户一致性分析，发现多评估者比单一评估者更可靠


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM标注相关性的用户一致性存在争议，且主要关注传统文本检索场景，需要探索在播客音频转录场景下的表现

Method: 使用TREC 2020-2021播客赛道数据集，将音频转录为2分钟片段，用5个不同LLM模型重新评估所有查询-片段对，并对LLM与TREC评估者分歧最大的子集进行重新评估

Result: 人类专家更倾向于同意LLM的标注而非TREC评估者，验证了Sormunen（2002）的发现：依赖单一评估者会导致较低的用户一致性

Conclusion: 在播客检索场景下，LLM标注具有较高可靠性，多评估者机制比单一评估者更可靠，为LLM在音频内容检索中的应用提供了支持

Abstract: Using large language models (LLMs) to annotate relevance is an increasingly important technique in the information retrieval community. While some studies demonstrate that LLMs can achieve high user agreement with ground truth (human) judgments, other studies have argued for the opposite conclusion. To the best of our knowledge, these studies have primarily focused on classic ad-hoc text search scenarios. In this paper, we conduct an analysis on user agreement between LLM and human experts, and explore the impact disagreement has on system rankings. In contrast to prior studies, we focus on a collection composed of audio files that are transcribed into two-minute segments -- the TREC 2020 and 2021 podcast track. We employ five different LLM models to re-assess all of the query-segment pairs, which were originally annotated by TREC assessors. Furthermore, we re-assess a small subset of pairs where LLM and TREC assessors have the highest disagreement, and found that the human experts tend to agree with LLMs more than with the TREC assessors. Our results reinforce the previous insights of Sormunen in 2002 -- that relying on a single assessor leads to lower user agreement.

</details>


### [23] [Statistical Foundations of DIME: Risk Estimation for Practical Index Selection](https://arxiv.org/abs/2601.05649)
*Giulio D'Erasmo,Cesare Campagnano,Antonio Mallia,Pierpaolo Brutti,Nicola Tonellotto,Fabrizio Silvestri*

Main category: cs.IR

TL;DR: 提出一种统计准则，在推理时直接为每个查询识别最优维度集合，无需预选维度，平均减少约50%嵌入大小


<details>
  <summary>Details</summary>
Motivation: 高维密集嵌入存在噪声和冗余维度，现有DIME方法需要昂贵的网格搜索预选维度，且对所有查询使用相同维度

Method: 提出统计准则，在推理时直接为每个查询识别最优维度集合，无需预选维度

Result: 在不同模型和数据集上，保持检索效果的同时，平均减少约50%嵌入大小

Conclusion: 提出的统计准则能有效识别每个查询的最优维度，显著减少嵌入大小而不损失检索效果

Abstract: High-dimensional dense embeddings have become central to modern Information Retrieval, but many dimensions are noisy or redundant. Recently proposed DIME (Dimension IMportance Estimation), provides query-dependent scores to identify informative components of embeddings. DIME relies on a costly grid search to select a priori a dimensionality for all the query corpus's embeddings. Our work provides a statistically grounded criterion that directly identifies the optimal set of dimensions for each query at inference time. Experiments confirm achieving parity of effectiveness and reduces embedding size by an average of $\sim50\%$ across different models and datasets at inference time.

</details>

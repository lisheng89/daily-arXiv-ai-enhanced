{"id": "2602.15189", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.15189", "abs": "https://arxiv.org/abs/2602.15189", "authors": ["William Brach", "Francesco Zuppichini", "Marco Vinciguerra", "Lorenzo Padoan"], "title": "ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction", "comment": null, "summary": "The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.", "AI": {"tldr": "ScrapeGraphAI-100k\u662f\u4e00\u4e2a\u5305\u542b10\u4e07\u6761\u771f\u5b9e\u4e16\u754cLLM\u63d0\u53d6\u4e8b\u4ef6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7f51\u9875\u4fe1\u606f\u63d0\u53d6\u7814\u7a76\uff0c\u652f\u6301\u5c0f\u6a21\u578b\u5fae\u8c03\u3001\u7ed3\u6784\u5316\u63d0\u53d6\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u5f0f\u5f52\u7eb3\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7f51\u9875\u4fe1\u606f\u63d0\u53d6\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u5c0f\u3001\u5408\u6210\u6216\u4ec5\u5305\u542b\u6587\u672c\uff0c\u65e0\u6cd5\u6355\u6349\u7f51\u9875\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u9650\u5236\u4e86\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4fe1\u606f\u68c0\u7d22\u7ba1\u9053\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7ScrapeGraphAI\u9065\u6d4b\u7cfb\u7edf\uff082025\u5e74Q2-Q3\u671f\u95f4\uff09\u6536\u96c6900\u4e07\u6761\u4e8b\u4ef6\uff0c\u7ecf\u8fc7\u53bb\u91cd\u548c\u6a21\u5f0f\u5e73\u8861\u5904\u7406\uff0c\u6700\u7ec8\u5f97\u523093,695\u4e2a\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u5b9e\u4f8b\u5305\u542bMarkdown\u5185\u5bb9\u3001\u63d0\u793a\u8bcd\u3001JSON\u6a21\u5f0f\u3001LLM\u54cd\u5e94\u4ee5\u53ca\u590d\u6742\u5ea6/\u9a8c\u8bc1\u5143\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u591a\u6837\u9886\u57df\u548c\u8bed\u8a00\u7684ScrapeGraphAI-100k\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u6a21\u5f0f\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u8bc1\u660e1.7B\u5c0f\u6a21\u578b\u5728\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u8bad\u7ec3\u540e\u80fd\u7f29\u5c0f\u4e0e30B\u5927\u57fa\u7ebf\u7684\u5dee\u8ddd\u3002", "conclusion": "ScrapeGraphAI-100k\u6570\u636e\u96c6\u4e3a\u5c0f\u6a21\u578b\u5fae\u8c03\u3001\u7ed3\u6784\u5316\u63d0\u53d6\u57fa\u51c6\u6d4b\u8bd5\u548c\u7f51\u9875\u4fe1\u606f\u68c0\u7d22\u7d22\u5f15\u7684\u6a21\u5f0f\u5f52\u7eb3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5df2\u5728HuggingFace\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.15359", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15359", "abs": "https://arxiv.org/abs/2602.15359", "authors": ["Xikai Yang", "Yang Wang", "Yilin Li", "Sebastian Sun"], "title": "Semantics-Aware Denoising: A PLM-Guided Sample Reweighting Strategy for Robust Recommendation", "comment": null, "summary": "Implicit feedback, such as user clicks, serves as the primary data source for modern recommender systems. However, click interactions inherently contain substantial noise, including accidental clicks, clickbait-induced interactions, and exploratory browsing behaviors that do not reflect genuine user preferences. Training recommendation models with such noisy positive samples leads to degraded prediction accuracy and unreliable recommendations. In this paper, we propose SAID (Semantics-Aware Implicit Denoising), a simple yet effective framework that leverages semantic consistency between user interests and item content to identify and downweight potentially noisy interactions. Our approach constructs textual user interest profiles from historical behaviors and computes semantic similarity with target item descriptions using pre-trained language model (PLM) based text encoders. The similarity scores are then transformed into sample weights that modulate the training loss, effectively reducing the impact of semantically inconsistent clicks. Unlike existing denoising methods that require complex auxiliary networks or multi-stage training procedures, SAID only modifies the loss function while keeping the backbone recommendation model unchanged. Extensive experiments on two real-world datasets demonstrate that SAID consistently improves recommendation performance, achieving up to 2.2% relative improvement in AUC over strong baselines, with particularly notable robustness under high noise conditions.", "AI": {"tldr": "SAID\u6846\u67b6\u5229\u7528\u8bed\u4e49\u4e00\u81f4\u6027\u8bc6\u522b\u548c\u964d\u6743\u9690\u5f0f\u53cd\u9988\u4e2d\u7684\u566a\u58f0\u70b9\u51fb\uff0c\u901a\u8fc7PLM\u7f16\u7801\u5668\u8ba1\u7b97\u7528\u6237\u5174\u8da3\u4e0e\u7269\u54c1\u5185\u5bb9\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6837\u672c\u6743\u91cd\u6765\u8c03\u6574\u8bad\u7ec3\u635f\u5931\uff0c\u65e0\u9700\u4fee\u6539\u63a8\u8350\u6a21\u578b\u7ed3\u6784\u5373\u53ef\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u9690\u5f0f\u53cd\u9988\uff08\u5982\u7528\u6237\u70b9\u51fb\uff09\u5305\u542b\u5927\u91cf\u566a\u58f0\uff08\u610f\u5916\u70b9\u51fb\u3001\u6807\u9898\u515a\u8bf1\u5bfc\u70b9\u51fb\u3001\u63a2\u7d22\u6027\u6d4f\u89c8\u7b49\uff09\uff0c\u8fd9\u4e9b\u566a\u58f0\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u7528\u6237\u504f\u597d\uff0c\u5bfc\u81f4\u63a8\u8350\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u4e0b\u964d\u548c\u63a8\u8350\u4e0d\u53ef\u9760\u3002", "method": "SAID\u6846\u67b6\uff1a1\uff09\u4ece\u5386\u53f2\u884c\u4e3a\u6784\u5efa\u6587\u672c\u5316\u7528\u6237\u5174\u8da3\u753b\u50cf\uff1b2\uff09\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u5668\u8ba1\u7b97\u7528\u6237\u5174\u8da3\u4e0e\u76ee\u6807\u7269\u54c1\u63cf\u8ff0\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff1b3\uff09\u5c06\u76f8\u4f3c\u5ea6\u5206\u6570\u8f6c\u5316\u4e3a\u6837\u672c\u6743\u91cd\uff0c\u8c03\u6574\u8bad\u7ec3\u635f\u5931\uff0c\u964d\u4f4e\u8bed\u4e49\u4e0d\u4e00\u81f4\u70b9\u51fb\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u53ea\u4fee\u6539\u635f\u5931\u51fd\u6570\uff0c\u4e0d\u6539\u53d8\u9aa8\u5e72\u63a8\u8350\u6a21\u578b\u7ed3\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAID\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u8350\u6027\u80fd\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5728AUC\u6307\u6807\u4e0a\u83b7\u5f97\u6700\u9ad82.2%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7279\u522b\u663e\u8457\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SAID\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u9690\u5f0f\u53cd\u9988\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u8bc6\u522b\u566a\u58f0\u70b9\u51fb\uff0c\u4ec5\u901a\u8fc7\u635f\u5931\u51fd\u6570\u8c03\u6574\u5c31\u80fd\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd\uff0c\u65e0\u9700\u590d\u6742\u8f85\u52a9\u7f51\u7edc\u6216\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2602.15381", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.15381", "abs": "https://arxiv.org/abs/2602.15381", "authors": ["Sibendu Paul", "Haotian Jiang", "Caren Chen"], "title": "Automatic Funny Scene Extraction from Long-form Cinematic Videos", "comment": null, "summary": "Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u7cfb\u7edf\u81ea\u52a8\u8bc6\u522b\u548c\u6392\u540d\u957f\u7247\u7535\u5f71\u4e2d\u7684\u5e7d\u9ed8\u573a\u666f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u63d0\u5347\u573a\u666f\u5b9a\u4f4d\u548c\u5e7d\u9ed8\u68c0\u6d4b\u6548\u679c", "motivation": "\u4ece\u957f\u7247\u7535\u5f71\u4e2d\u81ea\u52a8\u63d0\u53d6\u9ad8\u8d28\u91cf\u5e7d\u9ed8\u573a\u666f\u5bf9\u4e8e\u521b\u5efa\u5438\u5f15\u4eba\u7684\u89c6\u9891\u9884\u89c8\u548c\u77ed\u89c6\u9891\u5185\u5bb9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u957f\u7247\u7535\u5f71\u7684\u65f6\u957f\u3001\u590d\u6742\u53d9\u4e8b\u4ee5\u53ca\u5e7d\u9ed8\u7684\u591a\u6a21\u6001\u7279\u6027\u5e26\u6765\u4e86\u6311\u6218", "method": "\u7aef\u5230\u7aef\u7cfb\u7edf\u5305\u542b\u955c\u5934\u68c0\u6d4b\u3001\u591a\u6a21\u6001\u573a\u666f\u5b9a\u4f4d\u548c\u5e7d\u9ed8\u6807\u6ce8\uff1b\u521b\u65b0\u5305\u62ec\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7684\u573a\u666f\u5206\u5272\u65b9\u6cd5\u3001\u901a\u8fc7\u5f15\u5bfc\u4e09\u5143\u7ec4\u6316\u6398\u6539\u8fdb\u955c\u5934\u8868\u793a\u3001\u4ee5\u53ca\u5229\u7528\u97f3\u9891\u548c\u6587\u672c\u7684\u591a\u6a21\u6001\u5e7d\u9ed8\u6807\u6ce8\u6846\u67b6", "result": "\u5728OVSD\u6570\u636e\u96c6\u4e0a\u6bd4\u6700\u5148\u8fdb\u573a\u666f\u68c0\u6d4b\u63d0\u534718.3% AP\uff1b\u957f\u6587\u672c\u5e7d\u9ed8\u68c0\u6d4bF1\u5206\u65700.834\uff1b\u5728\u4e94\u90e8\u7535\u5f71\u8bc4\u4f30\u4e2d\uff0c87%\u63d0\u53d6\u7684\u7247\u6bb5\u662f\u6709\u610f\u5e7d\u9ed8\u7684\uff0c98%\u573a\u666f\u51c6\u786e\u5b9a\u4f4d\uff1b\u80fd\u6210\u529f\u63a8\u5e7f\u5230\u9884\u544a\u7247", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6f5c\u529b\u589e\u5f3a\u5185\u5bb9\u521b\u4f5c\u6d41\u7a0b\u3001\u63d0\u9ad8\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5e76\u4e3a\u591a\u6837\u5316\u7684\u7535\u5f71\u5a92\u4f53\u683c\u5f0f\u7b80\u5316\u77ed\u89c6\u9891\u5185\u5bb9\u751f\u6210"}}
{"id": "2602.15423", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15423", "abs": "https://arxiv.org/abs/2602.15423", "authors": ["Rong Fu", "Wenxin Zhang", "Jia Yee Tan", "Chunlei Meng", "Shuo Yin", "Xiaowen Ma", "Wangyu Wu", "Muge Qi", "Guangzhen Yao", "Zhaolu Kang", "Zeli Su", "Simon Fong"], "title": "GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search", "comment": "19 pages, 7 figures", "summary": "As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.", "AI": {"tldr": "GaiaFlow\u662f\u4e00\u4e2a\u78b3\u8282\u7ea6\u578b\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u6269\u6563\u8c03\u4f18\u6765\u5e73\u8861\u641c\u7d22\u7cbe\u5ea6\u4e0e\u73af\u5883\u4fdd\u62a4\uff0c\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u78b3\u8db3\u8ff9\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u67b6\u6784\u7684\u529f\u8017\u9700\u6c42\u6025\u5267\u589e\u957f\uff0c\u4fe1\u606f\u68c0\u7d22\u793e\u533a\u8ba4\u8bc6\u5230\u751f\u6001\u53ef\u6301\u7eed\u6027\u6210\u4e3a\u5173\u952e\u4f18\u5148\u4e8b\u9879\uff0c\u9700\u8981\u6a21\u578b\u8bbe\u8ba1\u7684\u6839\u672c\u8303\u5f0f\u8f6c\u53d8\u3002\u867d\u7136\u73b0\u4ee3\u795e\u7ecf\u6392\u5e8f\u5668\u8fbe\u5230\u4e86\u524d\u6240\u672a\u6709\u7684\u51c6\u786e\u6027\uff0c\u4f46\u5176\u8ba1\u7b97\u5f3a\u5ea6\u5e26\u6765\u7684\u91cd\u5927\u73af\u5883\u5916\u90e8\u6027\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "GaiaFlow\u6846\u67b6\u91c7\u7528\u8bed\u4e49\u5f15\u5bfc\u6269\u6563\u8c03\u4f18\uff0c\u7ed3\u5408\u68c0\u7d22\u5f15\u5bfc\u7684Langevin\u52a8\u529b\u5b66\u548c\u786c\u4ef6\u65e0\u5173\u7684\u6027\u80fd\u5efa\u6a21\u7b56\u7565\uff0c\u4f18\u5316\u641c\u7d22\u7cbe\u5ea6\u4e0e\u73af\u5883\u4fdd\u62a4\u7684\u6743\u8861\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u65e9\u671f\u9000\u51fa\u534f\u8bae\u548c\u7cbe\u5ea6\u611f\u77e5\u91cf\u5316\u63a8\u7406\uff0c\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u78b3\u8db3\u8ff9\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cGaiaFlow\u5728\u6709\u6548\u6027\u548c\u80fd\u6e90\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u795e\u7ecf\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u6301\u7eed\u7684\u8def\u5f84\u3002", "conclusion": "GaiaFlow\u901a\u8fc7\u521b\u65b0\u7684\u78b3\u8282\u7ea6\u8bbe\u8ba1\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u73af\u5883\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u68c0\u7d22\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u78b3\u8db3\u8ff9\u3002"}}
{"id": "2602.15505", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15505", "abs": "https://arxiv.org/abs/2602.15505", "authors": ["Giuseppe Spillo", "Alessandro Petruzzelli", "Cataldo Musto", "Marco de Gemmis", "Pasquale Lops", "Giovanni Semeraro"], "title": "Binge Watch: Reproducible Multimodal Benchmarks Datasets for Large-Scale Movie Recommendation on MovieLens-10M and 20M", "comment": null, "summary": "With the growing interest in Multimodal Recommender Systems (MRSs), collecting high-quality datasets provided with multimedia side information (text, images, audio, video) has become a fundamental step. However, most of the current literature in the field relies on small- or medium-scale datasets that are either not publicly released or built using undocumented processes.\n  In this paper, we aim to fill this gap by releasing M3L-10M and M3L-20M, two large-scale, reproducible, multimodal datasets for the movie domain, obtained by enriching with multimodal features the popular MovieLens-10M and MovieLens-20M, respectively. By following a fully documented pipeline, we collect movie plots, posters, and trailers, from which textual, visual, acoustic, and video features are extracted using several state-of-the-art encoders. We publicly release mappings to download the original raw data, the extracted features, and the complete datasets in multiple formats, fostering reproducibility and advancing the field of MRSs. In addition, we conduct qualitative and quantitative analyses that showcase our datasets across several perspectives.\n  This work represents a foundational step to ensure reproducibility and replicability in the large-scale, multimodal movie recommendation domain. Our resource can be fully accessed at the following link: https://zenodo.org/records/18499145, while the source code is accessible at https://github.com/giuspillo/M3L_10M_20M.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u5e03\u4e86M3L-10M\u548cM3L-20M\u4e24\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7535\u5f71\u63a8\u8350\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u589e\u5f3aMovieLens\u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u7279\u5f81\uff08\u5267\u60c5\u3001\u6d77\u62a5\u3001\u9884\u544a\u7247\uff09\uff0c\u63d0\u4f9b\u6587\u672c\u3001\u89c6\u89c9\u3001\u97f3\u9891\u548c\u89c6\u9891\u7279\u5f81\uff0c\u4fc3\u8fdb\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u590d\u73b0\u6027\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u3001\u53ef\u516c\u5f00\u83b7\u53d6\u4e14\u6784\u5efa\u8fc7\u7a0b\u900f\u660e\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5927\u591a\u6570\u73b0\u6709\u6570\u636e\u96c6\u8981\u4e48\u89c4\u6a21\u8f83\u5c0f\uff0c\u8981\u4e48\u672a\u516c\u5f00\u6216\u6784\u5efa\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u8fd9\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53ef\u590d\u73b0\u6027\u548c\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u57fa\u4e8e\u6d41\u884c\u7684MovieLens-10M\u548cMovieLens-20M\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b8c\u5168\u6587\u6863\u5316\u7684\u6d41\u7a0b\u6536\u96c6\u7535\u5f71\u5267\u60c5\u3001\u6d77\u62a5\u548c\u9884\u544a\u7247\uff0c\u4f7f\u7528\u591a\u79cd\u5148\u8fdb\u7f16\u7801\u5668\u63d0\u53d6\u6587\u672c\u3001\u89c6\u89c9\u3001\u97f3\u9891\u548c\u89c6\u9891\u7279\u5f81\uff0c\u6784\u5efaM3L-10M\u548cM3L-20M\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u4e24\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7535\u5f71\u63a8\u8350\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u539f\u59cb\u6570\u636e\u6620\u5c04\u3001\u63d0\u53d6\u7279\u5f81\u548c\u5b8c\u6574\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u9a8c\u8bc1\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u6240\u6709\u8d44\u6e90\u901a\u8fc7Zenodo\u548cGitHub\u516c\u5f00\u8bbf\u95ee\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u7535\u5f71\u63a8\u8350\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u8d44\u6e90\uff0c\u786e\u4fdd\u4e86\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\u548c\u53ef\u590d\u5236\u6027\uff0c\u5c06\u63a8\u52a8\u591a\u6a21\u6001\u63a8\u8350\u7cfb\u7edf\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.15508", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15508", "abs": "https://arxiv.org/abs/2602.15508", "authors": ["Giuseppe Spillo", "Allegra De Filippo", "Cataldo Musto", "Michela Milano", "Giovanni Semeraro"], "title": "Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations", "comment": null, "summary": "In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.", "AI": {"tldr": "Eco-Amazon\uff1a\u4e3a\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u5f15\u5165\u73af\u5883\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u7684\u65b0\u8d44\u6e90\uff0c\u901a\u8fc7LLM\u96f6\u6837\u672c\u6846\u67b6\u4e3a\u4e9a\u9a6c\u900a\u5546\u54c1\u6dfb\u52a0\u78b3\u8db3\u8ff9\u6570\u636e", "motivation": "\u5f53\u524d\u8d1f\u8d23\u4efb\u548c\u53ef\u6301\u7eedAI\u7814\u7a76\u4e2d\uff0c\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7f3a\u4e4f\u5546\u54c1\u7ea7\u73af\u5883\u5f71\u54cd\u6570\u636e\uff0c\u9650\u5236\u4e86\u73af\u5883\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u7684\u53d1\u5c55", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u6846\u67b6\uff0c\u57fa\u4e8e\u5546\u54c1\u5c5e\u6027\u4f30\u7b97\u4ea7\u54c1\u78b3\u8db3\u8ff9\uff0c\u4e3a\u4e09\u4e2a\u5e38\u7528\u4e9a\u9a6c\u900a\u6570\u636e\u96c6\uff08\u5bb6\u5c45\u3001\u670d\u88c5\u3001\u7535\u5b50\u4ea7\u54c1\uff09\u6dfb\u52a0\u73af\u5883\u5143\u6570\u636e", "result": "\u53d1\u5e03\u4e86Eco-Amazon\u6570\u636e\u96c6\u3001LLM\u78b3\u8db3\u8ff9\u4f30\u7b97\u811a\u672c\u548c\u53ef\u6301\u7eed\u4ea7\u54c1\u63a8\u8350\u7528\u4f8b\uff0c\u586b\u8865\u4e86\u73af\u5883\u6570\u636e\u7a7a\u767d", "conclusion": "Eco-Amazon\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f00\u53d1\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u53ef\u6301\u7eed\u68c0\u7d22\u4e0e\u63a8\u8350\u6a21\u578b\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u63a8\u52a8AI\u5411\u73af\u5883\u53cb\u597d\u65b9\u5411\u53d1\u5c55"}}
{"id": "2602.15659", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15659", "abs": "https://arxiv.org/abs/2602.15659", "authors": ["Luankang Zhang", "Hao Wang", "Zhongzhou Liu", "Mingjia Yin", "Yonghao Huang", "Jiaqi Li", "Wei Guo", "Yong Liu", "Huifeng Guo", "Defu Lian", "Enhong Chen"], "title": "Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control", "comment": null, "summary": "The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with user's approximate preference manifold, and a successor model is augmented on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/r/RSIR-7C5B .", "AI": {"tldr": "RSIR\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u81ea\u6211\u751f\u6210\u8bad\u7ec3\u6570\u636e\u6765\u7f13\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u6570\u636e\u6216\u6559\u5e08\u6a21\u578b\u7684\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u4ea4\u4e92\u6570\u636e\u7684\u6781\u7aef\u7a00\u758f\u6027\u5bfc\u81f4\u4f18\u5316\u666f\u89c2\u5d0e\u5c96\u548c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6210\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6269\u5c55\u7684\u6839\u672c\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u63a8\u8350\uff08RSIR\uff09\u6846\u67b6\uff1a\u6a21\u578b\u5728\u95ed\u73af\u4e2d\u751f\u6210\u53ef\u4fe1\u7684\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\uff0c\u901a\u8fc7\u57fa\u4e8e\u4fdd\u771f\u5ea6\u7684\u8d28\u91cf\u63a7\u5236\u673a\u5236\u7b5b\u9009\u7b26\u5408\u7528\u6237\u504f\u597d\u6d41\u5f62\u7684\u6570\u636e\uff0c\u7136\u540e\u7528\u589e\u5f3a\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u540e\u7ee7\u6a21\u578b\u3002", "result": "RSIR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e00\u81f4\u3001\u7d2f\u79ef\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u53d7\u76ca\uff0c\u5f31\u6a21\u578b\u53ef\u4ee5\u4e3a\u5f3a\u6a21\u578b\u751f\u6210\u6709\u6548\u7684\u8bad\u7ec3\u8bfe\u7a0b\u3002", "conclusion": "\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u662f\u4e00\u79cd\u901a\u7528\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u514b\u670d\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u53ca\u5176\u4ed6\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u524d\u8fdb\u8def\u5f84\u3002"}}
{"id": "2602.15682", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.15682", "abs": "https://arxiv.org/abs/2602.15682", "authors": ["Luankang Zhang", "Hang Lv", "Qiushi Pan", "Kefen Wang", "Yonghao Huang", "Xinrui Miao", "Yin Xu", "Wei Guo", "Yong Liu", "Hao Wang", "Enhong Chen"], "title": "The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service", "comment": null, "summary": "Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u6570\u5b57\u670d\u52a1\u5e94\u4ece\u5e73\u53f0\u4e2d\u5fc3\u8f6c\u5411\u7528\u6237\u4e2d\u5fc3\u4ee3\u7406\uff0c\u5229\u7528LLM\u548c\u7aef\u4fa7\u667a\u80fd\u5b9e\u73b0\u771f\u6b63\u4ee5\u7528\u6237\u5229\u76ca\u4e3a\u4f18\u5148\u7684\u670d\u52a1\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u5e73\u53f0\u4e2d\u5fc3\u7684\u670d\u52a1\u6a21\u5f0f\u4ee5\u5e73\u53f0\u6307\u6807\uff08\u5982\u53c2\u4e0e\u5ea6\u3001\u8f6c\u5316\u7387\uff09\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u5f80\u5f80\u4e0e\u7528\u6237\u771f\u5b9e\u9700\u6c42\u4e0d\u7b26\uff0c\u751a\u81f3\u4e0e\u7528\u6237\u5229\u76ca\u76f8\u51b2\u7a81\u3002\u5e73\u53f0\u6280\u672f\u8fdb\u6b65\uff08\u7279\u522b\u662fLLM\u96c6\u6210\uff09\u5e76\u672a\u771f\u6b63\u8f6c\u5316\u4e3a\u7528\u6237\u5229\u76ca\uff0c\u800c\u662f\u4f18\u5148\u8003\u8651\u670d\u52a1\u63d0\u4f9b\u5546\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u5411\u7528\u6237\u4e2d\u5fc3\u667a\u80fd\u7684\u8f6c\u53d8\uff0c\u63a2\u7d22\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u7684\u673a\u4f1a\u4e0e\u6311\u6218\uff0c\u8bbe\u8ba1\u5b9e\u7528\u7684\u8bbe\u5907-\u4e91\u7aef\u7ba1\u9053\u5b9e\u65bd\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u5fc5\u8981\u7684\u6cbb\u7406\u548c\u751f\u6001\u7cfb\u7edf\u7ed3\u6784\u3002", "result": "\u8bba\u8bc1\u4e86\u7528\u6237\u4e2d\u5fc3\u4ee3\u7406\u7684\u53ef\u884c\u6027\uff0c\u5f3a\u8c03\u5176\u5e94\u5177\u5907\u9690\u79c1\u4fdd\u62a4\u3001\u7528\u6237\u76ee\u6807\u5bf9\u9f50\u3001\u7528\u6237\u63a7\u5236\u7b49\u7279\u6027\uff0c\u5e76\u6307\u51faLLM\u548c\u7aef\u4fa7\u667a\u80fd\u7684\u8fdb\u6b65\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002", "conclusion": "\u6570\u5b57\u670d\u52a1\u7684\u672a\u6765\u5e94\u4ece\u5e73\u53f0\u4e2d\u5fc3\u8f6c\u5411\u7528\u6237\u4e2d\u5fc3\u4ee3\u7406\uff0c\u8fd9\u9700\u8981\u6280\u672f\u5b9e\u73b0\u3001\u6cbb\u7406\u7ed3\u6784\u548c\u751f\u6001\u7cfb\u7edf\u652f\u6301\uff0c\u6700\u7ec8\u5b9e\u73b0\u771f\u6b63\u4ee5\u7528\u6237\u5229\u76ca\u4e3a\u6838\u5fc3\u7684\u6570\u5b57\u670d\u52a1\u6a21\u5f0f\u3002"}}

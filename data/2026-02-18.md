<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [ScrapeGraphAI-100k: A Large-Scale Dataset for LLM-Based Web Information Extraction](https://arxiv.org/abs/2602.15189)
*William Brach,Francesco Zuppichini,Marco Vinciguerra,Lorenzo Padoan*

Main category: cs.IR

TL;DR: ScrapeGraphAI-100k是一个包含10万条真实世界LLM提取事件的大规模数据集，用于网页信息提取研究，支持小模型微调、结构化提取基准测试和模式归纳研究。


<details>
  <summary>Details</summary>
Motivation: 现有网页信息提取数据集通常规模小、合成或仅包含文本，无法捕捉网页的结构化上下文，限制了基于LLM的网页信息检索管道的发展。

Method: 通过ScrapeGraphAI遥测系统（2025年Q2-Q3期间）收集900万条事件，经过去重和模式平衡处理，最终得到93,695个实例，每个实例包含Markdown内容、提示词、JSON模式、LLM响应以及复杂度/验证元数据。

Result: 创建了包含多样领域和语言的ScrapeGraphAI-100k数据集，分析了模式复杂度增加时的失败模式，并通过微调实验证明1.7B小模型在数据集子集上训练后能缩小与30B大基线的差距。

Conclusion: ScrapeGraphAI-100k数据集为小模型微调、结构化提取基准测试和网页信息检索索引的模式归纳研究提供了重要资源，已在HuggingFace公开可用。

Abstract: The use of large language models for web information extraction is becoming increasingly fundamental to modern web information retrieval pipelines. However, existing datasets tend to be small, synthetic or text-only, failing to capture the structural context of the web. We introduce ScrapeGraphAI-100k, a large-scale dataset comprising real-world LLM extraction events, collected via opt-in ScrapeGraphAI telemetry during Q2 and Q3 of 2025. Starting from 9M events, we deduplicate and balance by schema to produce 93,695 examples spanning diverse domains and languages. Each instance includes Markdown content, a prompt, a JSON schema, the LLM response, and complexity/validation metadata. We characterize the datasets structural diversity and its failure modes as schema complexity increases. We also provide a fine-tuning experiment showing that a small language model (1.7B) trained on a subset narrows the gap to larger baselines (30B), underscoring the datasets utility for efficient extraction. ScrapeGraphAI-100k enables fine-tuning small models, benchmarking structured extraction, and studying schema induction for web IR indexing, and is publicly available on HuggingFace.

</details>


### [2] [Semantics-Aware Denoising: A PLM-Guided Sample Reweighting Strategy for Robust Recommendation](https://arxiv.org/abs/2602.15359)
*Xikai Yang,Yang Wang,Yilin Li,Sebastian Sun*

Main category: cs.IR

TL;DR: SAID框架利用语义一致性识别和降权隐式反馈中的噪声点击，通过PLM编码器计算用户兴趣与物品内容的语义相似度，将其转化为样本权重来调整训练损失，无需修改推荐模型结构即可提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈（如用户点击）包含大量噪声（意外点击、标题党诱导点击、探索性浏览等），这些噪声不能反映真实用户偏好，导致推荐模型预测准确性下降和推荐不可靠。

Method: SAID框架：1）从历史行为构建文本化用户兴趣画像；2）使用预训练语言模型编码器计算用户兴趣与目标物品描述的语义相似度；3）将相似度分数转化为样本权重，调整训练损失，降低语义不一致点击的影响。该方法只修改损失函数，不改变骨干推荐模型结构。

Result: 在两个真实世界数据集上的实验表明，SAID能持续提升推荐性能，相比强基线在AUC指标上获得最高2.2%的相对提升，在高噪声条件下表现出特别显著的鲁棒性。

Conclusion: SAID是一个简单有效的隐式反馈去噪框架，通过语义一致性识别噪声点击，仅通过损失函数调整就能显著提升推荐性能，无需复杂辅助网络或多阶段训练流程。

Abstract: Implicit feedback, such as user clicks, serves as the primary data source for modern recommender systems. However, click interactions inherently contain substantial noise, including accidental clicks, clickbait-induced interactions, and exploratory browsing behaviors that do not reflect genuine user preferences. Training recommendation models with such noisy positive samples leads to degraded prediction accuracy and unreliable recommendations. In this paper, we propose SAID (Semantics-Aware Implicit Denoising), a simple yet effective framework that leverages semantic consistency between user interests and item content to identify and downweight potentially noisy interactions. Our approach constructs textual user interest profiles from historical behaviors and computes semantic similarity with target item descriptions using pre-trained language model (PLM) based text encoders. The similarity scores are then transformed into sample weights that modulate the training loss, effectively reducing the impact of semantically inconsistent clicks. Unlike existing denoising methods that require complex auxiliary networks or multi-stage training procedures, SAID only modifies the loss function while keeping the backbone recommendation model unchanged. Extensive experiments on two real-world datasets demonstrate that SAID consistently improves recommendation performance, achieving up to 2.2% relative improvement in AUC over strong baselines, with particularly notable robustness under high noise conditions.

</details>


### [3] [Automatic Funny Scene Extraction from Long-form Cinematic Videos](https://arxiv.org/abs/2602.15381)
*Sibendu Paul,Haotian Jiang,Caren Chen*

Main category: cs.IR

TL;DR: 提出端到端系统自动识别和排名长片电影中的幽默场景，通过多模态方法提升场景定位和幽默检测效果


<details>
  <summary>Details</summary>
Motivation: 从长片电影中自动提取高质量幽默场景对于创建吸引人的视频预览和短视频内容至关重要，但长片电影的时长、复杂叙事以及幽默的多模态特性带来了挑战

Method: 端到端系统包含镜头检测、多模态场景定位和幽默标注；创新包括结合视觉和文本线索的场景分割方法、通过引导三元组挖掘改进镜头表示、以及利用音频和文本的多模态幽默标注框架

Result: 在OVSD数据集上比最先进场景检测提升18.3% AP；长文本幽默检测F1分数0.834；在五部电影评估中，87%提取的片段是有意幽默的，98%场景准确定位；能成功推广到预告片

Conclusion: 该系统有潜力增强内容创作流程、提高用户参与度，并为多样化的电影媒体格式简化短视频内容生成

Abstract: Automatically extracting engaging and high-quality humorous scenes from cinematic titles is pivotal for creating captivating video previews and snackable content, boosting user engagement on streaming platforms. Long-form cinematic titles, with their extended duration and complex narratives, challenge scene localization, while humor's reliance on diverse modalities and its nuanced style add further complexity. This paper introduces an end-to-end system for automatically identifying and ranking humorous scenes from long-form cinematic titles, featuring shot detection, multimodal scene localization, and humor tagging optimized for cinematic content. Key innovations include a novel scene segmentation approach combining visual and textual cues, improved shot representations via guided triplet mining, and a multimodal humor tagging framework leveraging both audio and text. Our system achieves an 18.3% AP improvement over state-of-the-art scene detection on the OVSD dataset and an F1 score of 0.834 for detecting humor in long text. Extensive evaluations across five cinematic titles demonstrate 87% of clips extracted by our pipeline are intended to be funny, while 98% of scenes are accurately localized. With successful generalization to trailers, these results showcase the pipeline's potential to enhance content creation workflows, improve user engagement, and streamline snackable content generation for diverse cinematic media formats.

</details>


### [4] [GaiaFlow: Semantic-Guided Diffusion Tuning for Carbon-Frugal Search](https://arxiv.org/abs/2602.15423)
*Rong Fu,Wenxin Zhang,Jia Yee Tan,Chunlei Meng,Shuo Yin,Xiaowen Ma,Wangyu Wu,Muge Qi,Guangzhen Yao,Zhaolu Kang,Zeli Su,Simon Fong*

Main category: cs.IR

TL;DR: GaiaFlow是一个碳节约型搜索框架，通过语义引导扩散调优来平衡搜索精度与环境保护，显著降低运行碳足迹同时保持检索质量。


<details>
  <summary>Details</summary>
Motivation: 随着神经架构的功耗需求急剧增长，信息检索社区认识到生态可持续性成为关键优先事项，需要模型设计的根本范式转变。虽然现代神经排序器达到了前所未有的准确性，但其计算强度带来的重大环境外部性在大规模部署中常被忽视。

Method: GaiaFlow框架采用语义引导扩散调优，结合检索引导的Langevin动力学和硬件无关的性能建模策略，优化搜索精度与环境保护的权衡。通过自适应早期退出协议和精度感知量化推理，显著降低运行碳足迹。

Result: 广泛的实验评估表明，GaiaFlow在有效性和能源效率之间实现了优越的平衡，为下一代神经搜索系统提供了可扩展且可持续的路径。

Conclusion: GaiaFlow通过创新的碳节约设计，为大规模神经搜索系统提供了环境可持续的解决方案，在保持检索质量的同时显著降低碳足迹。

Abstract: As the burgeoning power requirements of sophisticated neural architectures escalate, the information retrieval community has recognized ecological sustainability as a pivotal priority that necessitates a fundamental paradigm shift in model design. While contemporary neural rankers have attained unprecedented accuracy, the substantial environmental externalities associated with their computational intensity often remain overlooked in large-scale deployments. We present GaiaFlow, an innovative framework engineered to facilitate carbon-frugal search by operationalizing semantic-guided diffusion tuning. Our methodology orchestrates the convergence of retrieval-guided Langevin dynamics and a hardware-independent performance modeling strategy to optimize the trade-off between search precision and environmental preservation. By incorporating adaptive early exit protocols and precision-aware quantized inference, the proposed architecture significantly mitigates operational carbon footprints while maintaining robust retrieval quality across heterogeneous computing infrastructures. Extensive experimental evaluations demonstrate that GaiaFlow achieves a superior equilibrium between effectiveness and energy efficiency, offering a scalable and sustainable pathway for next-generation neural search systems.

</details>


### [5] [Binge Watch: Reproducible Multimodal Benchmarks Datasets for Large-Scale Movie Recommendation on MovieLens-10M and 20M](https://arxiv.org/abs/2602.15505)
*Giuseppe Spillo,Alessandro Petruzzelli,Cataldo Musto,Marco de Gemmis,Pasquale Lops,Giovanni Semeraro*

Main category: cs.IR

TL;DR: 该论文发布了M3L-10M和M3L-20M两个大规模多模态电影推荐数据集，通过增强MovieLens数据集的多模态特征（剧情、海报、预告片），提供文本、视觉、音频和视频特征，促进多模态推荐系统的可复现性研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统研究缺乏高质量、大规模、可公开获取且构建过程透明的多模态数据集，大多数现有数据集要么规模较小，要么未公开或构建过程不透明，这阻碍了该领域的可复现性和研究进展。

Method: 基于流行的MovieLens-10M和MovieLens-20M数据集，通过完全文档化的流程收集电影剧情、海报和预告片，使用多种先进编码器提取文本、视觉、音频和视频特征，构建M3L-10M和M3L-20M数据集。

Result: 成功构建并公开发布了两个大规模多模态电影推荐数据集，提供原始数据映射、提取特征和完整数据集，进行了定性和定量分析验证数据集质量，所有资源通过Zenodo和GitHub公开访问。

Conclusion: 这项工作为大规模多模态电影推荐领域提供了基础性资源，确保了研究的可复现性和可复制性，将推动多模态推荐系统领域的发展。

Abstract: With the growing interest in Multimodal Recommender Systems (MRSs), collecting high-quality datasets provided with multimedia side information (text, images, audio, video) has become a fundamental step. However, most of the current literature in the field relies on small- or medium-scale datasets that are either not publicly released or built using undocumented processes.
  In this paper, we aim to fill this gap by releasing M3L-10M and M3L-20M, two large-scale, reproducible, multimodal datasets for the movie domain, obtained by enriching with multimodal features the popular MovieLens-10M and MovieLens-20M, respectively. By following a fully documented pipeline, we collect movie plots, posters, and trailers, from which textual, visual, acoustic, and video features are extracted using several state-of-the-art encoders. We publicly release mappings to download the original raw data, the extracted features, and the complete datasets in multiple formats, fostering reproducibility and advancing the field of MRSs. In addition, we conduct qualitative and quantitative analyses that showcase our datasets across several perspectives.
  This work represents a foundational step to ensure reproducibility and replicability in the large-scale, multimodal movie recommendation domain. Our resource can be fully accessed at the following link: https://zenodo.org/records/18499145, while the source code is accessible at https://github.com/giuspillo/M3L_10M_20M.

</details>


### [6] [Eco-Amazon: Enriching E-commerce Datasets with Product Carbon Footprint for Sustainable Recommendations](https://arxiv.org/abs/2602.15508)
*Giuseppe Spillo,Allegra De Filippo,Cataldo Musto,Michela Milano,Giovanni Semeraro*

Main category: cs.IR

TL;DR: Eco-Amazon：为信息检索和推荐系统引入环境可持续性评估的新资源，通过LLM零样本框架为亚马逊商品添加碳足迹数据


<details>
  <summary>Details</summary>
Motivation: 当前负责任和可持续AI研究中，信息检索和推荐系统缺乏商品级环境影响数据，限制了环境可持续性评估的发展

Method: 使用大型语言模型零样本框架，基于商品属性估算产品碳足迹，为三个常用亚马逊数据集（家居、服装、电子产品）添加环境元数据

Result: 发布了Eco-Amazon数据集、LLM碳足迹估算脚本和可持续产品推荐用例，填补了环境数据空白

Conclusion: Eco-Amazon为社区提供了开发、基准测试和评估可持续检索与推荐模型的基础资源，推动AI向环境友好方向发展

Abstract: In the era of responsible and sustainable AI, information retrieval and recommender systems must expand their scope beyond traditional accuracy metrics to incorporate environmental sustainability. However, this research line is severely limited by the lack of item-level environmental impact data in standard benchmarks. This paper introduces Eco-Amazon, a novel resource designed to bridge this gap. Our resource consists of an enriched version of three widely used Amazon datasets (i.e., Home, Clothing, and Electronics) augmented with Product Carbon Footprint (PCF) metadata. CO2e emission scores were generated using a zero-shot framework that leverages Large Language Models (LLMs) to estimate item-level PCF based on product attributes. Our contribution is three-fold: (i) the release of the Eco-Amazon datasets, enriching item metadata with PCF signals; (ii) the LLM-based PCF estimation script, which allows researchers to enrich any product catalogue and reproduce our results; (iii) a use case demonstrating how PCF estimates can be exploited to promote more sustainable products. By providing these environmental signals, Eco-Amazon enables the community to develop, benchmark, and evaluate the next generation of sustainable retrieval and recommendation models. Our resource is available at https://doi.org/10.5281/zenodo.18549130, while our source code is available at: http://github.com/giuspillo/EcoAmazon/.

</details>


### [7] [Can Recommender Systems Teach Themselves? A Recursive Self-Improving Framework with Fidelity Control](https://arxiv.org/abs/2602.15659)
*Luankang Zhang,Hao Wang,Zhongzhou Liu,Mingjia Yin,Yonghao Huang,Jiaqi Li,Wei Guo,Yong Liu,Huifeng Guo,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: RSIR框架通过模型自我生成训练数据来缓解推荐系统中的数据稀疏问题，实现无需外部数据或教师模型的递归自我改进。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中用户交互数据的极端稀疏性导致优化景观崎岖和泛化能力差，高质量训练数据的稀缺成为机器学习模型扩展的根本瓶颈。

Method: 提出递归自我改进推荐（RSIR）框架：模型在闭环中生成可信的用户交互序列，通过基于保真度的质量控制机制筛选符合用户偏好流形的数据，然后用增强的数据集训练后继模型。

Result: RSIR在多个基准测试和架构中实现一致、累积的性能提升，小型模型也能受益，弱模型可以为强模型生成有效的训练课程。

Conclusion: 递归自我改进是一种通用、模型无关的方法，能够克服数据稀疏问题，为推荐系统及其他领域提供了可扩展的前进路径。

Abstract: The scarcity of high-quality training data presents a fundamental bottleneck to scaling machine learning models. This challenge is particularly acute in recommendation systems, where extreme sparsity in user interactions leads to rugged optimization landscapes and poor generalization. We propose the Recursive Self-Improving Recommendation (RSIR) framework, a paradigm in which a model bootstraps its own performance without reliance on external data or teacher models. RSIR operates in a closed loop: the current model generates plausible user interaction sequences, a fidelity-based quality control mechanism filters them for consistency with user's approximate preference manifold, and a successor model is augmented on the enriched dataset. Our theoretical analysis shows that RSIR acts as a data-driven implicit regularizer, smoothing the optimization landscape and guiding models toward more robust solutions. Empirically, RSIR yields consistent, cumulative gains across multiple benchmarks and architectures. Notably, even smaller models benefit, and weak models can generate effective training curricula for stronger ones. These results demonstrate that recursive self-improvement is a general, model-agnostic approach to overcoming data sparsity, suggesting a scalable path forward for recommender systems and beyond. Our anonymized code is available at https://anonymous.4open.science/r/RSIR-7C5B .

</details>


### [8] [The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service](https://arxiv.org/abs/2602.15682)
*Luankang Zhang,Hang Lv,Qiushi Pan,Kefen Wang,Yonghao Huang,Xinrui Miao,Yin Xu,Wei Guo,Yong Liu,Hao Wang,Enhong Chen*

Main category: cs.IR

TL;DR: 论文主张数字服务应从平台中心转向用户中心代理，利用LLM和端侧智能实现真正以用户利益为优先的服务模式。


<details>
  <summary>Details</summary>
Motivation: 当前平台中心的服务模式以平台指标（如参与度、转化率）为优化目标，往往与用户真实需求不符，甚至与用户利益相冲突。平台技术进步（特别是LLM集成）并未真正转化为用户利益，而是优先考虑服务提供商目标。

Method: 提出向用户中心智能的转变，探索实现这一愿景的机会与挑战，设计实用的设备-云端管道实施方案，并讨论必要的治理和生态系统结构。

Result: 论证了用户中心代理的可行性，强调其应具备隐私保护、用户目标对齐、用户控制等特性，并指出LLM和端侧智能的进步为实现这一愿景提供了技术基础。

Conclusion: 数字服务的未来应从平台中心转向用户中心代理，这需要技术实现、治理结构和生态系统支持，最终实现真正以用户利益为核心的数字服务模式。

Abstract: Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption.

</details>

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 5]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs](https://arxiv.org/abs/2510.18104)
*Joeran Beel,Bela Gipp,Tobias Vente,Moritz Baumgart,Philipp Meister*

Main category: cs.IR

TL;DR: 提出从窄化的AutoRecSys工具转向全自动推荐系统研究实验室(AutoRecLab)，实现从问题构思到论文撰写的端到端自动化研究流程。


<details>
  <summary>Details</summary>
Motivation: 推荐系统研究在模型和评估方面取得进展，但忽视了研究过程本身的自动化。当前工具仅关注算法选择和超参数调优，缺乏完整的自动化研究流程。

Method: 借鉴自动化科学的最新进展，提出AutoRecLab框架，整合LLM驱动的问题构思、文献分析、实验设计与执行、结果解释、论文撰写和溯源记录。

Result: 为RecSys社区制定了五项议程：构建开放原型、建立基准测试、创建AI生成论文评审渠道、制定归因和可复现性标准、促进跨学科伦理对话。

Conclusion: 推进这一议程可提高研究效率、发现非显而易见的见解，并让RecSys为新兴的人工研究智能做出贡献。呼吁组织社区研讨会来协调后续步骤。

Abstract: Recommender-systems research has accelerated model and evaluation advances,
yet largely neglects automating the research process itself. We argue for a
shift from narrow AutoRecSys tools -- focused on algorithm selection and
hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab
(AutoRecLab) that integrates end-to-end automation: problem ideation,
literature analysis, experimental design and execution, result interpretation,
manuscript drafting, and provenance logging. Drawing on recent progress in
automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems),
we outline an agenda for the RecSys community: (1) build open AutoRecLab
prototypes that combine LLM-driven ideation and reporting with automated
experimentation; (2) establish benchmarks and competitions that evaluate agents
on producing reproducible RecSys findings with minimal human input; (3) create
review venues for transparently AI-generated submissions; (4) define standards
for attribution and reproducibility via detailed research logs and metadata;
and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and
fairness in autonomous research. Advancing this agenda can increase research
throughput, surface non-obvious insights, and position RecSys to contribute to
emerging Artificial Research Intelligence. We conclude with a call to organise
a community retreat to coordinate next steps and co-author guidance for the
responsible integration of automated research systems.

</details>


### [2] [LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling](https://arxiv.org/abs/2510.18239)
*Yunjiang Jiang,Ayush Agarwal,Yang Liu,Bi Xue*

Main category: cs.IR

TL;DR: LIME是一个新颖的推荐系统架构，通过低秩链接嵌入和线性注意力机制，显著降低了计算复杂度，在保持性能的同时实现10倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在推荐系统中面临计算复杂度问题：用户序列长度的二次方增长和候选集大小的线性增长，限制了系统扩展能力。

Method: 采用两个关键创新：1）低秩链接嵌入实现用户和候选交互的解耦，使推理成本几乎独立于候选集大小；2）LIME-XOR线性注意力机制将用户序列长度复杂度从O(N²)降至O(N)。

Result: 在公共和工业数据集上，LIME达到与最先进Transformer相近的性能，但在大候选集或长序列上实现10倍推理加速。在主要推荐平台测试中提高了用户参与度。

Conclusion: LIME为高效且表达能力强的推荐系统建立了新范式，在保持最小推理成本的同时显著提升性能。

Abstract: Scaling large recommendation systems requires advancing three major
frontiers: processing longer user histories, expanding candidate sets, and
increasing model capacity. While promising, transformers' computational cost
scales quadratically with the user sequence length and linearly with the number
of candidates. This trade-off makes it prohibitively expensive to expand
candidate sets or increase sequence length at inference, despite the
significant performance improvements.
  We introduce \textbf{LIME}, a novel architecture that resolves this
trade-off. Through two key innovations, LIME fundamentally reduces
computational complexity. First, low-rank ``link embeddings" enable
pre-computation of attention weights by decoupling user and candidate
interactions, making the inference cost nearly independent of candidate set
size. Second, a linear attention mechanism, \textbf{LIME-XOR}, reduces the
complexity with respect to user sequence length from quadratic ($O(N^2)$) to
linear ($O(N)$).
  Experiments on public and industrial datasets show LIME achieves near-parity
with state-of-the-art transformers but with a 10$\times$ inference speedup on
large candidate sets or long sequence lengths. When tested on a major
recommendation platform, LIME improved user engagement while maintaining
minimal inference costs with respect to candidate set size and user history
length, establishing a new paradigm for efficient and expressive recommendation
systems.

</details>


### [3] [Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization and Query-Driven Insights](https://arxiv.org/abs/2510.18277)
*Nikolaos Belibasakis,Anastasios Giannaros,Ioanna Giannoukou,Spyros Sioutas*

Main category: cs.IR

TL;DR: 开发了一个名为instaGuide的Web应用，利用大语言模型(LLMs)自动分析Booking.com的短租公寓评论，通过总结和挖掘关键信息来改进推荐系统，显著减少用户搜索时间。


<details>
  <summary>Details</summary>
Motivation: 随着预订平台数据量激增，用户难以高效浏览住宿选项和分析评论。虽然平台提供基于星级、设施、价格等的过滤功能，但最有价值的洞察来自非结构化的文本评论，而逐条阅读这些评论耗时且效率低下。

Method: 开发instaGuide工具，自动化从Booking.com提取物业评论的过程，使用LLMs进行评论总结，并允许用户查询物业的特定方面以获取个性化反馈。评估了多个LLM模型在准确性、成本和响应质量方面的表现。

Result: LLM驱动的总结显著减少了用户寻找合适短租公寓所需的时间，改善了整体决策过程。

Conclusion: 大语言模型能够有效增强短租公寓推荐系统，通过自动化评论分析和总结，为用户提供更高效的搜索体验。

Abstract: The increasing number of data a booking platform such as Booking.com and
AirBnB offers make it challenging for interested parties to browse through the
available accommodations and analyze reviews in an efficient way. Efforts have
been made from the booking platform providers to utilize recommender systems in
an effort to enable the user to filter the results by factors such as stars,
amenities, cost but most valuable insights can be provided by the unstructured
text-based reviews. Going through these reviews one-by-one requires a
substantial amount of time to be devoted while a respectable percentage of the
reviews won't provide to the user what they are actually looking for.
  This research publication explores how Large Language Models (LLMs) can
enhance short rental apartments recommendations by summarizing and mining key
insights from user reviews. The web application presented in this paper, named
"instaGuide", automates the procedure of isolating the text-based user reviews
from a property on the Booking.com platform, synthesizing the summary of the
reviews, and enabling the user to query specific aspects of the property in an
effort to gain feedback on their personal questions/criteria.
  During the development of the instaGuide tool, numerous LLM models were
evaluated based on accuracy, cost, and response quality. The results suggest
that the LLM-powered summarization reduces significantly the amount of time the
users need to devote on their search for the right short rental apartment,
improving the overall decision-making procedure.

</details>


### [4] [Evaluating LLM-Based Mobile App Recommendations: An Empirical Study](https://arxiv.org/abs/2510.18364)
*Quim Motger,Xavier Franch,Vincenzo Gervasi,Jordi Marco*

Main category: cs.IR

TL;DR: 该论文对大型语言模型在移动应用推荐中的表现进行实证分析，揭示了其推荐标准与传统应用商店优化指标的部分一致性，以及推荐一致性和对明确指令敏感度的变化规律。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地通过自然语言提示推荐移动应用，但其推荐逻辑不透明，引发了对其一致性、可解释性以及与ASO指标对齐性的质疑。

Method: 通过构建16个通用排名标准的分类法，建立系统性评估框架来分析推荐一致性和对明确排名指令的响应性，并提供可复现的研究包。

Result: 发现LLMs依赖广泛但分散的排名标准，仅部分与标准ASO指标对齐；排名靠前的应用在不同运行中较为一致，但随着排名深度和搜索特异性增加，变异性增大；LLMs对明确排名指令表现出不同程度的敏感性。

Conclusion: 研究结果旨在帮助终端用户、应用开发者和推荐系统研究者在对话式应用发现的新兴领域中更好地导航。

Abstract: Large Language Models (LLMs) are increasingly used to recommend mobile
applications through natural language prompts, offering a flexible alternative
to keyword-based app store search. Yet, the reasoning behind these
recommendations remains opaque, raising questions about their consistency,
explainability, and alignment with traditional App Store Optimization (ASO)
metrics. In this paper, we present an empirical analysis of how widely-used
general purpose LLMs generate, justify, and rank mobile app recommendations.
Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria
elicited from LLM outputs; (ii) a systematic evaluation framework to analyse
recommendation consistency and responsiveness to explicit ranking instructions;
and (iii) a replication package to support reproducibility and future research
on AI-based recommendation systems. Our findings reveal that LLMs rely on a
broad yet fragmented set of ranking criteria, only partially aligned with
standard ASO metrics. While top-ranked apps tend to be consistent across runs,
variability increases with ranking depth and search specificity. LLMs exhibit
varying sensitivity to explicit ranking instructions - ranging from substantial
adaptations to near-identical outputs - highlighting their complex reasoning
dynamics in conversational app discovery. Our results aim to support end-users,
app developers, and recommender-systems researchers in navigating the emerging
landscape of conversational app discovery.

</details>


### [5] [LLMs as Sparse Retrievers:A Framework for First-Stage Product Search](https://arxiv.org/abs/2510.18527)
*Hongru Song,Yu-an Liu,Ruqing Zhang,Jiafeng Guo,Maarten de Rijke,Sen Li,Wenjun Peng,Fuyu Lv,Xueqi Cheng*

Main category: cs.IR

TL;DR: PROSPER是一个利用大语言模型作为稀疏检索器的产品搜索框架，通过字面残差网络缓解幻觉问题，使用词汇聚焦窗口实现有效训练初始化，在保持稀疏检索优势的同时显著提升召回性能。


<details>
  <summary>Details</summary>
Motivation: 产品搜索中稀疏检索方法存在严重的词汇不匹配问题，导致性能不佳。大语言模型具有语义分析潜力，但直接应用会面临幻觉问题和训练初始化困难两大挑战。

Method: 提出PROSPER框架：1) 字面残差网络通过残差补偿机制强化被低估的字面术语；2) 词汇聚焦窗口通过粗到细的稀疏化策略实现有效训练初始化。

Result: 离线和在线实验显示，PROSPER显著优于稀疏基线方法，召回性能可与先进稠密检索器相媲美，同时在线实现了收入增长。

Conclusion: PROSPER成功解决了LLM在稀疏检索中的关键挑战，在保持稀疏检索可解释性和存储效率优势的同时，显著提升了产品搜索的检索质量。

Abstract: Product search is a crucial component of modern e-commerce platforms, with
billions of user queries every day. In product search systems, first-stage
retrieval should achieve high recall while ensuring efficient online
deployment. Sparse retrieval is particularly attractive in this context due to
its interpretability and storage efficiency. However, sparse retrieval methods
suffer from severe vocabulary mismatch issues, leading to suboptimal
performance in product search scenarios.With their potential for semantic
analysis, large language models (LLMs) offer a promising avenue for mitigating
vocabulary mismatch issues and thereby improving retrieval quality. Directly
applying LLMs to sparse retrieval in product search exposes two key
challenges:(1)Queries and product titles are typically short and highly
susceptible to LLM-induced hallucinations, such as generating irrelevant
expansion terms or underweighting critical literal terms like brand names and
model numbers;(2)The large vocabulary space of LLMs leads to difficulty in
initializing training effectively, making it challenging to learn meaningful
sparse representations in such ultra-high-dimensional spaces.To address these
challenges, we propose PROSPER, a framework for PROduct search leveraging LLMs
as SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that
alleviates hallucination in lexical expansion by reinforcing underweighted
literal terms through a residual compensation mechanism; and (2)A lexical
focusing window that facilitates effective training initialization via a
coarse-to-fine sparsification strategy.Extensive offline and online experiments
show that PROSPER significantly outperforms sparse baselines and achieves
recall performance comparable to advanced dense retrievers, while also
achieving revenue increments online.

</details>

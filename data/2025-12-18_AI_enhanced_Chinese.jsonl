{"id": "2512.14733", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.14733", "abs": "https://arxiv.org/abs/2512.14733", "authors": ["Qiang Chen", "Venkatesh Ganapati Hegde"], "title": "Where to Explore: A Reach and Cost-Aware Approach for Unbiased Data Collection in Recommender Systems", "comment": "IEEE Conference on Cognitive Machine Intelligence Nov. 11-14, 2025, Pittsburgh, PA, USA", "summary": "Exploration is essential to improve long-term recommendation quality, but it often degrades short-term business performance, especially in remote-first TV environments where users engage passively, expect instant relevance, and offer few chances for correction. This paper introduces an approach for delivering content-level exploration safely and efficiently by optimizing its placement based on reach and opportunity cost. Deployed on a large-scale streaming platform with over 100 million monthly active users, our approach identifies scroll-depth regions with lower engagement and strategically introduces a dedicated container, the \"Something Completely Different\" row containing randomized content. Rather than enforcing exploration uniformly across the user interface (UI), we condition its appearance on empirically low-cost, high-reach positions to ensure minimal tradeoff against platform-level watch time goals. Extensive A/B testing shows that this strategy preserves business metrics while collecting unbiased interaction data. Our method complements existing intra-row diversification and bandit-based exploration techniques by introducing a deployable, behaviorally informed mechanism for surfacing exploratory content at scale. Moreover, we demonstrate that the collected unbiased data, integrated into downstream candidate generation, significantly improves user engagement, validating its value for recommender systems.", "AI": {"tldr": "\u5728\u6d41\u5a92\u4f53\u5e73\u53f0\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316\u63a2\u7d22\u5185\u5bb9\u7684\u4f4d\u7f6e\uff08\u57fa\u4e8e\u8986\u76d6\u9762\u548c\u673a\u4f1a\u6210\u672c\uff09\uff0c\u5728\u4f4e\u53c2\u4e0e\u5ea6\u7684\u6eda\u52a8\u533a\u57df\u5f15\u5165\"Something Completely Different\"\u968f\u673a\u5185\u5bb9\u884c\uff0c\u65e2\u4fdd\u6301\u4e1a\u52a1\u6307\u6807\u53c8\u6536\u96c6\u65e0\u504f\u4ea4\u4e92\u6570\u636e\u3002", "motivation": "\u63a2\u7d22\u5bf9\u63d0\u5347\u957f\u671f\u63a8\u8350\u8d28\u91cf\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u6d41\u5a92\u4f53\u73af\u5883\u4e2d\u4f1a\u635f\u5bb3\u77ed\u671f\u4e1a\u52a1\u8868\u73b0\uff0c\u56e0\u4e3a\u7528\u6237\u88ab\u52a8\u53c2\u4e0e\u3001\u671f\u671b\u5373\u65f6\u76f8\u5173\u6027\u4e14\u7ea0\u6b63\u673a\u4f1a\u5c11\u3002\u9700\u8981\u627e\u5230\u5b89\u5168\u9ad8\u6548\u7684\u63a2\u7d22\u65b9\u6cd5\u3002", "method": "\u8bc6\u522b\u53c2\u4e0e\u5ea6\u4f4e\u7684\u6eda\u52a8\u6df1\u5ea6\u533a\u57df\uff0c\u6218\u7565\u6027\u5730\u5f15\u5165\u5305\u542b\u968f\u673a\u5185\u5bb9\u7684\u4e13\u7528\u5bb9\u5668\"Something Completely Different\"\u884c\u3002\u6839\u636e\u7ecf\u9a8c\u4f4e\u6210\u672c\u3001\u9ad8\u8986\u76d6\u9762\u7684\u4f4d\u7f6e\u6761\u4ef6\u5316\u663e\u793a\uff0c\u6700\u5c0f\u5316\u4e0e\u5e73\u53f0\u89c2\u770b\u65f6\u95f4\u76ee\u6807\u7684\u6743\u8861\u3002", "result": "A/B\u6d4b\u8bd5\u663e\u793a\u8be5\u7b56\u7565\u4fdd\u6301\u4e1a\u52a1\u6307\u6807\u7684\u540c\u65f6\u6536\u96c6\u65e0\u504f\u4ea4\u4e92\u6570\u636e\u3002\u6536\u96c6\u7684\u65e0\u504f\u6570\u636e\u96c6\u6210\u5230\u4e0b\u6e38\u5019\u9009\u751f\u6210\u4e2d\u663e\u8457\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u884c\u4e3a\u611f\u77e5\u673a\u5236\u5728\u5927\u89c4\u6a21\u4e0a\u5b89\u5168\u9ad8\u6548\u5730\u5c55\u793a\u63a2\u7d22\u5185\u5bb9\uff0c\u8865\u5145\u73b0\u6709\u884c\u5185\u591a\u6837\u5316\u548c\u57fa\u4e8ebandit\u7684\u63a2\u7d22\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u65e0\u504f\u6570\u636e\u5bf9\u63a8\u8350\u7cfb\u7edf\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.15372", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.15372", "abs": "https://arxiv.org/abs/2512.15372", "authors": ["Mikel Williams-Lekuona", "Georgina Cosma"], "title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models", "comment": "Accepted paper for ECIR 2026", "summary": "Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.", "AI": {"tldr": "ICAR\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u50cf\u590d\u6742\u5ea6\u611f\u77e5\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u8ba9\u89c6\u89c9Transformer\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\uff0c\u7b80\u5355\u56fe\u50cf\u4f7f\u7528\u8f83\u5c11\u8ba1\u7b97\uff0c\u590d\u6742\u56fe\u50cf\u4f7f\u7528\u5b8c\u6574\u7f51\u7edc\u6df1\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e8620%\u7684\u5b9e\u9645\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9Transformer\u5bf9\u6240\u6709\u56fe\u50cf\u90fd\u4f7f\u7528\u76f8\u540c\u7684\u8ba1\u7b97\u91cf\uff08175.33 GFLOPs\uff09\uff0c\u65e0\u8bba\u56fe\u50cf\u7b80\u5355\u8fd8\u662f\u590d\u6742\uff0c\u8fd9\u9020\u6210\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u6d6a\u8d39\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6839\u636e\u56fe\u50cf\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\u3002", "method": "1. ICAR\u6846\u67b6\uff1a\u901a\u8fc7\u53cc\u8def\u5f84\u8bad\u7ec3\uff0c\u4f7f\u4e0d\u540c\u5904\u7406\u6df1\u5ea6\u4ea7\u751f\u7684\u5d4c\u5165\u4fdd\u6301\u517c\u5bb9\uff0c\u786e\u4fdd\u56fe\u50cf\u8868\u793a\u4e0e\u6587\u672c\u5d4c\u5165\u5728\u540c\u4e00\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff1b2. ConvNeXt-IC\uff1a\u5c06\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u4f5c\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528\u73b0\u4ee3\u5206\u7c7b\u5668\u9aa8\u5e72\u7f51\u7edc\u800c\u975e\u4e13\u7528\u67b6\u6784\uff0c\u5b9e\u73b0\u590d\u6742\u5ea6\u9884\u6d4b\uff1b3. \u6839\u636e\u590d\u6742\u5ea6\u51b3\u5b9a\u8ba1\u7b97\u91cf\uff0c\u7b80\u5355\u56fe\u50cf\u63d0\u524d\u9000\u51fa\uff0c\u590d\u6742\u56fe\u50cf\u5b8c\u6574\u5904\u7406\u3002", "result": "ConvNeXt-IC\u5728\u56fe\u50cf\u590d\u6742\u5ea6\u8bc4\u4f30\u4e0a\u8fbe\u52300.959\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\uff08\u4eba\u7c7b\u5224\u65ad\uff09\uff0c\u901f\u5ea6\u63d0\u53474.4\u500d\uff1bICAR\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u7f51\u7edc\u6570\u636e\u4e0a\u5b9e\u73b020%\u7684\u5b9e\u9645\u52a0\u901f\uff0c\u4fdd\u6301\u7c7b\u522b\u7ea7\u6027\u80fd\uff0c\u8fbe\u5230\u5b9e\u4f8b\u7ea7\u6027\u80fd\u768495%\u3002", "conclusion": "ICAR\u901a\u8fc7\u56fe\u50cf\u590d\u6742\u5ea6\u611f\u77e5\u7684\u52a8\u6001\u8ba1\u7b97\u5206\u914d\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8bed\u8a00\u68c0\u7d22\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.15384", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.15384", "abs": "https://arxiv.org/abs/2512.15384", "authors": ["Gregor Donabauer", "Samy Ateia", "Udo Kruschwitz", "Maximilian Burger", "Matthias May", "Christian Gilfrich", "Maximilian Haas", "Julio Ruben Rodas Garzaro", "Christoph Eckl"], "title": "MedNuggetizer: Confidence-Based Information Nugget Extraction from Medical Documents", "comment": "Preprint accepted at ECIR 2026", "summary": "We present MedNuggetizer, https://mednugget-ai.de/; access is available upon request.}, a tool for query-driven extraction and clustering of information nuggets from medical documents to support clinicians in exploring underlying medical evidence. Backed by a large language model (LLM), \\textit{MedNuggetizer} performs repeated extractions of information nuggets that are then grouped to generate reliable evidence within and across multiple documents. We demonstrate its utility on the clinical use case of \\textit{antibiotic prophylaxis before prostate biopsy} by using major urological guidelines and recent PubMed studies as sources of information. Evaluation by domain experts shows that \\textit{MedNuggetizer} provides clinicians and researchers with an efficient way to explore long documents and easily extract reliable, query-focused medical evidence.", "AI": {"tldr": "MedNuggetizer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u533b\u5b66\u6587\u6863\u4e2d\u63d0\u53d6\u548c\u805a\u7c7b\u4fe1\u606f\u5757\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u63a2\u7d22\u533b\u5b66\u8bc1\u636e\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u4ece\u5927\u91cf\u533b\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u53ef\u9760\u8bc1\u636e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u6548\u7387\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u5de5\u5177\u80fd\u591f\u81ea\u52a8\u4ece\u957f\u6587\u6863\u4e2d\u63d0\u53d6\u67e5\u8be2\u76f8\u5173\u7684\u4fe1\u606f\u5757\uff0c\u5e76\u7ec4\u7ec7\u6210\u53ef\u63a2\u7d22\u7684\u8bc1\u636e\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u590d\u63d0\u53d6\u4fe1\u606f\u5757\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u4fe1\u606f\u5757\u8fdb\u884c\u5206\u7ec4\u805a\u7c7b\uff0c\u751f\u6210\u53ef\u9760\u8bc1\u636e\u3002\u8be5\u65b9\u6cd5\u53ef\u5728\u5355\u4e2a\u6587\u6863\u5185\u548c\u8de8\u591a\u4e2a\u6587\u6863\u95f4\u5de5\u4f5c\u3002", "result": "\u5728\"\u524d\u5217\u817a\u6d3b\u68c0\u524d\u6297\u751f\u7d20\u9884\u9632\"\u7684\u4e34\u5e8a\u7528\u4f8b\u4e2d\uff0c\u4f7f\u7528\u4e3b\u8981\u6ccc\u5c3f\u79d1\u6307\u5357\u548cPubMed\u7814\u7a76\u4f5c\u4e3a\u4fe1\u606f\u6765\u6e90\u8fdb\u884c\u9a8c\u8bc1\u3002\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u5de5\u5177\u4e3a\u4e34\u5e8a\u533b\u751f\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u63a2\u7d22\u957f\u6587\u6863\u548c\u63d0\u53d6\u53ef\u9760\u3001\u67e5\u8be2\u805a\u7126\u533b\u5b66\u8bc1\u636e\u7684\u9ad8\u6548\u65b9\u6cd5\u3002", "conclusion": "MedNuggetizer\u80fd\u591f\u6709\u6548\u652f\u6301\u4e34\u5e8a\u533b\u751f\u63a2\u7d22\u533b\u5b66\u8bc1\u636e\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u4fe1\u606f\u63d0\u53d6\u548c\u805a\u7c7b\uff0c\u63d0\u9ad8\u4e86\u4ece\u533b\u5b66\u6587\u732e\u4e2d\u83b7\u53d6\u53ef\u9760\u8bc1\u636e\u7684\u6548\u7387\u3002"}}
{"id": "2512.15526", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15526", "abs": "https://arxiv.org/abs/2512.15526", "authors": ["Abdullah Al Munem", "Sumona Yeasmin", "Mohammad Rezwanul Huq"], "title": "BERT and CNN integrated Neural Collaborative Filtering for Recommender Systems", "comment": null, "summary": "Every day, a significant number of users visit the internet for different needs. The owners of a website generate profits from the user interaction with the contents or items of the website. A robust recommendation system can increase user interaction with a website by recommending items according to the user's unique preferences. BERT and CNN-integrated neural collaborative filtering (NCF) have been proposed for the recommendation system in this experiment. The proposed model takes inputs from the user and item profile and finds the user's interest. This model can handle numeric, categorical, and image data to extract the latent features from the inputs. The model is trained and validated on a small sample of the MovieLens dataset for 25 epochs. The same dataset has been used to train and validate a simple NCF and a BERT-based NCF model and compared with the proposed model. The proposed model outperformed those two baseline models. The obtained result for the proposed model is 0.72 recall and 0.486 Hit Ratio @ 10 for 799 users on the MovieLens dataset. This experiment concludes that considering both categorical and image data can improve the performance of a recommendation system.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BERT\u548cCNN\u7684\u795e\u7ecf\u534f\u540c\u8fc7\u6ee4\u6a21\u578b\uff0c\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u6570\u503c\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u6570\u636e\uff0c\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfNCF\u548cBERT-NCF\u6a21\u578b\u3002", "motivation": "\u7f51\u7ad9\u6240\u6709\u8005\u901a\u8fc7\u7528\u6237\u4e0e\u5185\u5bb9\u7684\u4e92\u52a8\u83b7\u5229\uff0c\u800c\u5f3a\u5927\u7684\u63a8\u8350\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u72ec\u7279\u504f\u597d\u63a8\u8350\u9879\u76ee\uff0c\u4ece\u800c\u589e\u52a0\u7528\u6237\u4e92\u52a8\u3002\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u5728\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u80fd\u591f\u540c\u65f6\u5904\u7406\u6570\u503c\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u6570\u636e\u7684\u7efc\u5408\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86BERT\u548cCNN\u96c6\u6210\u7684\u795e\u7ecf\u534f\u540c\u8fc7\u6ee4\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u63a5\u6536\u7528\u6237\u548c\u9879\u76ee\u914d\u7f6e\u6587\u4ef6\u4f5c\u4e3a\u8f93\u5165\uff0c\u80fd\u591f\u5904\u7406\u6570\u503c\u3001\u5206\u7c7b\u548c\u56fe\u50cf\u6570\u636e\u4ee5\u63d0\u53d6\u6f5c\u5728\u7279\u5f81\u3002\u6a21\u578b\u5728MovieLens\u6570\u636e\u96c6\u7684\u5c0f\u6837\u672c\u4e0a\u8fdb\u884c25\u4e2aepoch\u7684\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u5e76\u4e0e\u7b80\u5355\u7684NCF\u6a21\u578b\u548c\u57fa\u4e8eBERT\u7684NCF\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u5bf9799\u540d\u7528\u6237\u53d6\u5f97\u4e860.72\u7684\u53ec\u56de\u7387\u548c0.486\u7684Hit Ratio @ 10\uff0c\u8868\u73b0\u4f18\u4e8e\u4e24\u4e2a\u57fa\u7ebf\u6a21\u578b\uff08\u7b80\u5355NCF\u548cBERT-NCF\uff09\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u540c\u65f6\u8003\u8651\u5206\u7c7b\u6570\u636e\u548c\u56fe\u50cf\u6570\u636e\u53ef\u4ee5\u6539\u5584\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u6355\u6349\u7528\u6237\u5174\u8da3\u3002"}}

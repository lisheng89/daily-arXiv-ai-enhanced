{"id": "2510.12014", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12014", "abs": "https://arxiv.org/abs/2510.12014", "authors": ["Eric He", "Akash Gupta", "Adian Liusie", "Vatsal Raina", "Piotr Molenda", "Shirom Chabra", "Vyas Raina"], "title": "Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval", "comment": null, "summary": "Text--image retrieval is necessary for applications such as product\nrecommendation. Embedding-based approaches like CLIP enable efficient\nlarge-scale retrieval via vector similarity search, but they are primarily\ntrained on literal caption-like text--image pairs and often fail to capture\nabstract or persona-driven attributes common in product recommendation\napplications (e.g., ``a gift for a mother who loves gardening''). In contrast,\nstate-of-the-art vision--language models (vLLMs) can align text with images in\na flexible manner, but their limited context window prevents them from directly\nhandling retrieval over large catalogs. We propose a framework that distills\nthe preference rankings of a powerful vLLM into an embedding-based system,\ntransferring its nuanced alignment abilities while maintaining the\ninference-time scalability of an embedding-based approach. Experiments on\npersona-driven product recommendation tasks demonstrate that our method\nsignificantly outperforms existing embedding-based baselines, providing an\nefficient solution for personalized text--image retrieval."}
{"id": "2510.12054", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12054", "abs": "https://arxiv.org/abs/2510.12054", "authors": ["Wenjin Xie", "Tao Jia"], "title": "MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation", "comment": null, "summary": "With the rapid expansion of scientific literature, scholars increasingly\ndemand precise and high-quality paper recommendations. Among various\nrecommendation methodologies, graph-based approaches have garnered attention by\neffectively exploiting the structural characteristics inherent in scholarly\nnetworks. However, these methods often overlook the asymmetric academic\ninfluence that is prevalent in scholarly networks when learning graph\nrepresentations. To address this limitation, this study proposes the\nMutual-Influence-Aware Recommendation (MIARec) model, which employs a\ngravity-based approach to measure the mutual academic influence between\nscholars and incorporates this influence into the feature aggregation process\nduring message propagation in graph representation learning. Additionally, the\nmodel utilizes a multi-channel aggregation method to capture both individual\nembeddings of distinct single relational sub-networks and their interdependent\nembeddings, thereby enabling a more comprehensive understanding of the\nheterogeneous scholarly network. Extensive experiments conducted on real-world\ndatasets demonstrate that the MIARec model outperforms baseline models across\nthree primary evaluation metrics, indicating its effectiveness in scientific\npaper recommendation tasks."}
{"id": "2510.12211", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12211", "abs": "https://arxiv.org/abs/2510.12211", "authors": ["Junfei Tan", "Yuxin Chen", "An Zhang", "Junguang Jiang", "Bin Liu", "Ziru Xu", "Han Zhu", "Jian Xu", "Bo Zheng", "Xiang Wang"], "title": "Reinforced Preference Optimization for Recommendation", "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have fundamentally\nshifted recommender systems from discriminative to generative paradigms, where\nuser behavior modeling is achieved by generating target items conditioned on\nhistorical interactions. Yet current generative recommenders still suffer from\ntwo core limitations: the lack of high-quality negative modeling and the\nreliance on implicit rewards. Reinforcement learning with verifiable rewards\n(RLVR) offers a natural solution by enabling on-policy sampling of harder\nnegatives and grounding optimization in explicit reward signals. However,\napplying RLVR to generative recommenders remains non-trivial. Its unique\ngeneration space often leads to invalid or repetitive items that undermine\nsampling efficiency, and ranking supervision is sparse since most items receive\nidentical zero rewards. To address these challenges, we propose Reinforced\nPreference Optimization for Recommendation (ReRe), a reinforcement-based\nparadigm tailored to LLM-based recommenders, an important direction in\ngenerative recommendation. ReRe incorporates constrained beam search to improve\nsampling efficiency and diversify hard negatives, while augmenting rule-based\naccuracy rewards with auxiliary ranking rewards for finer-grained supervision.\nExtensive experiments on three real-world datasets demonstrate that ReRe\nconsistently outperforms both traditional and LLM-based recommenders in ranking\nperformance. Further analysis shows that ReRe not only enhances performance\nacross both base and SFT-initialized models but also generalizes robustly\nacross different backbone families and scales. Beyond empirical gains, we\nsystematically investigate the design space of RLVR in recommendation across\ngeneration, sampling strategy, reward modeling, and optimization algorithm,\noffering insights for future research."}
{"id": "2510.12299", "categories": ["cs.IR", "I.2.10"], "pdf": "https://arxiv.org/pdf/2510.12299", "abs": "https://arxiv.org/abs/2510.12299", "authors": ["Zhi Li", "Yanan Wang", "Hao Niu", "Julio Vizcarra", "Masato Taya"], "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs", "comment": "6 pages, 3 figures", "summary": "Multimodal large language models have recently achieved remarkable progress\nin video question answering (VideoQA) by jointly processing visual, textual,\nand audio information. However, it remains unclear which video representations\nare most effective for MLLMs, and how different modalities balance task\naccuracy against computational efficiency. In this work, we present a\ncomprehensive empirical study of video representation methods for VideoQA with\nMLLMs. We systematically evaluate single modality inputs question only,\nsubtitles, visual frames, and audio signals as well as multimodal combinations,\non two widely used benchmarks: VideoMME and LongVideoBench. Our results show\nthat visual frames substantially enhance accuracy but impose heavy costs in GPU\nmemory and inference latency, while subtitles provide a lightweight yet\neffective alternative, particularly for long videos. These findings highlight\nclear trade-offs between effectiveness and efficiency and provide practical\ninsights for designing resource-aware MLLM-based VideoQA systems."}
{"id": "2510.12325", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12325", "abs": "https://arxiv.org/abs/2510.12325", "authors": ["Jie Yang", "Chenyang Gu", "Zixuan Liu"], "title": "Causal Inspired Multi Modal Recommendation", "comment": null, "summary": "Multimodal recommender systems enhance personalized recommendations in\ne-commerce and online advertising by integrating visual, textual, and user-item\ninteraction data. However, existing methods often overlook two critical biases:\n(i) modal confounding, where latent factors (e.g., brand style or product\ncategory) simultaneously drive multiple modalities and influence user\npreference, leading to spurious feature-preference associations; (ii)\ninteraction bias, where genuine user preferences are mixed with noise from\nexposure effects and accidental clicks. To address these challenges, we propose\na Causal-inspired multimodal Recommendation framework. Specifically, we\nintroduce a dual-channel cross-modal diffusion module to identify hidden modal\nconfounders, utilize back-door adjustment with hierarchical matching and\nvector-quantized codebooks to block confounding paths, and apply front-door\nadjustment combined with causal topology reconstruction to build a deconfounded\ncausal subgraph. Extensive experiments on three real-world e-commerce datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines while maintaining strong interpretability."}
{"id": "2510.12327", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12327", "abs": "https://arxiv.org/abs/2510.12327", "authors": ["Benjamin Clavi√©", "Sean Lee", "Rikiya Takehi", "Aamir Shakir", "Makoto P. Kato"], "title": "Simple Projection Variants Improve ColBERT Performance", "comment": null, "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a\nsingle-layer linear projection to reduce the dimensionality of individual\nvectors. In this study, we explore the implications of the MaxSim operator on\nthe gradient flows of the training of multi-vector models and show that such a\nsimple linear projection has inherent, if non-critical, limitations in this\nsetting. We then discuss the theoretical improvements that could result from\nreplacing this single-layer projection with well-studied alternative\nfeedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU\nblocks, and skip-connections, could alleviate these limitations. Through the\ndesign and systematic evaluation of alternate projection blocks, we show that\nbetter-designed final projections positively impact the downstream performance\nof ColBERT models. We highlight that many projection variants outperform the\noriginal linear projections, with the best-performing variants increasing\naverage performance on a range of retrieval benchmarks across domains by over 2\nNDCG@10 points. We then conduct further exploration on the individual\nparameters of these projections block in order to understand what drives this\nempirical performance, highlighting the particular importance of upscaled\nintermediate projections and residual connections. As part of these ablation\nstudies, we show that numerous suboptimal projection variants still outperform\nthe traditional single-layer projection across multiple benchmarks, confirming\nour hypothesis. Finally, we observe that this effect is consistent across\nrandom seeds, further confirming that replacing the linear layer of ColBERT\nmodels is a robust, drop-in upgrade."}
{"id": "2510.12369", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12369", "abs": "https://arxiv.org/abs/2510.12369", "authors": ["Yang Xiang", "Li Fan", "Chenke Yin", "Chengtao Ji"], "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph Representation Learning", "comment": null, "summary": "Recent progress in language and vision foundation models demonstrates the\nimportance of discrete token interfaces that transform complex inputs into\ncompact sequences for large-scale modeling. Extending this paradigm to graphs\nrequires a tokenization scheme that handles non-Euclidean structures and\nmulti-scale dependencies efficiently. Existing approaches to graph\ntokenization, linearized, continuous, and quantized, remain limited in\nadaptability and efficiency. In particular, most current quantization-based\ntokenizers organize hierarchical information in fixed or task-agnostic ways,\nwhich may either over-represent or under-utilize structural cues, and lack the\nability to dynamically reweight contributions from different levels without\nretraining the encoder. This work presents a hierarchical quantization\nframework that introduces a self-weighted mechanism for task-adaptive\naggregation across multiple scales. The proposed method maintains a frozen\nencoder while modulating information flow through a lightweight gating process,\nenabling parameter-efficient adaptation to diverse downstream tasks.\nExperiments on benchmark datasets for node classification and link prediction\ndemonstrate consistent improvements over strong baselines under comparable\ncomputational budgets."}
{"id": "2510.12461", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.12461", "abs": "https://arxiv.org/abs/2510.12461", "authors": ["Andrei Chernov", "Haroon Wahab", "Oleg Novitskij"], "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance", "comment": null, "summary": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}."}
{"id": "2510.12604", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12604", "abs": "https://arxiv.org/abs/2510.12604", "authors": ["Qihang Zhao", "Zhongbo Sun", "Xiaoyang Zheng", "Xian Guo", "Siyuan Wang", "Zihan Liang", "Mingcan Peng", "Ben Chen", "Chenyi Lei"], "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch", "comment": null, "summary": "With the rise of modern search and recommendation platforms, insufficient\ncollaborative information of cold-start items exacerbates the Matthew effect of\nexisting platform items, challenging platform diversity and becoming a\nlongstanding issue. Existing methods align items' side content with\ncollaborative information to transfer collaborative signals from\nhigh-popularity items to cold-start items. However, these methods fail to\naccount for the asymmetry between collaboration and content, nor the\nfine-grained differences among items. To address these issues, we propose\nSMILE, an item representation enhancement approach based on fused alignment of\nsemantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and\ncollaborative information, followed by a two-step alignment: RQ encoding\ntransfers shared collaborative signals across items, while OPQ encoding learns\ndifferentiated information of items. Comprehensive offline experiments on\nlarge-scale industrial datasets demonstrate superiority of SMILE, and rigorous\nonline A/B tests confirm statistically significant improvements: item CTR\n+1.66%, buyers +1.57%, and order volume +2.17%."}
{"id": "2510.12668", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.12668", "abs": "https://arxiv.org/abs/2510.12668", "authors": ["Minghao Tang", "Shiyu Ni", "Jingtong Wu", "Zengxin Han", "Keping Bi"], "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG."}
{"id": "2510.12709", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12709", "abs": "https://arxiv.org/abs/2510.12709", "authors": ["Lin Lin", "Jiefeng Long", "Zhihe Wan", "Yuchi Wang", "Dingkang Yang", "Shuang Yang", "Yueyang Yao", "Xu Chen", "Zirui Guo", "Shengqiang Li", "Weiran Li", "Hanyu Li", "Yaling Mou", "Yan Qiu", "Haiyang Yu", "Xiao Liang", "Hongsheng Li", "Chao Feng"], "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model", "comment": "Technical Report", "summary": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the\nDouyin-Selected scenario. For the Douyin feed rank model, the match features\nproduced by SAIL-Embedding yield a +0.08% AUC gain."}

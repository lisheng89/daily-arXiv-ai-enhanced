{"id": "2601.08901", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08901", "abs": "https://arxiv.org/abs/2601.08901", "authors": ["Yuexi Shen", "Minqian Liu", "Dawei Zhou", "Lifu Huang"], "title": "Navigating Ideation Space: Decomposed Conceptual Representations for Positioning Scientific Ideas", "comment": "21 pages, 6 tables", "summary": "Scientific discovery is a cumulative process and requires new ideas to be situated within an ever-expanding landscape of existing knowledge. An emerging and critical challenge is how to identify conceptually relevant prior work from rapidly growing literature, and assess how a new idea differentiates from existing research. Current embedding approaches typically conflate distinct conceptual aspects into single representations and cannot support fine-grained literature retrieval; meanwhile, LLM-based evaluators are subject to sycophancy biases, failing to provide discriminative novelty assessment. To tackle these challenges, we introduce the Ideation Space, a structured representation that decomposes scientific knowledge into three distinct dimensions, i.e., research problem, methodology, and core findings, each learned through contrastive training. This framework enables principled measurement of conceptual distance between ideas, and modeling of ideation transitions that capture the logical connections within a proposed idea. Building upon this representation, we propose a Hierarchical Sub-Space Retrieval framework for efficient, targeted literature retrieval, and a Decomposed Novelty Assessment algorithm that identifies which aspects of an idea are novel. Extensive experiments demonstrate substantial improvements, where our approach achieves Recall@30 of 0.329 (16.7% over baselines), our ideation transition retrieval reaches Hit Rate@30 of 0.643, and novelty assessment attains 0.37 correlation with expert judgments. In summary, our work provides a promising paradigm for future research on accelerating and evaluating scientific discovery.", "AI": {"tldr": "\u63d0\u51faIdeation Space\u6846\u67b6\uff0c\u5c06\u79d1\u5b66\u77e5\u8bc6\u5206\u89e3\u4e3a\u7814\u7a76\u95ee\u9898\u3001\u65b9\u6cd5\u548c\u6838\u5fc3\u53d1\u73b0\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6587\u732e\u68c0\u7d22\u548c\u5206\u89e3\u5f0f\u65b0\u9896\u6027\u8bc4\u4f30", "motivation": "\u79d1\u5b66\u53d1\u73b0\u662f\u7d2f\u79ef\u8fc7\u7a0b\uff0c\u9700\u8981\u5c06\u65b0\u60f3\u6cd5\u7f6e\u4e8e\u4e0d\u65ad\u6269\u5c55\u7684\u73b0\u6709\u77e5\u8bc6\u4f53\u7cfb\u4e2d\u3002\u5f53\u524d\u5d4c\u5165\u65b9\u6cd5\u5c06\u4e0d\u540c\u6982\u5ff5\u65b9\u9762\u6df7\u4e3a\u5355\u4e00\u8868\u793a\uff0c\u65e0\u6cd5\u652f\u6301\u7ec6\u7c92\u5ea6\u6587\u732e\u68c0\u7d22\uff1bLLM\u8bc4\u4f30\u5668\u5b58\u5728\u5949\u627f\u504f\u89c1\uff0c\u65e0\u6cd5\u63d0\u4f9b\u533a\u5206\u6027\u65b0\u9896\u6027\u8bc4\u4f30", "method": "\u5f15\u5165Ideation Space\u7ed3\u6784\u5316\u8868\u793a\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u79d1\u5b66\u77e5\u8bc6\u5206\u89e3\u4e3a\u7814\u7a76\u95ee\u9898\u3001\u65b9\u6cd5\u548c\u6838\u5fc3\u53d1\u73b0\u4e09\u4e2a\u7ef4\u5ea6\uff1b\u63d0\u51fa\u5206\u5c42\u5b50\u7a7a\u95f4\u68c0\u7d22\u6846\u67b6\u8fdb\u884c\u9ad8\u6548\u9488\u5bf9\u6027\u6587\u732e\u68c0\u7d22\uff0c\u4ee5\u53ca\u5206\u89e3\u5f0f\u65b0\u9896\u6027\u8bc4\u4f30\u7b97\u6cd5\u8bc6\u522b\u60f3\u6cd5\u7684\u54ea\u4e9b\u65b9\u9762\u662f\u65b0\u9896\u7684", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1aRecall@30\u8fbe\u52300.329\uff08\u6bd4\u57fa\u7ebf\u63d0\u534716.7%\uff09\uff0c\u6784\u601d\u8f6c\u6362\u68c0\u7d22\u7684Hit Rate@30\u8fbe\u52300.643\uff0c\u65b0\u9896\u6027\u8bc4\u4f30\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u76f8\u5173\u6027\u8fbe\u52300.37", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u52a0\u901f\u548c\u8bc4\u4f30\u79d1\u5b66\u53d1\u73b0\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u8303\u5f0f"}}
{"id": "2601.08919", "categories": ["cs.IR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08919", "abs": "https://arxiv.org/abs/2601.08919", "authors": ["Sourav Saha", "Mandar Mitra"], "title": "Fine Grained Evaluation of LLMs-as-Judges", "comment": null, "summary": "A good deal of recent research has focused on how Large Language Models\n  (LLMs) may be used as `judges' in place of humans to evaluate the quality\n  of the output produced by various text / image processing systems. Within\n  this broader context, a number of studies have investigated the specific\n  question of how effectively LLMs can be used as relevance assessors for\n  the standard ad hoc task in Information Retrieval (IR). We extend these\n  studies by looking at additional questions. Most importantly, we use a\n  Wikipedia based test collection created by the INEX initiative, and\n  prompt LLMs to not only judge whether documents are relevant /\n  non-relevant, but to highlight relevant passages in documents that it\n  regards as useful. The human relevance assessors involved in creating\n  this collection were given analogous instructions, i.e., they were asked\n  to highlight all passages within a document that respond to the\n  information need expressed in a query. This enables us to evaluate the\n  quality of LLMs as judges not only at the document level, but to also\n  quantify how often these `judges' are right for the right reasons.\n  Our findings suggest that LLMs-as-judges work best under human\n  supervision.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4fe1\u606f\u68c0\u7d22\u76f8\u5173\u6027\u8bc4\u4f30\u8005\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u7ea7\u522b\u548c\u6bb5\u843d\u7ea7\u522b\u8bc4\u4f30\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5173\u6ce8LLM\u4f5c\u4e3a\"\u6cd5\u5b98\"\u66ff\u4ee3\u4eba\u7c7b\u8bc4\u4f30\u6587\u672c/\u56fe\u50cf\u5904\u7406\u7cfb\u7edf\u8f93\u51fa\u7684\u8d28\u91cf\u3002\u672c\u7814\u7a76\u6269\u5c55\u4e86\u73b0\u6709\u7814\u7a76\uff0c\u7279\u522b\u5173\u6ce8LLM\u5728\u4fe1\u606f\u68c0\u7d22\u6807\u51c6\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u76f8\u5173\u6027\u8bc4\u4f30\u8005\u7684\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22LLM\u80fd\u5426\u50cf\u4eba\u7c7b\u8bc4\u4f30\u8005\u4e00\u6837\u8bc6\u522b\u6587\u6863\u4e2d\u7684\u76f8\u5173\u6bb5\u843d\u3002", "method": "\u4f7f\u7528INEX\u5021\u8bae\u521b\u5efa\u7684\u57fa\u4e8e\u7ef4\u57fa\u767e\u79d1\u7684\u6d4b\u8bd5\u96c6\uff0c\u63d0\u793aLLM\u4e0d\u4ec5\u5224\u65ad\u6587\u6863\u662f\u5426\u76f8\u5173/\u4e0d\u76f8\u5173\uff0c\u8fd8\u8981\u5728\u8ba4\u4e3a\u6709\u7528\u7684\u6587\u6863\u4e2d\u9ad8\u4eae\u76f8\u5173\u6bb5\u843d\u3002\u4eba\u7c7b\u8bc4\u4f30\u8005\u5728\u521b\u5efa\u8be5\u96c6\u5408\u65f6\u4e5f\u6536\u5230\u4e86\u7c7b\u4f3c\u6307\u4ee4\uff0c\u8fd9\u4f7f\u7814\u7a76\u8005\u80fd\u591f\u5728\u6587\u6863\u7ea7\u522b\u548c\u6bb5\u843d\u7ea7\u522b\u8bc4\u4f30LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u6548\u679c\u6700\u4f73\u3002\u7814\u7a76\u4e0d\u4ec5\u8bc4\u4f30\u4e86\u6587\u6863\u7ea7\u522b\u7684\u76f8\u5173\u6027\u5224\u65ad\uff0c\u8fd8\u91cf\u5316\u4e86\u8fd9\u4e9b\"\u6cd5\u5b98\"\u662f\u5426\u51fa\u4e8e\u6b63\u786e\u7684\u539f\u56e0\u505a\u51fa\u6b63\u786e\u5224\u65ad\u3002", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u4fe1\u606f\u68c0\u7d22\u76f8\u5173\u6027\u8bc4\u4f30\u8005\uff0c\u4f46\u5728\u6bb5\u843d\u7ea7\u522b\u7684\u76f8\u5173\u6027\u8bc6\u522b\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u6700\u91cd\u8981\u7684\u662f\uff0cLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u5728\u4eba\u7c7b\u76d1\u7763\u4e0b\u8868\u73b0\u6700\u597d\uff0c\u8fd9\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.09159", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09159", "abs": "https://arxiv.org/abs/2601.09159", "authors": ["Zhibo Zhang", "Yang Xu", "Kai Ming Ting", "Cam-Tu Nguyen"], "title": "LLMs Meet Isolation Kernel: Lightweight, Learning-free Binary Embeddings for Fast Retrieval", "comment": null, "summary": "Large language models (LLMs) have recently enabled remarkable progress in text representation. However, their embeddings are typically high-dimensional, leading to substantial storage and retrieval overhead. Although recent approaches such as Matryoshka Representation Learning (MRL) and Contrastive Sparse Representation (CSR) alleviate these issues to some extent, they still suffer from retrieval accuracy degradation. This paper proposes \\emph{Isolation Kernel Embedding} or IKE, a learning-free method that transforms an LLM embedding into a binary embedding using Isolation Kernel (IK). IKE is an ensemble of diverse (random) partitions, enabling robust estimation of ideal kernel in the LLM embedding space, thus reducing retrieval accuracy loss as the ensemble grows. Lightweight and based on binary encoding, it offers low memory footprint and fast bitwise computation, lowering retrieval latency. Experiments on multiple text retrieval datasets demonstrate that IKE offers up to 16.7x faster retrieval and 16x lower memory usage than LLM embeddings, while maintaining comparable or better accuracy. Compared to CSR and other compression methods, IKE consistently achieves the best balance between retrieval efficiency and effectiveness.", "AI": {"tldr": "IKE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06LLM\u9ad8\u7ef4\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u5d4c\u5165\uff0c\u4f7f\u7528\u9694\u79bb\u6838\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6548\u7387\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "LLM\u5d4c\u5165\u901a\u5e38\u7ef4\u5ea6\u5f88\u9ad8\uff0c\u5bfc\u81f4\u5b58\u50a8\u548c\u68c0\u7d22\u5f00\u9500\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u5982MRL\u548cCSR\u867d\u7136\u6709\u6240\u7f13\u89e3\uff0c\u4f46\u4ecd\u5b58\u5728\u68c0\u7d22\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faIsolation Kernel Embedding (IKE)\uff0c\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u9694\u79bb\u6838\u5c06LLM\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u5d4c\u5165\u3002IKE\u901a\u8fc7\u591a\u6837\u5316\u7684\u968f\u673a\u5206\u533a\u96c6\u5408\u6765\u7a33\u5065\u4f30\u8ba1\u7406\u60f3\u6838\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\uff0cIKE\u76f8\u6bd4LLM\u5d4c\u5165\u5b9e\u73b0\u9ad8\u8fbe16.7\u500d\u7684\u68c0\u7d22\u52a0\u901f\u548c16\u500d\u7684\u5185\u5b58\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u76f8\u6bd4CSR\u548c\u5176\u4ed6\u538b\u7f29\u65b9\u6cd5\uff0cIKE\u5728\u68c0\u7d22\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "IKE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684LLM\u5d4c\u5165\u538b\u7f29\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u8fbe\u5230\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.09286", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09286", "abs": "https://arxiv.org/abs/2601.09286", "authors": ["Hanze Guo", "Jianxun Lian", "Xiao Zhou"], "title": "Why not Collaborative Filtering in Dual View? Bridging Sparse and Dense Models", "comment": "25 pages, 6 figures", "summary": "Collaborative Filtering (CF) remains the cornerstone of modern recommender systems, with dense embedding--based methods dominating current practice. However, these approaches suffer from a critical limitation: our theoretical analysis reveals a fundamental signal-to-noise ratio (SNR) ceiling when modeling unpopular items, where parameter-based dense models experience diminishing SNR under severe data sparsity. To overcome this bottleneck, we propose SaD (Sparse and Dense), a unified framework that integrates the semantic expressiveness of dense embeddings with the structural reliability of sparse interaction patterns. We theoretically show that aligning these dual views yields a strictly superior global SNR. Concretely, SaD introduces a lightweight bidirectional alignment mechanism: the dense view enriches the sparse view by injecting semantic correlations, while the sparse view regularizes the dense model through explicit structural signals. Extensive experiments demonstrate that, under this dual-view alignment, even a simple matrix factorization--style dense model can achieve state-of-the-art performance. Moreover, SaD is plug-and-play and can be seamlessly applied to a wide range of existing recommender models, highlighting the enduring power of collaborative filtering when leveraged from dual perspectives. Further evaluations on real-world benchmarks show that SaD consistently outperforms strong baselines, ranking first on the BarsMatch leaderboard. The code is publicly available at https://github.com/harris26-G/SaD.", "AI": {"tldr": "SaD\u63d0\u51fa\u7a00\u758f\u4e0e\u7a20\u5bc6\u53cc\u89c6\u56fe\u5bf9\u9f50\u6846\u67b6\uff0c\u89e3\u51b3\u4f20\u7edf\u534f\u540c\u8fc7\u6ee4\u5728\u51b7\u95e8\u7269\u54c1\u5efa\u6a21\u4e2d\u7684\u4fe1\u566a\u6bd4\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5411\u5bf9\u9f50\u673a\u5236\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7a20\u5bc6\u5d4c\u5165\u7684\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u5728\u5904\u7406\u51b7\u95e8\u7269\u54c1\u65f6\u5b58\u5728\u4fe1\u566a\u6bd4\u74f6\u9888\uff0c\u5728\u6570\u636e\u6781\u5ea6\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u53c2\u6570\u5316\u7a20\u5bc6\u6a21\u578b\u7684\u4fe1\u566a\u6bd4\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faSaD\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7a20\u5bc6\u5d4c\u5165\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u4e0e\u7a00\u758f\u4ea4\u4e92\u6a21\u5f0f\u7684\u7ed3\u6784\u53ef\u9760\u6027\u76f8\u7ed3\u5408\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u53cc\u5411\u5bf9\u9f50\u673a\u5236\uff1a\u7a20\u5bc6\u89c6\u56fe\u901a\u8fc7\u6ce8\u5165\u8bed\u4e49\u76f8\u5173\u6027\u6765\u4e30\u5bcc\u7a00\u758f\u89c6\u56fe\uff0c\u800c\u7a00\u758f\u89c6\u56fe\u901a\u8fc7\u663e\u5f0f\u7ed3\u6784\u4fe1\u53f7\u6765\u6b63\u5219\u5316\u7a20\u5bc6\u6a21\u578b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u53cc\u89c6\u56fe\u5bf9\u9f50\u80fd\u83b7\u5f97\u4e25\u683c\u66f4\u4f18\u7684\u5168\u5c40\u4fe1\u566a\u6bd4\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u53cc\u89c6\u56fe\u5bf9\u9f50\u4e0b\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u77e9\u9635\u5206\u89e3\u5f0f\u7a20\u5bc6\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002SaD\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728BarsMatch\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "SaD\u6846\u67b6\u8bc1\u660e\u4e86\u4ece\u53cc\u89c6\u89d2\uff08\u7a00\u758f\u4e0e\u7a20\u5bc6\uff09\u534f\u540c\u5229\u7528\u534f\u540c\u8fc7\u6ee4\u7684\u6301\u4e45\u5a01\u529b\uff0c\u8be5\u6846\u67b6\u5373\u63d2\u5373\u7528\uff0c\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u5e7f\u6cdb\u7684\u73b0\u6709\u63a8\u8350\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u51b7\u95e8\u7269\u54c1\u63a8\u8350\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.09306", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09306", "abs": "https://arxiv.org/abs/2601.09306", "authors": ["Xin Xia", "Hongzhi Yin", "Shane Culpepper"], "title": "On-Device Large Language Models for Sequential Recommendation", "comment": "WSDM'26", "summary": "On-device recommendation is critical for a number of real-world applications, especially in scenarios that have agreements on execution latency, user privacy, and robust functionality when internet connectivity is unstable or even impossible. While large language models (LLMs) can now provide exceptional capabilities that model user behavior for sequential recommendation tasks, their substantial memory footprint and computational overhead make the deployment on resource-constrained devices a high risk proposition. In this paper, we propose OD-LLM, the first task-adaptive compression framework explicitly designed to provide efficient and accurate on-device deployment of LLMs for sequential recommendation tasks. OD-LLM uniquely integrates two complementary compression strategies: a low-rank structural compression algorithm which uses Singular Value Decomposition (SVD) to significantly reduce parameter redundancy in the model, and a novel tokenization normalization technique that better complements the low-rank decomposition process being used. Additionally, to minimize any potential performance degradation when using higher compression ratios, a novel progressive alignment algorithm is used to iteratively refine the parameters required layerwise in the target model. Empirical evaluations conducted on sequential recommendation benchmarks show that OD-LLM exhibits no loss in effectiveness when compared to the original recommendation model, when the deployed model size is halved. These promising results demonstrate the efficacy and scalability of OD-LLM, making this novel solution a practical alternative for real-time, on-device solutions wishing to replace expensive, remotely executed LLMs.", "AI": {"tldr": "OD-LLM\uff1a\u9996\u4e2a\u9762\u5411\u987a\u5e8f\u63a8\u8350\u4efb\u52a1\u7684LLM\u8bbe\u5907\u7aef\u81ea\u9002\u5e94\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u7ed3\u6784\u538b\u7f29\u548ctokenization\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5728\u6a21\u578b\u5927\u5c0f\u51cf\u534a\u65f6\u4fdd\u6301\u63a8\u8350\u6548\u679c\u4e0d\u53d8\u3002", "motivation": "\u8bbe\u5907\u7aef\u63a8\u8350\u5bf9\u4e8e\u5b9e\u65f6\u6027\u3001\u7528\u6237\u9690\u79c1\u548c\u7f51\u7edc\u4e0d\u7a33\u5b9a\u573a\u666f\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u5de8\u5927\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\u4f7f\u5176\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "1\uff09\u4f4e\u79e9\u7ed3\u6784\u538b\u7f29\u7b97\u6cd5\uff08SVD\uff09\u51cf\u5c11\u53c2\u6570\u5197\u4f59\uff1b2\uff09tokenization\u5f52\u4e00\u5316\u6280\u672f\u589e\u5f3a\u4f4e\u79e9\u5206\u89e3\u6548\u679c\uff1b3\uff09\u6e10\u8fdb\u5bf9\u9f50\u7b97\u6cd5\u9010\u5c42\u4f18\u5316\u53c2\u6570\u4ee5\u51cf\u5c11\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728\u987a\u5e8f\u63a8\u8350\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOD-LLM\u5728\u6a21\u578b\u5927\u5c0f\u51cf\u534a\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u539f\u59cb\u63a8\u8350\u6a21\u578b\u6ca1\u6709\u6548\u679c\u635f\u5931\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "OD-LLM\u4e3a\u5b9e\u65f6\u8bbe\u5907\u7aef\u63a8\u8350\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u66ff\u4ee3\u6602\u8d35\u7684\u8fdc\u7a0b\u6267\u884cLLMs\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u8bbe\u5907\u7aef\u90e8\u7f72\u3002"}}
{"id": "2601.09366", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09366", "abs": "https://arxiv.org/abs/2601.09366", "authors": ["Jana Isabelle Friese", "Andreas Konstantin Kruff", "Philipp Schaer", "Norbert Fuhr", "Nicola Ferro"], "title": "LISP -- A Rich Interaction Dataset and Loggable Interactive Search Platform", "comment": null, "summary": "We present a reusable dataset and accompanying infrastructure for studying human search behavior in Interactive Information Retrieval (IIR). The dataset combines detailed interaction logs from 61 participants (122 sessions) with user characteristics, including perceptual speed, topic-specific interest, search expertise, and demographic information. To facilitate reproducibility and reuse, we provide a fully documented study setup, a web-based perceptual speed test, and a framework for conducting similar user studies. Our work allows researchers to investigate individual and contextual factors affecting search behavior, and to develop or validate user simulators that account for such variability. We illustrate the datasets potential through an illustrative analysis and release all resources as open-access, supporting reproducible research and resource sharing in the IIR community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u7814\u7a76\u4ea4\u4e92\u5f0f\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u4eba\u7c7b\u641c\u7d22\u884c\u4e3a\uff0c\u5305\u542b61\u540d\u53c2\u4e0e\u8005\u7684\u8be6\u7ec6\u4ea4\u4e92\u65e5\u5fd7\u548c\u7528\u6237\u7279\u5f81\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u4ea4\u4e92\u5f0f\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u5bf9\u4eba\u7c7b\u641c\u7d22\u884c\u4e3a\u7684\u7814\u7a76\uff0c\u9700\u8981\u53ef\u590d\u7528\u7684\u6570\u636e\u96c6\u548c\u57fa\u7840\u8bbe\u65bd\u6765\u652f\u6301\u53ef\u91cd\u590d\u6027\u7814\u7a76\u548c\u8d44\u6e90\u5206\u4eab\u3002", "method": "\u6536\u96c6\u4e8661\u540d\u53c2\u4e0e\u8005\uff08122\u4e2a\u4f1a\u8bdd\uff09\u7684\u8be6\u7ec6\u4ea4\u4e92\u65e5\u5fd7\uff0c\u7ed3\u5408\u7528\u6237\u7279\u5f81\u6570\u636e\uff08\u611f\u77e5\u901f\u5ea6\u3001\u4e3b\u9898\u5174\u8da3\u3001\u641c\u7d22\u4e13\u4e1a\u77e5\u8bc6\u3001\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff09\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u7814\u7a76\u8bbe\u7f6e\u6587\u6863\u3001\u57fa\u4e8e\u7f51\u7edc\u7684\u611f\u77e5\u901f\u5ea6\u6d4b\u8bd5\u6846\u67b6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e30\u5bcc\u7528\u6237\u7279\u5f81\u548c\u4ea4\u4e92\u6570\u636e\u7684\u53ef\u590d\u7528\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u5206\u6790\u5c55\u793a\u4e86\u6570\u636e\u96c6\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aIIR\u793e\u533a\u63d0\u4f9b\u4e86\u5f00\u653e\u8bbf\u95ee\u7684\u8d44\u6e90\uff0c\u652f\u6301\u7814\u7a76\u4e2a\u4f53\u548c\u60c5\u5883\u56e0\u7d20\u5bf9\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5f00\u53d1\u548c\u9a8c\u8bc1\u8003\u8651\u7528\u6237\u5dee\u5f02\u6027\u7684\u7528\u6237\u6a21\u62df\u5668\u3002"}}
{"id": "2601.09459", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.09459", "abs": "https://arxiv.org/abs/2601.09459", "authors": ["Pei-Chi Lo", "Thomas Y. Lu"], "title": "Dissecting Judicial Reasoning in U.S. Copyright Damage Awards", "comment": "Presented in SIGKDD'25 SciSoc LLM Workshop: Large Language Models for Scientific and Societal Advances", "summary": "Judicial reasoning in copyright damage awards poses a core challenge for computational legal analysis. Although federal courts follow the 1976 Copyright Act, their interpretations and factor weightings vary widely across jurisdictions. This inconsistency creates unpredictability for litigants and obscures the empirical basis of legal decisions. This research introduces a novel discourse-based Large Language Model (LLM) methodology that integrates Rhetorical Structure Theory (RST) with an agentic workflow to extract and quantify previously opaque reasoning patterns from judicial opinions. Our framework addresses a major gap in empirical legal scholarship by parsing opinions into hierarchical discourse structures and using a three-stage pipeline, i.e., Dataset Construction, Discourse Analysis, and Agentic Feature Extraction. This pipeline identifies reasoning components and extract feature labels with corresponding discourse subtrees. In analyzing copyright damage rulings, we show that discourse-augmented LLM analysis outperforms traditional methods while uncovering unquantified variations in factor weighting across circuits. These findings offer both methodological advances in computational legal analysis and practical insights into judicial reasoning, with implications for legal practitioners seeking predictive tools, scholars studying legal principle application, and policymakers confronting inconsistencies in copyright law.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bdd\u8bed\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u6790\u7248\u6743\u635f\u5bb3\u8d54\u507f\u5224\u51b3\u4e2d\u7684\u53f8\u6cd5\u63a8\u7406\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u5de1\u56de\u6cd5\u9662\u5728\u56e0\u7d20\u6743\u91cd\u5206\u914d\u4e0a\u7684\u5dee\u5f02\u3002", "motivation": "\u7248\u6743\u635f\u5bb3\u8d54\u507f\u5224\u51b3\u4e2d\u7684\u53f8\u6cd5\u63a8\u7406\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002\u867d\u7136\u8054\u90a6\u6cd5\u9662\u9075\u5faa1976\u5e74\u7248\u6743\u6cd5\uff0c\u4f46\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u5bf9\u6cd5\u5f8b\u7684\u89e3\u91ca\u548c\u56e0\u7d20\u6743\u91cd\u5206\u914d\u5b58\u5728\u5e7f\u6cdb\u5dee\u5f02\u3002\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u7ed9\u8bc9\u8bbc\u5f53\u4e8b\u4eba\u5e26\u6765\u4e86\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u63a9\u76d6\u4e86\u6cd5\u5f8b\u51b3\u7b56\u7684\u5b9e\u8bc1\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u8bdd\u8bed\u5206\u6790\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u4fee\u8f9e\u7ed3\u6784\u7406\u8bba\uff08RST\uff09\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a\u6570\u636e\u96c6\u6784\u5efa\u3001\u8bdd\u8bed\u5206\u6790\u548c\u667a\u80fd\u4f53\u7279\u5f81\u63d0\u53d6\uff0c\u5c06\u53f8\u6cd5\u610f\u89c1\u89e3\u6790\u4e3a\u5c42\u6b21\u5316\u8bdd\u8bed\u7ed3\u6784\uff0c\u8bc6\u522b\u63a8\u7406\u7ec4\u4ef6\u5e76\u63d0\u53d6\u7279\u5f81\u6807\u7b7e\u53ca\u5176\u5bf9\u5e94\u7684\u8bdd\u8bed\u5b50\u6811\u3002", "result": "\u8bdd\u8bed\u589e\u5f3a\u7684LLM\u5206\u6790\u5728\u5206\u6790\u7248\u6743\u635f\u5bb3\u8d54\u507f\u88c1\u51b3\u65f6\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u5de1\u56de\u6cd5\u9662\u5728\u56e0\u7d20\u6743\u91cd\u5206\u914d\u4e0a\u5148\u524d\u672a\u91cf\u5316\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u6cd5\u5f8b\u5206\u6790\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u8fdb\u6b65\uff0c\u5e76\u4e3a\u53f8\u6cd5\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6d1e\u5bdf\u3002\u5bf9\u5bfb\u6c42\u9884\u6d4b\u5de5\u5177\u7684\u6cd5\u5f8b\u4ece\u4e1a\u8005\u3001\u7814\u7a76\u6cd5\u5f8b\u539f\u5219\u5e94\u7528\u7684\u5b66\u8005\u4ee5\u53ca\u9762\u5bf9\u7248\u6743\u6cd5\u4e0d\u4e00\u81f4\u6027\u7684\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.09478", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09478", "abs": "https://arxiv.org/abs/2601.09478", "authors": ["Renqiang Luo", "Dong Zhang", "Yupeng Gao", "Wen Shi", "Mingliang Hou", "Jiaying Liu", "Zhe Wang", "Shuo Yu"], "title": "Bridging Semantic Understanding and Popularity Bias with LLMs", "comment": "10 pages, 4 figs, WWW 2026 accepted", "summary": "Semantic understanding of popularity bias is a crucial yet underexplored challenge in recommender systems, where popular items are often favored at the expense of niche content. Most existing debiasing methods treat the semantic understanding of popularity bias as a matter of diversity enhancement or long-tail coverage, neglecting the deeper semantic layer that embodies the causal origins of the bias itself. Consequently, such shallow interpretations limit both their debiasing effectiveness and recommendation accuracy. In this paper, we propose FairLRM, a novel framework that bridges the gap in the semantic understanding of popularity bias with Recommendation via Large Language Model (RecLLM). FairLRM decomposes popularity bias into item-side and user-side components, using structured instruction-based prompts to enhance the model's comprehension of both global item distributions and individual user preferences. Unlike traditional methods that rely on surface-level features such as \"diversity\" or \"debiasing\", FairLRM improves the model's ability to semantically interpret and address the underlying bias. Through empirical evaluation, we show that FairLRM significantly enhances both fairness and recommendation accuracy, providing a more semantically aware and trustworthy approach to enhance the semantic understanding of popularity bias. The implementation is available at https://github.com/LuoRenqiang/FairLRM.", "AI": {"tldr": "FairLRM\u662f\u4e00\u4e2a\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u63a8\u8350\u7cfb\u7edf\u5bf9\u6d41\u884c\u5ea6\u504f\u5dee\u8bed\u4e49\u7406\u89e3\u7684\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u5206\u89e3\u4e3a\u7269\u54c1\u4fa7\u548c\u7528\u6237\u4fa7\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u548c\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u5927\u591a\u5c06\u6d41\u884c\u5ea6\u504f\u5dee\u7684\u8bed\u4e49\u7406\u89e3\u7b80\u5316\u4e3a\u591a\u6837\u6027\u589e\u5f3a\u6216\u957f\u5c3e\u8986\u76d6\uff0c\u5ffd\u89c6\u4e86\u504f\u5dee\u672c\u8eab\u7684\u56e0\u679c\u8d77\u6e90\uff0c\u8fd9\u79cd\u6d45\u5c42\u7406\u89e3\u9650\u5236\u4e86\u53bb\u504f\u6548\u679c\u548c\u63a8\u8350\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faFairLRM\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u5bf9\u6d41\u884c\u5ea6\u504f\u5dee\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5c06\u504f\u5dee\u5206\u89e3\u4e3a\u7269\u54c1\u4fa7\u548c\u7528\u6237\u4fa7\u7ec4\u4ef6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u6307\u4ee4\u63d0\u793a\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5168\u5c40\u7269\u54c1\u5206\u5e03\u548c\u4e2a\u4f53\u7528\u6237\u504f\u597d\u7684\u7406\u89e3\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aFairLRM\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u6027\u548c\u63a8\u8350\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u8bed\u4e49\u611f\u77e5\u548c\u53ef\u4fe1\u8d56\u7684\u6d41\u884c\u5ea6\u504f\u5dee\u7406\u89e3\u65b9\u6cd5\u3002", "conclusion": "FairLRM\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u5bf9\u6d41\u884c\u5ea6\u504f\u5dee\u7684\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u516c\u5e73\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u8868\u9762\u7279\u5f81\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.09496", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09496", "abs": "https://arxiv.org/abs/2601.09496", "authors": ["Jujia Zhao", "Zihan Wang", "Shuaiqun Pan", "Suzan Verberne", "Zhaochun Ren"], "title": "Unifying Search and Recommendation in LLMs via Gradient Multi-Subspace Tuning", "comment": null, "summary": "Search and recommendation (S&R) are core to online platforms, addressing explicit intent through queries and modeling implicit intent from behaviors, respectively. Their complementary roles motivate a unified modeling paradigm. Early studies to unify S&R adopt shared encoders with task-specific heads, while recent efforts reframe item ranking in both S&R as conditional generation. The latter holds particular promise, enabling end-to-end optimization and leveraging the semantic understanding of LLMs. However, existing methods rely on full fine-tuning, which is computationally expensive and limits scalability. Parameter-efficient fine-tuning (PEFT) offers a more practical alternative but faces two critical challenges in unifying S&R: (1) gradient conflicts across tasks due to divergent optimization objectives, and (2) shifts in user intent understanding caused by overfitting to fine-tuning data, which distort general-domain knowledge and weaken LLM reasoning. To address the above issues, we propose Gradient Multi-Subspace Tuning (GEMS), a novel framework that unifies S&R with LLMs while alleviating gradient conflicts and preserving general-domain knowledge. GEMS introduces (1) \\textbf{Multi-Subspace Decomposition}, which disentangles shared and task-specific optimization signals into complementary low-rank subspaces, thereby reducing destructive gradient interference, and (2) \\textbf{Null-Space Projection}, which constrains parameter updates to a subspace orthogonal to the general-domain knowledge space, mitigating shifts in user intent understanding. Extensive experiments on benchmark datasets show that GEMS consistently outperforms the state-of-the-art baselines across both search and recommendation tasks, achieving superior effectiveness.", "AI": {"tldr": "GEMS\uff1a\u4e00\u4e2a\u7edf\u4e00\u641c\u7d22\u4e0e\u63a8\u8350\u7684LLM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5b50\u7a7a\u95f4\u5206\u89e3\u548c\u96f6\u7a7a\u95f4\u6295\u5f71\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\u548c\u77e5\u8bc6\u504f\u79fb\u95ee\u9898", "motivation": "\u641c\u7d22\u548c\u63a8\u8350\u5728\u5728\u7ebf\u5e73\u53f0\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\uff0c\u7edf\u4e00\u5efa\u6a21\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u5171\u4eab\u7f16\u7801\u5668\uff0c\u8981\u4e48\u4f9d\u8d56LLM\u5168\u5fae\u8c03\uff0c\u4f46\u5168\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u5dee\u3002\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u66f4\u5b9e\u7528\uff0c\u4f46\u5728\u7edf\u4e00S&R\u65f6\u9762\u4e34\u68af\u5ea6\u51b2\u7a81\u548c\u7528\u6237\u610f\u56fe\u7406\u89e3\u504f\u79fb\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faGEMS\u6846\u67b6\uff1a1) \u591a\u5b50\u7a7a\u95f4\u5206\u89e3\uff1a\u5c06\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u4f18\u5316\u4fe1\u53f7\u89e3\u8026\u5230\u4e92\u8865\u7684\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u51cf\u5c11\u7834\u574f\u6027\u68af\u5ea6\u5e72\u6270\uff1b2) \u96f6\u7a7a\u95f4\u6295\u5f71\uff1a\u5c06\u53c2\u6570\u66f4\u65b0\u7ea6\u675f\u5230\u4e0e\u901a\u7528\u77e5\u8bc6\u7a7a\u95f4\u6b63\u4ea4\u7684\u5b50\u7a7a\u95f4\uff0c\u51cf\u8f7b\u7528\u6237\u610f\u56fe\u7406\u89e3\u7684\u504f\u79fb\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGEMS\u5728\u641c\u7d22\u548c\u63a8\u8350\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6709\u6548\u6027\u3002", "conclusion": "GEMS\u6210\u529f\u89e3\u51b3\u4e86\u7edf\u4e00\u641c\u7d22\u4e0e\u63a8\u8350\u65f6PEFT\u9762\u4e34\u7684\u68af\u5ea6\u51b2\u7a81\u548c\u77e5\u8bc6\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5b50\u7a7a\u95f4\u5206\u89e3\u548c\u96f6\u7a7a\u95f4\u6295\u5f71\u6280\u672f\uff0c\u5728\u4fdd\u6301LLM\u901a\u7528\u77e5\u8bc6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4efb\u52a1\u95f4\u7684\u6709\u6548\u534f\u540c\u3002"}}
{"id": "2601.09523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09523", "abs": "https://arxiv.org/abs/2601.09523", "authors": ["Abdelrahman Abdallah", "Mohammed Ali", "Muhammad Abdul-Mageed", "Adam Jatowt"], "title": "TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval", "comment": null, "summary": "Existing temporal QA benchmarks focus on simple fact-seeking queries from news corpora, while reasoning-intensive retrieval benchmarks lack temporal grounding. However, real-world information needs often require reasoning about temporal evolution and synthesizing evidence across time periods. We introduce TEMPO, the first benchmark combining temporal reasoning with reasoning-intensive retrieval across 13 domains. TEMPO features: (1) 1,730 complex queries requiring deep temporal reasoning such as tracking changes, identifying trends, or comparing cross-period evidence; (2) step-wise retrieval planning with 3,976 decomposed steps and gold documents mapped to each step for multi-hop evaluation; and (3) novel temporal metrics including Temporal Coverage@k and Temporal Precision@k measuring whether results span required time periods. Evaluation of 12 retrieval systems reveals substantial challenges: the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4\\% Temporal Coverage@10, demonstrating difficulty in retrieving temporally complete evidence. We believe TEMPO provides a challenging benchmark for improving temporal reasoning in retrieval and RAG systems. Our code and data are available at https://github.com/tempo-bench/Tempo. See also our official website: https://tempo-bench.github.io/.", "AI": {"tldr": "TEMPO\u662f\u9996\u4e2a\u7ed3\u5408\u65f6\u5e8f\u63a8\u7406\u4e0e\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,730\u4e2a\u590d\u6742\u67e5\u8be2\u30013,976\u4e2a\u5206\u89e3\u6b65\u9aa4\u548c\u65b0\u7684\u65f6\u5e8f\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u5728\u83b7\u53d6\u5b8c\u6574\u65f6\u5e8f\u8bc1\u636e\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8fQA\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u65b0\u95fb\u8bed\u6599\u4e2d\u7684\u7b80\u5355\u4e8b\u5b9e\u67e5\u8be2\uff0c\u800c\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u57fa\u51c6\u7f3a\u4e4f\u65f6\u5e8f\u57fa\u7840\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u4fe1\u606f\u9700\u6c42\u901a\u5e38\u9700\u8981\u7406\u89e3\u65f6\u95f4\u6f14\u53d8\u548c\u8de8\u65f6\u671f\u8bc1\u636e\u7efc\u5408\u3002", "method": "\u6784\u5efa\u5305\u542b13\u4e2a\u9886\u57df\u7684TEMPO\u57fa\u51c6\uff0c\u5177\u6709\uff1a(1) 1,730\u4e2a\u9700\u8981\u6df1\u5ea6\u65f6\u5e8f\u63a8\u7406\u7684\u590d\u6742\u67e5\u8be2\uff1b(2) \u9010\u6b65\u68c0\u7d22\u89c4\u5212\uff0c\u5305\u542b3,976\u4e2a\u5206\u89e3\u6b65\u9aa4\u548c\u6bcf\u4e2a\u6b65\u9aa4\u7684\u9ec4\u91d1\u6587\u6863\u6620\u5c04\uff1b(3) \u65b0\u7684\u65f6\u5e8f\u8bc4\u4f30\u6307\u6807Temporal Coverage@k\u548cTemporal Precision@k\u3002", "result": "\u8bc4\u4f3012\u4e2a\u68c0\u7d22\u7cfb\u7edf\u663e\u793a\u663e\u8457\u6311\u6218\uff1a\u6700\u4f73\u6a21\u578b(DiVeR)\u4ec5\u8fbe\u523032.0 NDCG@10\u548c71.4% Temporal Coverage@10\uff0c\u8868\u660e\u5728\u68c0\u7d22\u5b8c\u6574\u65f6\u5e8f\u8bc1\u636e\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "TEMPO\u4e3a\u6539\u8fdb\u68c0\u7d22\u548cRAG\u7cfb\u7edf\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u83b7\u53d6\u8de8\u65f6\u95f4\u6bb5\u7684\u5b8c\u6574\u8bc1\u636e\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2601.09530", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09530", "abs": "https://arxiv.org/abs/2601.09530", "authors": ["Bingde Hu", "Enhao Pan", "Wanjing Zhou", "Yang Gao", "Zunlei Feng", "Hao Zhong"], "title": "SpatCode: Rotary-based Unified Encoding Framework for Efficient Spatiotemporal Vector Retrieval", "comment": null, "summary": "Spatiotemporal vector retrieval has emerged as a critical paradigm in modern information retrieval, enabling efficient access to massive, heterogeneous data that evolve over both time and space. However, existing spatiotemporal retrieval methods are often extensions of conventional vector search systems that rely on external filters or specialized indices to incorporate temporal and spatial constraints, leading to inefficiency, architectural complexity, and limited flexibility in handling heterogeneous modalities. To overcome these challenges, we present a unified spatiotemporal vector retrieval framework that integrates temporal, spatial, and semantic cues within a coherent similarity space while maintaining scalability and adaptability to continuous data streams. Specifically, we propose (1) a Rotary-based Unified Encoding Method that embeds time and location into rotational position vectors for consistent spatiotemporal representation; (2) a Circular Incremental Update Mechanism that supports efficient sliding-window updates without global re-encoding or index reconstruction; and (3) a Weighted Interest-based Retrieval Algorithm that adaptively balances modality weights for context-aware and personalized retrieval. Extensive experiments across multiple real-world datasets demonstrate that our framework substantially outperforms state-of-the-art baselines in both retrieval accuracy and efficiency, while maintaining robustness under dynamic data evolution. These results highlight the effectiveness and practicality of the proposed approach for scalable spatiotemporal information retrieval in intelligent systems.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u65f6\u7a7a\u5411\u91cf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u65cb\u8f6c\u7f16\u7801\u3001\u589e\u91cf\u66f4\u65b0\u548c\u52a0\u6743\u68c0\u7d22\u7b97\u6cd5\uff0c\u5728\u5355\u4e00\u76f8\u4f3c\u7a7a\u95f4\u96c6\u6210\u65f6\u7a7a\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u68c0\u7d22\u65b9\u6cd5\u591a\u4e3a\u4f20\u7edf\u5411\u91cf\u641c\u7d22\u7684\u6269\u5c55\uff0c\u4f9d\u8d56\u5916\u90e8\u8fc7\u6ee4\u5668\u6216\u4e13\u7528\u7d22\u5f15\u5904\u7406\u65f6\u7a7a\u7ea6\u675f\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u67b6\u6784\u590d\u6742\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u5f02\u6784\u6a21\u6001\u6570\u636e\u3002", "method": "1) \u57fa\u4e8e\u65cb\u8f6c\u7684\u7edf\u4e00\u7f16\u7801\u65b9\u6cd5\uff1a\u5c06\u65f6\u95f4\u548c\u4f4d\u7f6e\u5d4c\u5165\u65cb\u8f6c\u4f4d\u7f6e\u5411\u91cf\uff1b2) \u5faa\u73af\u589e\u91cf\u66f4\u65b0\u673a\u5236\uff1a\u652f\u6301\u6ed1\u52a8\u7a97\u53e3\u66f4\u65b0\u800c\u65e0\u9700\u5168\u5c40\u91cd\u7f16\u7801\uff1b3) \u52a0\u6743\u5174\u8da3\u68c0\u7d22\u7b97\u6cd5\uff1a\u81ea\u9002\u5e94\u5e73\u8861\u6a21\u6001\u6743\u91cd\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u52a8\u6001\u6570\u636e\u6f14\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u4e3a\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u65f6\u7a7a\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.09543", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09543", "abs": "https://arxiv.org/abs/2601.09543", "authors": ["Jason Carpenter", "Faaiq Bilal", "Eman Ramadan", "Zhi-Li Zhang"], "title": "Examining DOM Coordinate Effectiveness For Page Segmentation", "comment": null, "summary": "Web pages form a cornerstone of available data for daily human consumption and with the rise of LLM-based search and learning systems a treasure trove of valuable data. The scale of this data and its unstructured format still continue to grow requiring ever more robust automated extraction and retrieval mechanisms. Existing work, leveraging the web pages Document Object Model (DOM), often derives clustering vectors from coordinates informed by the DOM such as visual placement or tree structure. The construction and component value of these vectors often go unexamined. Our work proposes and examines DOM coordinates in a detail to understand their impact on web page segmentation. Our work finds that there is no one-size-fits-all vector, and that visual coordinates under-perform compared to DOM coordinates by about 20-30% on average. This challenges the necessity of including visual coordinates in clustering vectors. Further, our work finds that simple vectors, comprised of single coordinates, fare better than complex vectors constituting 68.2% of the top performing vectors of the pages examined. Finally, we find that if a vector, clustering algorithm, and page are properly matched, one can achieve overall high segmentation accuracy at 74%. This constitutes a 20% improvement over a naive application of vectors. Conclusively, our results challenge the current orthodoxy for segmentation vector creation, opens up the possibility to optimize page segmentation via clustering on DOM coordinates, and highlights the importance of finding mechanisms to match the best approach for web page segmentation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7f51\u9875DOM\u5750\u6807\u5bf9\u9875\u9762\u5206\u5272\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u89c6\u89c9\u5750\u6807\u8868\u73b0\u4e0d\u5982DOM\u5750\u6807\uff0c\u7b80\u5355\u5411\u91cf\u4f18\u4e8e\u590d\u6742\u5411\u91cf\uff0c\u6700\u4f73\u5339\u914d\u53ef\u5b9e\u73b074%\u7684\u5206\u5272\u51c6\u786e\u7387\u3002", "motivation": "\u7f51\u9875\u6570\u636e\u89c4\u6a21\u5e9e\u5927\u4e14\u975e\u7ed3\u6784\u5316\uff0c\u73b0\u6709\u57fa\u4e8eDOM\u7684\u65b9\u6cd5\u5e38\u4f7f\u7528\u89c6\u89c9\u4f4d\u7f6e\u6216\u6811\u7ed3\u6784\u7b49\u5750\u6807\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u5411\u91cf\u7684\u6784\u5efa\u548c\u7ec4\u4ef6\u4ef7\u503c\u7f3a\u4e4f\u6df1\u5165\u5206\u6790\u3002\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3DOM\u5750\u6807\u5bf9\u7f51\u9875\u5206\u5272\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5e76\u8be6\u7ec6\u68c0\u9a8cDOM\u5750\u6807\uff0c\u6bd4\u8f83\u89c6\u89c9\u5750\u6807\u4e0eDOM\u5750\u6807\u7684\u6027\u80fd\uff0c\u5206\u6790\u7b80\u5355\u5411\u91cf\u4e0e\u590d\u6742\u5411\u91cf\u7684\u6548\u679c\uff0c\u63a2\u7d22\u5411\u91cf\u3001\u805a\u7c7b\u7b97\u6cd5\u548c\u9875\u9762\u7684\u6700\u4f73\u5339\u914d\u3002", "result": "\u89c6\u89c9\u5750\u6807\u5e73\u5747\u8868\u73b0\u6bd4DOM\u5750\u6807\u5dee20-30%\uff1b\u7b80\u5355\u5411\u91cf\uff08\u5355\u5750\u6807\uff09\u572868.2%\u7684\u9875\u9762\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u5411\u91cf\uff1b\u901a\u8fc7\u6700\u4f73\u5339\u914d\u53ef\u5b9e\u73b074%\u7684\u6574\u4f53\u5206\u5272\u51c6\u786e\u7387\uff0c\u6bd4\u6734\u7d20\u5e94\u7528\u5411\u91cf\u65b9\u6cd5\u63d0\u534720%\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5f53\u524d\u5206\u5272\u5411\u91cf\u6784\u5efa\u7684\u6b63\u7edf\u89c2\u5ff5\uff0c\u8868\u660e\u65e0\u9700\u5305\u542b\u89c6\u89c9\u5750\u6807\uff0c\u7b80\u5355DOM\u5750\u6807\u5411\u91cf\u6548\u679c\u66f4\u597d\uff0c\u5f3a\u8c03\u9700\u8981\u5bfb\u627e\u673a\u5236\u6765\u5339\u914d\u6700\u4f73\u7f51\u9875\u5206\u5272\u65b9\u6cd5\u3002"}}
{"id": "2601.09562", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.09562", "abs": "https://arxiv.org/abs/2601.09562", "authors": ["Abdelrahman Abdallah", "Mohamed Darwish Mounis", "Mahmoud Abdalla", "Mahmoud SalahEldin Kasem", "Mostafa Farouk Senussi", "Mohamed Mahmoud", "Mohammed Ali", "Adam Jatowt", "Hyun-Soo Kang"], "title": "MM-BRIGHT: A Multi-Task Multimodal Benchmark for Reasoning-Intensive Retrieval", "comment": null, "summary": "Existing retrieval benchmarks primarily consist of text-based queries where keyword or semantic matching is usually sufficient. Many real-world queries contain multimodal elements, particularly, images such as diagrams, charts, and screenshots that require intensive reasoning to identify relevant documents. To address this gap, we introduce MM-BRIGHT, the first multimodal benchmark for reasoning-intensive retrieval. Our dataset consists of 2,803 real-world queries spanning 29 diverse technical domains, with four tasks of increasing complexity: text-to-text, multimodal-to-text, multimodal-to-image, and multimodal-to-multimodal retrieval. Extensive evaluation reveals that state-of-the-art models struggle across all tasks: BM25 achieves only 8.5 nDCG@10 on text-only retrieval, while the best multimodal model Nomic-Vision reaches just 27.6 nDCG@10 on multimodal-to-text retrieval actually underperforming the best text-only model (DiVeR: 32.2). These results highlight substantial headroom and position MM-BRIGHT as a testbed for next-generation retrieval models that better integrate visual reasoning. Our code and data are available at https://github.com/mm-bright/MM-BRIGHT. See also our official website: https://mm-bright.github.io/.", "AI": {"tldr": "MM-BRIGHT\u662f\u9996\u4e2a\u9488\u5bf9\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u7684\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,803\u4e2a\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\uff0c\u6db5\u76d629\u4e2a\u6280\u672f\u9886\u57df\u548c\u56db\u79cd\u590d\u6742\u5ea6\u9012\u589e\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u57fa\u51c6\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\u67e5\u8be2\uff0c\u4f9d\u8d56\u5173\u952e\u8bcd\u6216\u8bed\u4e49\u5339\u914d\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u67e5\u8be2\u5e38\u5305\u542b\u56fe\u8868\u3001\u622a\u56fe\u7b49\u591a\u6a21\u6001\u5143\u7d20\uff0c\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u624d\u80fd\u627e\u5230\u76f8\u5173\u6587\u6863\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u6b64\u7c7b\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b2,803\u4e2a\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6db5\u76d629\u4e2a\u6280\u672f\u9886\u57df\uff0c\u8bbe\u8ba1\u56db\u79cd\u590d\u6742\u5ea6\u9012\u589e\u7684\u4efb\u52a1\uff1a\u6587\u672c\u5230\u6587\u672c\u3001\u591a\u6a21\u6001\u5230\u6587\u672c\u3001\u591a\u6a21\u6001\u5230\u56fe\u50cf\u3001\u591a\u6a21\u6001\u5230\u591a\u6a21\u6001\u68c0\u7d22\u3002", "result": "\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4e0d\u4f73\uff1aBM25\u5728\u7eaf\u6587\u672c\u68c0\u7d22\u4e0a\u4ec5\u83b7\u5f978.5 nDCG@10\uff0c\u6700\u4f73\u591a\u6a21\u6001\u6a21\u578bNomic-Vision\u5728\u591a\u6a21\u6001\u5230\u6587\u672c\u68c0\u7d22\u4e0a\u4ec5\u8fbe\u523027.6 nDCG@10\uff0c\u751a\u81f3\u4f4e\u4e8e\u6700\u4f73\u7eaf\u6587\u672c\u6a21\u578bDiVeR\u768432.2\u3002", "conclusion": "MM-BRIGHT\u63ed\u793a\u4e86\u5f53\u524d\u68c0\u7d22\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u96c6\u6210\u65b9\u9762\u7684\u91cd\u5927\u4e0d\u8db3\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u66f4\u597d\u5730\u6574\u5408\u89c6\u89c9\u63a8\u7406\u7684\u68c0\u7d22\u6a21\u578b\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6539\u8fdb\u7a7a\u95f4\u3002"}}

<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Emotion-Driven Personalized Recommendation for AI-Generated Content Using Multi-Modal Sentiment and Intent Analysis](https://arxiv.org/abs/2512.10963)
*Zheqi Hu,Xuanjing Chen,Jinlin Hu*

Main category: cs.IR

TL;DR: 提出基于多模态情感意图识别模型（MMEI）的云原生个性化AIGC推荐框架，通过融合视觉、听觉和文本模态的情感分析提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统主要依赖用户行为数据（点击、观看、评分），忽视了用户在内容交互过程中的实时情感和意图状态。随着AIGC在各领域的快速增长，对情感感知推荐系统的需求日益重要。

Method: 提出基于BERT的跨模态Transformer与注意力融合的多模态情感意图识别模型（MMEI）。系统通过预训练编码器（ViT、Wav2Vec2、BERT）联合处理视觉（面部表情）、听觉（语音语调）和文本（评论或话语）模态，然后通过注意力融合模块学习情感意图表示，最后通过上下文匹配层驱动个性化内容推荐。

Result: 在基准情感数据集（AIGC-INT、MELD、CMU-MOSEI）和AIGC交互数据集上的实验表明，MMEI模型相比最佳融合Transformer基线在F1分数上提升4.3%，交叉熵损失降低12.3%。用户级在线评估显示，情感驱动推荐使参与时间增加15.2%，满意度评分提升11.8%。

Conclusion: 这项工作凸显了跨模态情感智能在下一代AIGC生态系统中的潜力，能够实现自适应、共情和上下文感知的推荐体验，有效将AI生成内容与用户情感和意图状态对齐。

Abstract: With the rapid growth of AI-generated content (AIGC) across domains such as music, video, and literature, the demand for emotionally aware recommendation systems has become increasingly important. Traditional recommender systems primarily rely on user behavioral data such as clicks, views, or ratings, while neglecting users' real-time emotional and intentional states during content interaction. To address this limitation, this study proposes a Multi-Modal Emotion and Intent Recognition Model (MMEI) based on a BERT-based Cross-Modal Transformer with Attention-Based Fusion, integrated into a cloud-native personalized AIGC recommendation framework. The proposed system jointly processes visual (facial expression), auditory (speech tone), and textual (comments or utterances) modalities through pretrained encoders ViT, Wav2Vec2, and BERT, followed by an attention-based fusion module to learn emotion-intent representations. These embeddings are then used to drive personalized content recommendations through a contextual matching layer. Experiments conducted on benchmark emotion datasets (AIGC-INT, MELD, and CMU-MOSEI) and an AIGC interaction dataset demonstrate that the proposed MMEI model achieves a 4.3% improvement in F1-score and a 12.3% reduction in cross-entropy loss compared to the best fusion-based transformer baseline. Furthermore, user-level online evaluations reveal that emotion-driven recommendations increase engagement time by 15.2% and enhance satisfaction scores by 11.8%, confirming the model's effectiveness in aligning AI-generated content with users' affective and intentional states. This work highlights the potential of cross-modal emotional intelligence for next-generation AIGC ecosystems, enabling adaptive, empathetic, and context-aware recommendation experiences.

</details>


### [2] [FAIR: Focused Attention Is All You Need for Generative Recommendation](https://arxiv.org/abs/2512.11254)
*Longtao Xiao,Haolin Zhang,Guohao Cai,Jieming Zhu,Yifan Wang,Heng Chang,Zhenhua Dong,Xiu Li,Ruixuan Li*

Main category: cs.IR

TL;DR: FAIR是一个基于Transformer的生成式推荐框架，通过聚焦注意力机制解决多码表示导致的序列过长和噪声问题，提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的生成式推荐需要将物品离散化为多个代码表示（通常4个或更多），这显著增加了原始物品序列的长度。序列扩展使得Transformer模型在处理包含固有噪声的用户行为序列时面临挑战，因为它们倾向于过度关注不相关或噪声上下文。

Method: 提出FAIR框架，包含三个核心组件：1）聚焦注意力机制，集成到标准Transformer中，学习两组独立的Q和K注意力权重，计算它们的差异作为最终注意力分数，以消除注意力噪声并聚焦相关上下文；2）噪声鲁棒性目标，鼓励模型在随机扰动下保持稳定的注意力模式，防止因噪声而向不相关上下文偏移；3）互信息最大化目标，引导模型识别对下一物品预测最信息丰富的上下文。

Result: 在四个公共基准测试上验证了FAIR的有效性，证明了其相对于现有方法的优越性能。

Conclusion: FAIR是首个采用聚焦注意力的生成式推荐框架，通过创新的注意力机制和训练目标，有效解决了多码表示导致的序列噪声问题，提升了用户行为建模的质量。

Abstract: Recently, transformer-based generative recommendation has garnered significant attention for user behavior modeling. However, it often requires discretizing items into multi-code representations (e.g., typically four code tokens or more), which sharply increases the length of the original item sequence. This expansion poses challenges to transformer-based models for modeling user behavior sequences with inherent noises, since they tend to overallocate attention to irrelevant or noisy context. To mitigate this issue, we propose FAIR, the first generative recommendation framework with focused attention, which enhances attention scores to relevant context while suppressing those to irrelevant ones. Specifically, we propose (1) a focused attention mechanism integrated into the standard Transformer, which learns two separate sets of Q and K attention weights and computes their difference as the final attention scores to eliminate attention noise while focusing on relevant contexts; (2) a noise-robustness objective, which encourages the model to maintain stable attention patterns under stochastic perturbations, preventing undesirable shifts toward irrelevant context due to noise; and (3) a mutual information maximization objective, which guides the model to identify contexts that are most informative for next-item prediction. We validate the effectiveness of FAIR on four public benchmarks, demonstrating its superior performance compared to existing methods.

</details>

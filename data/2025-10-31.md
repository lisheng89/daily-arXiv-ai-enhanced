<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: ORBIT是一个统一的推荐系统基准测试平台，包含公开数据集和隐藏测试集ClueWeb-Reco，旨在解决现有数据集无法捕捉真实用户行为和评估设置不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统研究受到数据集无法反映真实用户行为和评估设置不一致的阻碍，导致结论模糊。

Method: ORBIT提供标准化的评估框架，包括可复现的数据分割和透明的公开排行榜设置，并引入基于8700万高质量网页浏览序列的新网页推荐任务ClueWeb-Reco。

Result: 基准测试结果显示推荐系统在公开数据集上普遍改进，但个体性能差异大；隐藏测试揭示了现有方法在大规模网页推荐中的局限性，并显示LLM集成的改进潜力。

Conclusion: ORBIT为推荐系统研究提供了统一的基准测试平台，通过公开数据集和隐藏测试促进可复现研究，并展示了LLM在推荐系统中的潜力。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [2] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: OneTrans是一个统一的Transformer骨干网络，同时处理用户行为序列建模和特征交互，通过统一tokenizer和参数共享机制实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统中，特征交互模块和用户行为序列模块通常是分开开发的，这阻碍了双向信息交换和统一优化扩展。

Method: 使用统一tokenizer将顺序和非顺序属性转换为单一token序列，通过因果注意力和跨请求KV缓存实现预计算和缓存，显著降低计算成本。

Result: 在工业规模数据集上，OneTrans随着参数增加能高效扩展，持续优于强基线，在线A/B测试中带来5.68%的每用户GMV提升。

Conclusion: OneTrans证明了统一架构在推荐系统中的有效性，能够同时处理序列建模和特征交互，实现更好的性能和效率。

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [3] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 提出ReaKase-8B框架，通过整合法律事实、法律问题、法律关系三元组和法律推理来增强法律案例检索性能


<details>
  <summary>Details</summary>
Motivation: 现有法律案例检索方法主要依赖文本编码，但忽略了法律实体间的关系和推理过程，这些信息能反映案例的独特特征，提高检索准确性

Method: 设计基于情境的法律案例表示学习范式，使用微调的大语言模型，整合法律事实、法律问题、法律关系三元组和法律推理

Result: 在COLIEE 2022和2023基准数据集上的实验表明，知识推理增强的嵌入显著优于基线模型

Conclusion: 将法律推理整合到法律案例检索系统中具有巨大潜力

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [4] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE是一个基于扩散的生成模型，整合多种光谱模态实现有机化合物的自动结构解析，为自驱动实验室提供关键能力。


<details>
  <summary>Details</summary>
Motivation: 实现自动结构解析对于自驱动实验室至关重要，它能够闭合实验反馈回路，为机器学习模型提供可靠的结构信息以进行实时决策和优化。

Method: 开发了DiSE，一个端到端的基于扩散的生成模型，整合了MS、13C和1H化学位移、HSQC和COSY等多种光谱模态。

Result: DiSE通过数据驱动方法学习光谱间的内在关联，实现了卓越的准确性、在化学多样性数据集上的强泛化能力，以及对实验数据的鲁棒性（尽管在计算光谱上训练）。

Conclusion: DiSE代表了向完全自动结构解析的重要进展，在天然产物研究、药物发现和自驱动实验室中具有广泛潜力。

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [5] [Barlow Twins for Sequential Recommendation](https://arxiv.org/abs/2510.26407)
*Ivan Razvorotnev,Marina Munkhoeva,Evgeny Frolov*

Main category: cs.IR

TL;DR: BT-SR是一个新颖的非对比自监督学习框架，将Barlow Twins冗余减少原则集成到基于Transformer的序列推荐模型中，无需负采样或人工扰动，显著提高了推荐准确性和长尾项目覆盖率。


<details>
  <summary>Details</summary>
Motivation: 解决序列推荐中的稀疏交互数据、流行度偏差以及准确性与多样性之间的冲突目标。现有对比自监督学习方法存在批量要求大、依赖手工增强和负采样可能加剧流行度偏差的问题。

Method: 集成Barlow Twins冗余减少原则到Transformer推荐器中，学习对齐相似短期行为的用户嵌入，同时保持长期区分性，无需负采样或人工扰动。

Result: 在五个公共基准测试中，BT-SR持续改进下一项预测准确性，显著增强长尾项目覆盖率和推荐校准度，单个超参数可控制准确性-多样性权衡。

Conclusion: BT-SR通过结构敏感的对齐机制有效识别新兴用户意图，减轻噪声历史上下文影响，为实践者提供可适应特定应用需求的推荐系统。

Abstract: Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs

</details>


### [6] [Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering](https://arxiv.org/abs/2510.26461)
*Danial Ebrat,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: 提出基于图注意力网络和大型语言模型的协同过滤框架，通过LLM生成上下文感知嵌入来解决数据稀疏和冷启动问题，在MovieLens数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 推荐系统面临数据稀疏和冷启动场景的挑战，难以为新用户或交互较少的用户提供准确推荐。

Method: 使用GAT框架结合LLM驱动的上下文感知嵌入，生成简洁的用户档案和统一的项目元数据作为二部图初始节点特征，采用混合损失函数结合BPR和余弦相似度项。

Result: 在MovieLens 100k和1M数据集上，Precision、NDCG和MAP指标均优于现有基线方法，对交互历史有限的用户表现出鲁棒性。

Conclusion: 通过将LLM衍生的上下文理解整合到基于图的架构中，有效缓解了稀疏性和冷启动限制，未来将关注推荐准确性、覆盖率、多样性、公平性和可解释性的平衡。

Abstract: Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.

</details>


### [7] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: WeaveRec是一种基于LLM的跨域序列推荐方法，通过交叉训练多个LoRA模块并融合它们来解决传统方法需要重叠用户/物品的限制，在无重叠的真实场景中有效提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法依赖重叠用户或物品来建立跨域关联，这在真实场景中很少成立。虽然LLM和模型融合技术似乎能克服这一限制，但实证研究表明，简单训练或合并领域特定LLM反而会降低性能。

Method: 提出WeaveRec方法：以交织方式交叉训练多个LoRA模块，使用源域和目标域数据，然后通过模型融合将它们合并。该方法可扩展到多源域场景，且不增加推理时的延迟或内存成本。

Result: 在单源、多源和跨平台跨域推荐场景的广泛实验中，WeaveRec有效缓解了性能下降问题，在真实推荐任务中持续优于基线方法。

Conclusion: WeaveRec通过交叉训练和融合LoRA模块，成功解决了LLM在跨域推荐中的性能下降问题，提供了理论保证，并在多种实际场景中验证了其有效性。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [8] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: ProfOlaf是一个半自动化的工具，旨在通过结合自动化和人工指导来简化系统综述过程，提高效率、质量和可重复性。


<details>
  <summary>Details</summary>
Motivation: 系统综述和映射研究对于综合研究、识别差距和指导未来工作至关重要，但它们通常劳动密集且耗时。现有工具仅支持特定步骤，大部分过程仍需手动操作且容易出错。

Method: ProfOlaf支持迭代式滚雪球法进行文章收集，采用人在回路中的过滤机制，并利用大语言模型协助分析文章、提取关键主题和回答关于论文内容的查询。

Result: 通过将自动化与指导性人工努力相结合，ProfOlaf提高了跨研究领域系统综述的效率、质量和可重复性。

Conclusion: ProfOlaf是一个有效的半自动化工具，能够在保持方法论严谨性的同时，显著改善系统综述过程的效率和可靠性。

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>
